working directory /home/cs.aau.dk/ng78zb/vec2text_exp
sif /home/cs.aau.dk/ng78zb/pytorch_23.10-py3.sif
launch evaluation yiyic/mt5_me5_semitic-fami_32_2layers_corrector with batch size 8
loading experiment and trainer from yiyic/mt5_me5_semitic-fami_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector
on rank 0, output dir: saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_semitic-fami_32_2layers_inverter
on rank 0, output dir: saves/yiyic__mt5_me5_semitic-fami_32_2layers_inverter
adding embedding type to dataset args.
Loading datasets with TOKENIZERS_PARALLELISM = False
loading data from yiyic/Semitic_heb_arb_mlt_train for sem_fami
loading data from yiyic/Semitic_heb_arb_mlt_dev for sem_fami
allowed columns ['text', 'lang']
>> using fast tokenizers: True True
dataset kwargs: {'batched': True, 'num_proc': 6, 'remove_columns': ['text', 'lang'], 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'batched': True, 'num_proc': 6, 'remove_columns': ['text', 'lang'], 'desc': 'Running tokenizer on dataset'}
[Precomputing embeddings with batch size: 256]
	saving precomputed embeddings to file: 7de7f346f65fc36ac111194716da8091da112d4db2a7a08e
dataset kwargs: {'batched': True, 'batch_size': 256, 'new_fingerprint': '7de7f346f65fc36ac111194716da8091da112d4db2a7a08e', 'num_proc': 1}
	saving precomputed embeddings to file: 4e7abf46674280a6c111194716da8091da112d4db2a7a08e
dataset kwargs: {'batched': True, 'batch_size': 256, 'new_fingerprint': '4e7abf46674280a6c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
saving train_dataset to path: /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/8ad007b5079e8fccf240a96420a6355f.arrow
loaded dict of val datasets from /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/b7c2d88873c9bb4061ee54b83d4d22ce.arrow
07/04/2024 14:47:56 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
07/04/2024 14:50:32 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
data arguments for experiment: DataArguments(dataset_name='mt-ms_sem_fami', max_eval_samples=500, use_less_data=3000)
model yiyic/mt5_me5_semitic-fami_32_2layers_corrector parameters 890566656
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
Using Frozen Embeddings as Input -- Val datasets
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '24e7b7ef1b0606ddc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7cf9deee5c6ee34dc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7d3c7a3fdc899099c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a808a07212534aa6c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '1835610d5c7fc595c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '74575508bfab4c9cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'af6bef1b30e3c5dbc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'bfa2f55e85bb2562c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd839c9a8871bbce8c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '67555e67fd16afc9c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '48ec325196197014c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a20c740ae5a0d86bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'cc2235accdffe49bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '104320b250ecc6cac111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd7b432ef61da1e93c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'c06e214a198bcc2cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '3ae3df71733802abc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '18a40ab4beb7f210c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '96cd5258a58b461bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7ee1e04e96a3faa2c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
output dir ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations
evaluating deu_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] 
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] 
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] 
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720097606/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720097606
{'eval_loss': nan, 'eval_pred_num_tokens': 0.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 0.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.5922611355781555, 'eval_emb_cos_sim_sem': 0.010021838982324838, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 64.3876, 'eval_samples_per_second': 7.765, 'eval_steps_per_second': 0.978}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/deu_Latn_steps-1.json
evaluating corrector with steps 20
[pred] 
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] 
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] 
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720098021/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720098021
{'eval_loss': nan, 'eval_pred_num_tokens': 0.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 0.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.5922611355781555, 'eval_emb_cos_sim_sem': 0.010021838982324838, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 414.62, 'eval_samples_per_second': 1.206, 'eval_steps_per_second': 0.152}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/deu_Latn_steps-20.json
evaluating corrector with steps 50
[pred] 
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] 
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] 
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720098991/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720098991
{'eval_loss': nan, 'eval_pred_num_tokens': 0.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 0.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.5922611355781555, 'eval_emb_cos_sim_sem': 0.010021838982324838, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 970.3945, 'eval_samples_per_second': 0.515, 'eval_steps_per_second': 0.065}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/deu_Latn_steps-50.json
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720101593/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720101593
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.00011764705882352942, 'eval_token_set_recall': 0.0005, 'eval_token_set_f1': 0.00019047619047619045, 'eval_token_set_f1_sem': 0.00019047619047619045, 'eval_n_ngrams_match_1': 0.006, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0010171304274729273, 'eval_bleu_score_sem': 0.0010171304274729273, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7175376415252686, 'eval_emb_cos_sim_sem': 0.006514191881152478, 'eval_emb_top1_equal': 0.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 2602.3769, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/deu_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 11.13 GiB is free. Including non-PyTorch memory, this process has 33.39 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 3.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating ydd_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: טשיינאַ (מעריאַט / הילטאָן) האָטעל קאַלעקשאַן עוראָטאָפּ 5 שטערן האָטעל מאַטראַ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: חול־המועד סוכּות, תּשע״ז - yiddish.forward.com חול־המועד סוכּ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: איראן פראוואקירט טראמפ: מיר וועלן אויסברייטערן דעם מיסיל פראגראם טראץ אפמא
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720101694/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720101694
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0009172077922077922, 'eval_token_set_recall': 0.003999999999999999, 'eval_token_set_f1': 0.0014878372401592216, 'eval_token_set_f1_sem': 0.0006082674250480639, 'eval_n_ngrams_match_1': 0.016, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.482, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.023167484068084818, 'eval_bleu_score_sem': 0.009444046777983751, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7710964679718018, 'eval_emb_cos_sim_sem': 0.007664337888710773, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 78.6187, 'eval_samples_per_second': 6.36, 'eval_steps_per_second': 0.801}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/ydd_Hebr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.43 GiB is free. Including non-PyTorch memory, this process has 35.09 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.18 GiB is free. Including non-PyTorch memory, this process has 35.34 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: טשיינאַ (מעריאַט / הילטאָן) האָטעל קאַלעקשאַן עוראָטאָפּ 5 שטערן האָטעל מאַטראַ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: חול־המועד סוכּות, תּשע״ז - yiddish.forward.com חול־המועד סוכּ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: איראן פראוואקירט טראמפ: מיר וועלן אויסברייטערן דעם מיסיל פראגראם טראץ אפמא
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720104338/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720104338
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0009172077922077922, 'eval_token_set_recall': 0.003, 'eval_token_set_f1': 0.0014000000000000002, 'eval_token_set_f1_sem': 0.0005719305722982723, 'eval_n_ngrams_match_1': 0.016, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.482, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.004929570167418107, 'eval_bleu_score_sem': 0.0020095013821800085, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7412903308868408, 'eval_emb_cos_sim_sem': 0.006209468849329698, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2600.8128, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/ydd_Hebr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.43 GiB is free. Including non-PyTorch memory, this process has 35.10 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating heb_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: במחקר שהתפרסם לאחרונה (ואני מתנצל שלא הגעתי לדון בו עד כה מפאת עניינים אחר



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: תוכנית ריאליטי בישראל היא לא רק תוכנית ריאליטי. היא שיעור באזרחות,



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: בפני בקשה לעיכוב ביצוע פסק הדין אשר ניתן ביום 20.4.11 ואשר במסגרתו חו
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720104441/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720104441
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0004761904761904762, 'eval_token_set_recall': 0.002, 'eval_token_set_f1': 0.0007652086475615886, 'eval_token_set_f1_sem': 0.00044569810310492126, 'eval_n_ngrams_match_1': 0.03, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.944, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.015988324114837398, 'eval_bleu_score_sem': 0.009215380369649454, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7785422801971436, 'eval_emb_cos_sim_sem': 0.005904145224915638, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 80.8235, 'eval_samples_per_second': 6.186, 'eval_steps_per_second': 0.779}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/heb_Hebr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: במחקר שהתפרסם לאחרונה (ואני מתנצל שלא הגעתי לדון בו עד כה מפאת עניינים אחר



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: תוכנית ריאליטי בישראל היא לא רק תוכנית ריאליטי. היא שיעור באזרחות,



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: בפני בקשה לעיכוב ביצוע פסק הדין אשר ניתן ביום 20.4.11 ואשר במסגרתו חו
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720107095/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720107095
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0004761904761904762, 'eval_token_set_recall': 0.0015, 'eval_token_set_f1': 0.0007184628237259817, 'eval_token_set_f1_sem': 0.00041791581490659325, 'eval_n_ngrams_match_1': 0.03, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.944, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.003458239805259904, 'eval_bleu_score_sem': 0.001994322142747288, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7522383332252502, 'eval_emb_cos_sim_sem': 0.005635339869102743, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 2611.325, 'eval_samples_per_second': 0.191, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/heb_Hebr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating arb_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: فستان زفاف أنيق بقصّة الأميرة من نسيج الميكادو، مُزيّن بالأزهار



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: رئيس القمة الدينية لمجموعة العشرين الشيخ د.محمد العيسى يطلق منصة R20 



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: تسعى كلية العلوم الإسلامية إلى تبوأ مكانة وسمعة مرموقة بين جامعات العالم، 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720107194/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720107194
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.48, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.739212155342102, 'eval_emb_cos_sim_sem': 0.006568781928141705, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 76.5363, 'eval_samples_per_second': 6.533, 'eval_steps_per_second': 0.823}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/arb_Arab_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: فستان زفاف أنيق بقصّة الأميرة من نسيج الميكادو، مُزيّن بالأزهار



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: رئيس القمة الدينية لمجموعة العشرين الشيخ د.محمد العيسى يطلق منصة R20 



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: تسعى كلية العلوم الإسلامية إلى تبوأ مكانة وسمعة مرموقة بين جامعات العالم، 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720109840/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720109840
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.48, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7220540642738342, 'eval_emb_cos_sim_sem': 0.003597487718584887, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2602.6501, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/arb_Arab_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating amh_Ethi val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: ማርች 20, 2016 የራስ ዱሜራ ደሴቶችና የባህር ግዛት የሚያሳን የኤ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: አንድነት አንድነት ውስጥ ጥንካሬ - የህብረት ማህበረሰብ እንክብካቤ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: ከቡናማዎቹ ጋር ነገ ወደ ዩጋንዳ የሚያቀኑ 20 ተጫዋቾች ተለይተው
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720109938/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720109938
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.001107936507936508, 'eval_token_set_recall': 0.004666666666666666, 'eval_token_set_f1': 0.0017816993464052286, 'eval_token_set_f1_sem': 0.0006763070948036226, 'eval_n_ngrams_match_1': 0.02, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 11.838, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.02715692710369573, 'eval_bleu_score_sem': 0.0102562010098835, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7717188596725464, 'eval_emb_cos_sim_sem': 0.003146690624540682, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 76.9148, 'eval_samples_per_second': 6.501, 'eval_steps_per_second': 0.819}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/amh_Ethi_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ማርች 20, 2016 የራስ ዱሜራ ደሴቶችና የባህር ግዛት የሚያሳን የኤ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: አንድነት አንድነት ውስጥ ጥንካሬ - የህብረት ማህበረሰብ እንክብካቤ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ከቡናማዎቹ ጋር ነገ ወደ ዩጋንዳ የሚያቀኑ 20 ተጫዋቾች ተለይተው
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720112581/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720112581
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.001107936507936508, 'eval_token_set_recall': 0.0035, 'eval_token_set_f1': 0.0016731893837156995, 'eval_token_set_f1_sem': 0.0006342618531495689, 'eval_n_ngrams_match_1': 0.02, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 11.838, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.005848651259615182, 'eval_bleu_score_sem': 0.002205594441879438, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7266525030136108, 'eval_emb_cos_sim_sem': 0.003638811715733056, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 2599.6853, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/amh_Ethi_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mlt_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Dħul Temi Temi Problemi muskuloskeletali Practical tools and guidance - Musculoskeletal disorders



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: test – One News Mindu feġġ l-ewwel każ ta' coronavirus f'pajjiżna, saru aktar



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 'L-MFA emmnet lil min ibagħbas il-logħob u mhux lili' - Illum
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720112686/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720112686
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0016368548118548118, 'eval_token_set_recall': 0.006666666666666667, 'eval_token_set_f1': 0.0026191415567576558, 'eval_token_set_f1_sem': 0.0008267585625574295, 'eval_n_ngrams_match_1': 0.042, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.932, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.041631616646314526, 'eval_bleu_score_sem': 0.013189943051107189, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7728322148323059, 'eval_emb_cos_sim_sem': 0.005367373828265273, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 83.112, 'eval_samples_per_second': 6.016, 'eval_steps_per_second': 0.758}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/mlt_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Dħul Temi Temi Problemi muskuloskeletali Practical tools and guidance - Musculoskeletal disorders



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: test – One News Mindu feġġ l-ewwel każ ta' coronavirus f'pajjiżna, saru aktar



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 'L-MFA emmnet lil min ibagħbas il-logħob u mhux lili' - Illum
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720115328/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720115328
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0016368548118548118, 'eval_token_set_recall': 0.005, 'eval_token_set_f1': 0.002456137893753993, 'eval_token_set_f1_sem': 0.000774647478177503, 'eval_n_ngrams_match_1': 0.042, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.932, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.009086775165644375, 'eval_bleu_score_sem': 0.0028700076633073695, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7281607985496521, 'eval_emb_cos_sim_sem': 0.007832949648901145, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2598.6092, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/mlt_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating hin_Deva val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Blog News: ग़लती न करे मुसलमान!!! ग़लती न करे मुसलमान!!! 



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: RBSE Solutions for Class 9 Hindi Sparsh Chapter 11 आदमी नामा - Rbse solutions RBSE Solutions for Class 9



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: नई दिल्ली, प्रधानमंत्री नरेंद्र मोदी का कार्यकाल पूरा होने में केवल 14 महीने का वक्त बा
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720115427/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720115427
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0004862745098039215, 'eval_token_set_recall': 0.0026666666666666666, 'eval_token_set_f1': 0.0008222222222222223, 'eval_token_set_f1_sem': 0.0004985254984744602, 'eval_n_ngrams_match_1': 0.018, 'eval_n_ngrams_match_2': 0.002, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 17.206, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.015276686247601185, 'eval_bleu_score_sem': 0.009102788483123143, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7472072839736938, 'eval_emb_cos_sim_sem': 0.009369563159513606, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 76.7819, 'eval_samples_per_second': 6.512, 'eval_steps_per_second': 0.821}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/hin_Deva_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Blog News: ग़लती न करे मुसलमान!!! ग़लती न करे मुसलमान!!! 



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: RBSE Solutions for Class 9 Hindi Sparsh Chapter 11 आदमी नामा - Rbse solutions RBSE Solutions for Class 9



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: नई दिल्ली, प्रधानमंत्री नरेंद्र मोदी का कार्यकाल पूरा होने में केवल 14 महीने का वक्त बा
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720118070/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720118070
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0004862745098039215, 'eval_token_set_recall': 0.002, 'eval_token_set_f1': 0.0007819548872180451, 'eval_token_set_f1_sem': 0.0004742933974809251, 'eval_n_ngrams_match_1': 0.018, 'eval_n_ngrams_match_2': 0.002, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 17.206, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0037336022945504265, 'eval_bleu_score_sem': 0.0023221580441061688, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.749690055847168, 'eval_emb_cos_sim_sem': 0.0039034004110613373, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 2600.0736, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/hin_Deva_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating urd_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: قائداعظم سے نہرو تک ۔۔ رؤف کلاسرا مرکزی صفحہ/ لکھاری/ ر



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: سوناکشی سنہا سوشل میڈیا میمز کے نشے میں مبتلا ہو گئیں اداکارہ نہ صرف خود



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: نئی دہلی:مہاراشٹر میں گزشتہ 5 سالوں میں (18-2014)14034 کسانوں نے خود
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720118168/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720118168
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0004222222222222222, 'eval_token_set_recall': 0.0026666666666666666, 'eval_token_set_f1': 0.0007287784679089027, 'eval_token_set_f1_sem': 0.0005153412660083615, 'eval_n_ngrams_match_1': 0.008, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 17.024, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.008437939176978385, 'eval_bleu_score_sem': 0.0059622379313945175, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7689958810806274, 'eval_emb_cos_sim_sem': 0.004579422361013664, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 76.8158, 'eval_samples_per_second': 6.509, 'eval_steps_per_second': 0.82}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/urd_Arab_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: قائداعظم سے نہرو تک ۔۔ رؤف کلاسرا مرکزی صفحہ/ لکھاری/ ر



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: سوناکشی سنہا سوشل میڈیا میمز کے نشے میں مبتلا ہو گئیں اداکارہ نہ صرف خود



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: نئی دہلی:مہاراشٹر میں گزشتہ 5 سالوں میں (18-2014)14034 کسانوں نے خود
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720120810/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720120810
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0004222222222222222, 'eval_token_set_recall': 0.002, 'eval_token_set_f1': 0.000696969696969697, 'eval_token_set_f1_sem': 0.0004928049268673052, 'eval_n_ngrams_match_1': 0.008, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 17.024, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0018381621843941503, 'eval_bleu_score_sem': 0.0012984739106463874, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7368135452270508, 'eval_emb_cos_sim_sem': 0.0036664208649579777, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 2598.3992, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/urd_Arab_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating guj_Gujr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 'અમારા વિરોધીને લગ્નમાં કેમ બોલાવ્યો?' કહી કન્યાના મા-બા



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: આજે સાંજથી કેજરીવાલની ગુજરાત યાત્રા શરૂ, જાણો આખો કાર્યક્રમ | Read here



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: ફરહાનની ડૉન 3માં જોવા મળશે ગુજરાતી શાહરુખ ખાન | Shahrukh Kahn
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720120908/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720120908
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0006103896103896104, 'eval_token_set_recall': 0.0026666666666666666, 'eval_token_set_f1': 0.0009915966386554623, 'eval_token_set_f1_sem': 0.0004962344081653925, 'eval_n_ngrams_match_1': 0.016, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.074, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.016720405943548365, 'eval_bleu_score_sem': 0.008426787390917324, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7538293600082397, 'eval_emb_cos_sim_sem': 0.005577900958571983, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 76.6959, 'eval_samples_per_second': 6.519, 'eval_steps_per_second': 0.821}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/guj_Gujr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 'અમારા વિરોધીને લગ્નમાં કેમ બોલાવ્યો?' કહી કન્યાના મા-બા



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: આજે સાંજથી કેજરીવાલની ગુજરાત યાત્રા શરૂ, જાણો આખો કાર્યક્રમ | Read here



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ફરહાનની ડૉન 3માં જોવા મળશે ગુજરાતી શાહરુખ ખાન | Shahrukh Kahn
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720123552/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720123552
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0006103896103896104, 'eval_token_set_recall': 0.002, 'eval_token_set_f1': 0.0009333333333333333, 'eval_token_set_f1_sem': 0.0004668543058844834, 'eval_n_ngrams_match_1': 0.016, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.074, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.003557762857814287, 'eval_bleu_score_sem': 0.001793049241227993, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7327566742897034, 'eval_emb_cos_sim_sem': 0.005139857566741625, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2600.2949, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/guj_Gujr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating sin_Sinh val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: මිනිස්සු | ඇවිද යන මඟ ← ගරු කිරීම ගුරුවරු → 16 අප් රේල් අපිට කෙනෙක් දැක්කාම ආ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: සිවුමැලියා සිකුරු ලියා... | Sunday Apple සිවුමැලියා සිකුරු ලියා... March 24, 2017 | 11:00 am 0



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: නිළිකමට මැදි වුණේ පුංචි කෙල්ලක කාලේ... - Sri Lanka News Update නිළිකමට මැ
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720123650/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720123650
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.00015384615384615385, 'eval_token_set_recall': 0.0006666666666666666, 'eval_token_set_f1': 0.00025, 'eval_token_set_f1_sem': 0.00025, 'eval_n_ngrams_match_1': 0.002, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.694, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.0036321698830878617, 'eval_bleu_score_sem': 0.0036321698830878617, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7562764883041382, 'eval_emb_cos_sim_sem': 0.00805240828909737, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 76.6167, 'eval_samples_per_second': 6.526, 'eval_steps_per_second': 0.822}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/sin_Sinh_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: මිනිස්සු | ඇවිද යන මඟ ← ගරු කිරීම ගුරුවරු → 16 අප් රේල් අපිට කෙනෙක් දැක්කාම ආ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: සිවුමැලියා සිකුරු ලියා... | Sunday Apple සිවුමැලියා සිකුරු ලියා... March 24, 2017 | 11:00 am 0



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: නිළිකමට මැදි වුණේ පුංචි කෙල්ලක කාලේ... - Sri Lanka News Update නිළිකමට මැ
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720126294/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720126294
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.00015384615384615385, 'eval_token_set_recall': 0.0005, 'eval_token_set_f1': 0.00023529411764705883, 'eval_token_set_f1_sem': 0.00023529411764705878, 'eval_n_ngrams_match_1': 0.002, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.694, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0007728519957559891, 'eval_bleu_score_sem': 0.0007728519957559891, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7308209538459778, 'eval_emb_cos_sim_sem': 0.004721072650502715, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 2600.611, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/sin_Sinh_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating pan_Guru val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: ਕਰੋਨਾ ਮਹਾਂਮਾਰੀ ਦੇ ਚੱਲਦੇ ਫੋਟੋਗ੍ਰਾਫਰ ਦੀਆਂ ਬੰਦ ਪਈਆਂ ਦੁਕਾਨਾਂ ਖੋ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: ਮੋਦੀ ਦੇ ਸੁਪਨਿਆਂ ਦਾ ਯੂ ਪੀ ਬਣਾਉਣ ਲਈ ਕਵਾਇਦ ਆਰੰਭ - Panja



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: ਸ਼ਹੀਦਾਂ ਦੀਆਂ ਤਸਵੀਰਾਂ ਅਜਾਇਬ ਘਰ 'ਚ ਲਗਾਉਣ ਦੀ ਮੰਗ :
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720126393/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720126393
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.00014285714285714284, 'eval_token_set_recall': 0.0006666666666666666, 'eval_token_set_f1': 0.00023529411764705883, 'eval_token_set_f1_sem': 0.00023529411764705883, 'eval_n_ngrams_match_1': 0.004, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.07, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.004319402267866686, 'eval_bleu_score_sem': 0.004319402267866686, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7382553815841675, 'eval_emb_cos_sim_sem': 0.0035561146598708723, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 76.9696, 'eval_samples_per_second': 6.496, 'eval_steps_per_second': 0.819}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/pan_Guru_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ਕਰੋਨਾ ਮਹਾਂਮਾਰੀ ਦੇ ਚੱਲਦੇ ਫੋਟੋਗ੍ਰਾਫਰ ਦੀਆਂ ਬੰਦ ਪਈਆਂ ਦੁਕਾਨਾਂ ਖੋ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ਮੋਦੀ ਦੇ ਸੁਪਨਿਆਂ ਦਾ ਯੂ ਪੀ ਬਣਾਉਣ ਲਈ ਕਵਾਇਦ ਆਰੰਭ - Panja



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ਸ਼ਹੀਦਾਂ ਦੀਆਂ ਤਸਵੀਰਾਂ ਅਜਾਇਬ ਘਰ 'ਚ ਲਗਾਉਣ ਦੀ ਮੰਗ :
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720129035/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720129035
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.00014285714285714284, 'eval_token_set_recall': 0.0005, 'eval_token_set_f1': 0.00022222222222222223, 'eval_token_set_f1_sem': 0.0002222222222222222, 'eval_n_ngrams_match_1': 0.004, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.07, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0009190810921970751, 'eval_bleu_score_sem': 0.0009190810921970749, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7200504541397095, 'eval_emb_cos_sim_sem': 0.00796818703197146, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2599.3294, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/pan_Guru_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating tur_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: tek parti devri TEK PARTİ DEVRİ haberleri haber haberi | Sayfa 7 Tek parti chp zamanında yapılan... 15 Şu



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Anasayfa Spor Fenerbahçe-Dinamo Zagreb maçının hakemleri açıklandı kaynuka Tarih: 2018-11-27 Saat: 15:07:19 



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Alışveriş Annelik Bebek Çocuk Çocuk Kitapları Eğitim Sağlık 7–14 yaş çocuklar için öğretim metodları 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720129133/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720129133
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0013101953601953602, 'eval_token_set_recall': 0.006, 'eval_token_set_f1': 0.002141770149355289, 'eval_token_set_f1_sem': 0.0007149457372481027, 'eval_n_ngrams_match_1': 0.044, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 16.658, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.0381738836916595, 'eval_bleu_score_sem': 0.012765572455871859, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7672379016876221, 'eval_emb_cos_sim_sem': 0.008166942349925724, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 76.3503, 'eval_samples_per_second': 6.549, 'eval_steps_per_second': 0.825}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/tur_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: tek parti devri TEK PARTİ DEVRİ haberleri haber haberi | Sayfa 7 Tek parti chp zamanında yapılan... 15 Şu



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Anasayfa Spor Fenerbahçe-Dinamo Zagreb maçının hakemleri açıklandı kaynuka Tarih: 2018-11-27 Saat: 15:07:19 



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Alışveriş Annelik Bebek Çocuk Çocuk Kitapları Eğitim Sağlık 7–14 yaş çocuklar için öğretim metodları 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720131779/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720131779
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0013101953601953602, 'eval_token_set_recall': 0.0045, 'eval_token_set_f1': 0.0020194932518461934, 'eval_token_set_f1_sem': 0.0006733885747509053, 'eval_n_ngrams_match_1': 0.044, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 16.658, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.008417688195317463, 'eval_bleu_score_sem': 0.0028146391987885653, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7216833233833313, 'eval_emb_cos_sim_sem': 0.0037895775783016042, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 2602.3293, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/tur_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating kaz_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: ҚМГ АЛТЫН ЕРЕЖЕЛЕРІ - ЕҢбек қауіпсіздігі және еңбекті қОРҒау жиналыстарын



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: психометриялық тестілеу кестесі инструкция по приму апелляциялық ведомость 18.10.2018 ж No 578 бұйры



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Ыбырай Алтынсарин - Ы - Ұлы заман Тұлғалары - Скачать Рефераты,
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720131877/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720131877
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0010816239316239315, 'eval_token_set_recall': 0.005333333333333333, 'eval_token_set_f1': 0.001794862155388471, 'eval_token_set_f1_sem': 0.0007071106564203936, 'eval_n_ngrams_match_1': 0.026, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.352, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.028991380362369597, 'eval_bleu_score_sem': 0.010977020900868007, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7539284229278564, 'eval_emb_cos_sim_sem': 0.0076267995346667415, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 76.3291, 'eval_samples_per_second': 6.551, 'eval_steps_per_second': 0.825}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/kaz_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: ҚМГ АЛТЫН ЕРЕЖЕЛЕРІ - ЕҢбек қауіпсіздігі және еңбекті қОРҒау жиналыстарын



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: психометриялық тестілеу кестесі инструкция по приму апелляциялық ведомость 18.10.2018 ж No 578 бұйры



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Ыбырай Алтынсарин - Ы - Ұлы заман Тұлғалары - Скачать Рефераты,
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720134524/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720134524
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0010816239316239315, 'eval_token_set_recall': 0.004, 'eval_token_set_f1': 0.0016986912468336616, 'eval_token_set_f1_sem': 0.0006690680244766138, 'eval_n_ngrams_match_1': 0.026, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.352, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.006168777037964427, 'eval_bleu_score_sem': 0.0023356871467363105, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7335757613182068, 'eval_emb_cos_sim_sem': 0.007042943852056883, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2603.0894, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/kaz_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating cmn_Hani val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 下行以及现货成交偏弱拖累,市场主导地区价格继续快速调低,邯郸中板价格重



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 每日彩票 珍珠棉的包装定位是一种具有高强的缓冲吸震抗震作用的有明显环保效力的包装



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 火币云于上个月刚刚推出,允许用户开发他们自己的类似火币网的数字货币交易平台,其中包括钱
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720134623/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720134623
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 31.9140625, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 7.802, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7871679663658142, 'eval_emb_cos_sim_sem': 0.003426791994299004, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 77.2965, 'eval_samples_per_second': 6.469, 'eval_steps_per_second': 0.815}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/cmn_Hani_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 下行以及现货成交偏弱拖累,市场主导地区价格继续快速调低,邯郸中板价格重



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 每日彩票 珍珠棉的包装定位是一种具有高强的缓冲吸震抗震作用的有明显环保效力的包装



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 火币云于上个月刚刚推出,允许用户开发他们自己的类似火币网的数字货币交易平台,其中包括钱
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720137266/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720137266
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 31.9140625, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 7.802, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7478057146072388, 'eval_emb_cos_sim_sem': 0.0033529510449801713, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 2599.858, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/cmn_Hani_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating jpn_Jpan val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 花立山荘では夜半から雨が降り続いていて、朝にちょっとだけ雨脚が弱くなったときに出発。 しばらく



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 今シーズン初の凍結した桧原湖の湖上撮影です。 かなり結氷は進んでいましたが、まだワカサギ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: まず少なくともこの人たちチームにネット周りが強い人間がいることは間違いなくて、YouTubeのメタタグ(メタタグって?って人はこの記事
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720137365/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720137365
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.001435064935064935, 'eval_token_set_recall': 0.003333333333333333, 'eval_token_set_f1': 0.001942857142857143, 'eval_token_set_f1_sem': 0.0011349396515939578, 'eval_n_ngrams_match_1': 0.012, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 4.93, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.01295820680360006, 'eval_bleu_score_sem': 0.007466416299171305, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.761200487613678, 'eval_emb_cos_sim_sem': 0.008556584037508367, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 76.8153, 'eval_samples_per_second': 6.509, 'eval_steps_per_second': 0.82}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/jpn_Jpan_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 花立山荘では夜半から雨が降り続いていて、朝にちょっとだけ雨脚が弱くなったときに出発。 しばらく



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 今シーズン初の凍結した桧原湖の湖上撮影です。 かなり結氷は進んでいましたが、まだワカサギ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: まず少なくともこの人たちチームにネット周りが強い人間がいることは間違いなくて、YouTubeのメタタグ(メタタグって?って人はこの記事
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720140007/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720140007
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.001435064935064935, 'eval_token_set_recall': 0.0025, 'eval_token_set_f1': 0.0017606060606060608, 'eval_token_set_f1_sem': 0.0010292177491805735, 'eval_n_ngrams_match_1': 0.012, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 4.93, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0027572432765912256, 'eval_bleu_score_sem': 0.0015887017743382366, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7358812093734741, 'eval_emb_cos_sim_sem': 0.007815410633020041, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2598.9682, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/jpn_Jpan_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating kor_Hang val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 세일 중 – Natural Health {% if first_available_variant.compare_at_price > first_available_variant.price



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 신용위험 결정요인 과 관련 - 토토사이트 검증사이트 메이저토토사이트 - 토토탐정 - 신



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: 안드로이드 : 삼성바다폰 영국 온라인 등록 - 타이젠 - 안드로이드 스마트폰과 태블릿 새
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720140110/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720140110
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.004137751137751137, 'eval_token_set_recall': 0.018, 'eval_token_set_f1': 0.006664140744016905, 'eval_token_set_f1_sem': 0.001425435595778316, 'eval_n_ngrams_match_1': 0.09, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.33, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.09629745720967563, 'eval_bleu_score_sem': 0.019784204748800147, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7717163562774658, 'eval_emb_cos_sim_sem': 0.005475135746645788, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 81.1621, 'eval_samples_per_second': 6.161, 'eval_steps_per_second': 0.776}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/kor_Hang_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 세일 중 – Natural Health {% if first_available_variant.compare_at_price > first_available_variant.price



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 신용위험 결정요인 과 관련 - 토토사이트 검증사이트 메이저토토사이트 - 토토탐정 - 신



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: 안드로이드 : 삼성바다폰 영국 온라인 등록 - 타이젠 - 안드로이드 스마트폰과 태블릿 새
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720142752/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720142752
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.004137751137751137, 'eval_token_set_recall': 0.0135, 'eval_token_set_f1': 0.006264010069737624, 'eval_token_set_f1_sem': 0.0013385237083355303, 'eval_n_ngrams_match_1': 0.09, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.33, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.02077771499078716, 'eval_bleu_score_sem': 0.0042536597462538295, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7649014592170715, 'eval_emb_cos_sim_sem': 0.008017707603338236, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 2599.1972, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/kor_Hang_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mon_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Алтны олборлолт амжилтад хүрнэ ОХУын гадаад өр 56 тэрбум ам.доллар болж өсчээ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Google Текстийн зарын өөрчлөлтийг анхаарч үзэх 3 зүйл | Martech Zone Google Текстийн зарын



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: » СЕКС (1085550) » ШАР МЭДЭЭ (805751) » ӨГҮҮЛЛЭГ (906359) хөлийг 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720142850/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720142850
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0030903568653568646, 'eval_token_set_recall': 0.013333333333333329, 'eval_token_set_f1': 0.005003092573061614, 'eval_token_set_f1_sem': 0.0011044278157789596, 'eval_n_ngrams_match_1': 0.05, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.004, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.0750499531491191, 'eval_bleu_score_sem': 0.01655414805377662, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7633570432662964, 'eval_emb_cos_sim_sem': 0.0034912928068611174, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 76.3604, 'eval_samples_per_second': 6.548, 'eval_steps_per_second': 0.825}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/mon_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Алтны олборлолт амжилтад хүрнэ ОХУын гадаад өр 56 тэрбум ам.доллар болж өсчээ



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Google Текстийн зарын өөрчлөлтийг анхаарч үзэх 3 зүйл | Martech Zone Google Текстийн зарын



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: » СЕКС (1085550) » ШАР МЭДЭЭ (805751) » ӨГҮҮЛЛЭГ (906359) хөлийг 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720145493/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720145493
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0030903568653568646, 'eval_token_set_recall': 0.01, 'eval_token_set_f1': 0.0047053205652276865, 'eval_token_set_f1_sem': 0.0010379343532462425, 'eval_n_ngrams_match_1': 0.05, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 14.004, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.016188185397325215, 'eval_bleu_score_sem': 0.0035615035170583178, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7444403171539307, 'eval_emb_cos_sim_sem': 0.004191397400198118, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 2599.341, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/mon_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating hun_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Ultrahang kezelés: Fájdalomcsillapítás tűszűrás nélkül [teljes útmutató] Ízületi fájdalom fono



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Az új Mike Tyson | SamanSport.hu 2019. 12. 13., Péntek, 18:40 Gervonta "Tank" Davis hatalmas



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Járművek | Hobbi Zóna - Part 2 TankChair – az Off Road tolószék A rendkívüli gépezet
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720145591/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720145591
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0008621794871794871, 'eval_token_set_recall': 0.003999999999999999, 'eval_token_set_f1': 0.0014149122807017543, 'eval_token_set_f1_sem': 0.0005782308877401847, 'eval_n_ngrams_match_1': 0.03, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 16.376, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.024617497171640787, 'eval_bleu_score_sem': 0.01016063423796964, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7590373754501343, 'eval_emb_cos_sim_sem': 0.0039559598360602004, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 76.7342, 'eval_samples_per_second': 6.516, 'eval_steps_per_second': 0.821}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/hun_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Ultrahang kezelés: Fájdalomcsillapítás tűszűrás nélkül [teljes útmutató] Ízületi fájdalom fono



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Az új Mike Tyson | SamanSport.hu 2019. 12. 13., Péntek, 18:40 Gervonta "Tank" Davis hatalmas



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Járművek | Hobbi Zóna - Part 2 TankChair – az Off Road tolószék A rendkívüli gépezet
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720148242/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720148242
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0008621794871794871, 'eval_token_set_recall': 0.003, 'eval_token_set_f1': 0.0013352941176470587, 'eval_token_set_f1_sem': 0.0005453366714319506, 'eval_n_ngrams_match_1': 0.03, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 16.376, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.005584347478764235, 'eval_bleu_score_sem': 0.00230323666490207, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7341576814651489, 'eval_emb_cos_sim_sem': 0.005494541736207022, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 2607.338, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/hun_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mhr_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Тургым жапыште, шошо ага годым, кеч ик гана Кугу Качак ялыште лияш



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Мо тыгай Агавайрем? Кузе тудо эрта? Тиде да моло йодышлан вашмутым Марий в



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Коряк рвезе Пётр Нестеров дене мый Наро-Фоминск олаште эртыше «По
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720148350/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720148350
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.858, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7577528953552246, 'eval_emb_cos_sim_sem': 0.006873831598040445, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 86.6178, 'eval_samples_per_second': 5.772, 'eval_steps_per_second': 0.727}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/mhr_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Тургым жапыште, шошо ага годым, кеч ик гана Кугу Качак ялыште лияш



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Мо тыгай Агавайрем? Кузе тудо эрта? Тиде да моло йодышлан вашмутым Марий в



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Коряк рвезе Пётр Нестеров дене мый Наро-Фоминск олаште эртыше «По
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720150993/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720150993
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0, 'eval_token_set_recall': 0.0, 'eval_token_set_f1': 0.0, 'eval_token_set_f1_sem': 0.0, 'eval_n_ngrams_match_1': 0.0, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 13.858, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.0, 'eval_bleu_score_sem': 0.0, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7054797410964966, 'eval_emb_cos_sim_sem': 0.005687410983348937, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 2599.4807, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/mhr_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating fin_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Tulemme jatkamaan asiakkaan kuuntelua, osallistamista ja haastamista. Työympäristö muutos hankkeissa tuotetaan myö



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Pohjois-suomalaisuus ja kaikille arvokas elämä - siinä tavoitteet toiminnalle. Työskentelen Diakonia-



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04>
[true] query: Kun kirjoitimme koivunjalojen lääketieteellisistä ominaisuuksista, mainitsimme, että paitsi munuaiset, myös ko
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720151092/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720151092
{'eval_loss': nan, 'eval_pred_num_tokens': 8.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0006978937728937728, 'eval_token_set_recall': 0.003333333333333333, 'eval_token_set_f1': 0.0011533367733058135, 'eval_token_set_f1_sem': 0.0005145871332417521, 'eval_n_ngrams_match_1': 0.022, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.462, 'eval_num_pred_words': 21.0, 'eval_bleu_score': 0.021500610337308993, 'eval_bleu_score_sem': 0.009671646770274779, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.765846848487854, 'eval_emb_cos_sim_sem': 0.006012485674646415, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 76.6306, 'eval_samples_per_second': 6.525, 'eval_steps_per_second': 0.822}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/fin_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 8.71 GiB is free. Including non-PyTorch memory, this process has 35.81 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 6.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.19 GiB is free. Including non-PyTorch memory, this process has 35.33 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Tulemme jatkamaan asiakkaan kuuntelua, osallistamista ja haastamista. Työympäristö muutos hankkeissa tuotetaan myö



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Pohjois-suomalaisuus ja kaikille arvokas elämä - siinä tavoitteet toiminnalle. Työskentelen Diakonia-



[pred] <0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x04><0x02>
[true] query: Kun kirjoitimme koivunjalojen lääketieteellisistä ominaisuuksista, mainitsimme, että paitsi munuaiset, myös ko
outptufile for decoded sequences:  saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720153735/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_semitic-fami_32_2layers_corrector/decoded_eval_1720153735
{'eval_loss': nan, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.0006978937728937728, 'eval_token_set_recall': 0.0025, 'eval_token_set_f1': 0.0010902648778809772, 'eval_token_set_f1_sem': 0.00048635976413599613, 'eval_n_ngrams_match_1': 0.022, 'eval_n_ngrams_match_2': 0.0, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 15.462, 'eval_num_pred_words': 93.0, 'eval_bleu_score': 0.004574893285287214, 'eval_bleu_score_sem': 0.002057930038861295, 'eval_rouge_score': 0.0, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.7159556150436401, 'eval_emb_cos_sim_sem': 0.007457823599363456, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 2599.8687, 'eval_samples_per_second': 0.192, 'eval_steps_per_second': 0.024}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_sem_fami_32_2layers_prefix/evaluations/fin_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.53 GiB of which 9.67 GiB is free. Including non-PyTorch memory, this process has 34.86 GiB memory in use. Of the allocated memory 29.10 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
