working directory /home/cs.aau.dk/ng78zb/vec2text_exp
sif /home/cs.aau.dk/ng78zb/pytorch_23.10-py3.sif
launch evaluation yiyic/mt5_me5_cyrl-script_32_2layers_corrector with batch size 8
loading experiment and trainer from yiyic/mt5_me5_cyrl-script_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector
on rank 0, output dir: saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_cyrl-script_32_2layers_inverter
on rank 0, output dir: saves/yiyic__mt5_me5_cyrl-script_32_2layers_inverter
adding embedding type to dataset args.
Loading datasets with TOKENIZERS_PARALLELISM = False
loading data from yiyic/Cyrl_train for cyr_scrp
loading data from yiyic/Cyrl_dev for cyr_scrp
allowed columns ['text', 'lang']
>> using fast tokenizers: True True
dataset kwargs: {'batched': True, 'num_proc': 6, 'remove_columns': ['text', 'lang'], 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'batched': True, 'num_proc': 6, 'remove_columns': ['text', 'lang'], 'desc': 'Running tokenizer on dataset'}
[Precomputing embeddings with batch size: 256]
	saving precomputed embeddings to file: 64601d33e0c41fe8c111194716da8091da112d4db2a7a08e
dataset kwargs: {'batched': True, 'batch_size': 256, 'new_fingerprint': '64601d33e0c41fe8c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
	saving precomputed embeddings to file: 2e43839cec81a22dc111194716da8091da112d4db2a7a08e
dataset kwargs: {'batched': True, 'batch_size': 256, 'new_fingerprint': '2e43839cec81a22dc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
saving train_dataset to path: /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/ba3118a6cc6bd0713dd32a64df09e3eb.arrow
loaded dict of val datasets from /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/b7c2d88873c9bb4061ee54b83d4d22ce.arrow
07/04/2024 16:54:45 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
07/04/2024 16:56:32 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
data arguments for experiment: DataArguments(dataset_name='mt-ms_cyr_scrp', max_eval_samples=500, use_less_data=3000)
model yiyic/mt5_me5_cyrl-script_32_2layers_corrector parameters 890566656
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
Using Frozen Embeddings as Input -- Val datasets
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '24e7b7ef1b0606ddc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7cf9deee5c6ee34dc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7d3c7a3fdc899099c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a808a07212534aa6c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '1835610d5c7fc595c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '74575508bfab4c9cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'af6bef1b30e3c5dbc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'bfa2f55e85bb2562c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd839c9a8871bbce8c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '67555e67fd16afc9c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '48ec325196197014c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a20c740ae5a0d86bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'cc2235accdffe49bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '104320b250ecc6cac111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd7b432ef61da1e93c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'c06e214a198bcc2cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '3ae3df71733802abc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '18a40ab4beb7f210c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '96cd5258a58b461bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7ee1e04e96a3faa2c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
output dir ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations
evaluating deu_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Diese Seite ist eine Seite mit aktuellen Informationen Ã¼ber Sortiment und Sortiment. Dieser Eintrag ist eine Menge Relevante Entscheidung
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Die Beratung und Beratung Ã¼ber Beauty und Hair Salon und Beauty bieten eine ganze breite Bereiche. Und Sie haben eine 
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: 2022/12/21 Â· Unsere besten Angebote Unsere besten Angebote Unsere besten Angebote â–· Zertifikate â€º
[true] query: â± Unsere Bestenliste Dec/2022 á… AusfÃ¼hrlicher Produkttest â˜‘ Ausgezeichnete Produkte â˜‘ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720105296/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720105296
{'eval_loss': 3.0405657291412354, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.29518154309906885, 'eval_token_set_recall': 0.3576752859646156, 'eval_token_set_f1': 0.3198313178263444, 'eval_token_set_f1_sem': 0.004159173619822781, 'eval_n_ngrams_match_1': 5.786, 'eval_n_ngrams_match_2': 1.542, 'eval_n_ngrams_match_3': 0.194, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 18.55, 'eval_bleu_score': 6.129508475931039, 'eval_bleu_score_sem': 0.16352843145549953, 'eval_rouge_score': 0.24811124049265879, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9191709756851196, 'eval_emb_cos_sim_sem': 0.012350937709854116, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 110.7891, 'eval_samples_per_second': 4.513, 'eval_steps_per_second': 0.569}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/deu_Latn_steps-1.json
evaluating corrector with steps 20
[pred] query: Diese Seite ist eine Seite mit Relevante Informationen Ã¼ber Einkaufen und Reduzierungen. Diese Seite ist attraktive Liste der Waren
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Beauty & Hair Salon bieten eine ganze breite Palette von Wellness und Wellness Beratung und Dienstleistungen. Und Sie sind unser
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: â–· Unsere Top Angebote â–· AusfÃ¼hrliche Testberichte â–· Unsere Top Angebote â–· Produkten von Produkten
[true] query: â± Unsere Bestenliste Dec/2022 á… AusfÃ¼hrlicher Produkttest â˜‘ Ausgezeichnete Produkte â˜‘ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720106012/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720106012
{'eval_loss': 3.0405657291412354, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.30182729332855873, 'eval_token_set_recall': 0.35849770799376096, 'eval_token_set_f1': 0.32429802950473485, 'eval_token_set_f1_sem': 0.0044911263811811975, 'eval_n_ngrams_match_1': 5.96, 'eval_n_ngrams_match_2': 1.616, 'eval_n_ngrams_match_3': 0.254, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 18.648, 'eval_bleu_score': 6.419016206085052, 'eval_bleu_score_sem': 0.18189407958290194, 'eval_rouge_score': 0.25793425988039187, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9204800128936768, 'eval_emb_cos_sim_sem': 0.010937419997218293, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 716.0168, 'eval_samples_per_second': 0.698, 'eval_steps_per_second': 0.088}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/deu_Latn_steps-20.json
evaluating corrector with steps 50
[pred] query: Diese Seite ist eine Seite mit Relevante Informationen Ã¼ber Einkaufen und Reduzierungen. Diese Seite ist attraktive Liste der Waren
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Beauty & Hair Salon bieten eine ganze breite Palette von Wellness und Wellness Beratung und Dienstleistungen. Und Sie sind unser
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: â–· Unsere Top Angebote â–· AusfÃ¼hrliche Testberichte â–· Unsere Top Angebote â–· Produkten von Produkten
[true] query: â± Unsere Bestenliste Dec/2022 á… AusfÃ¼hrlicher Produkttest â˜‘ Ausgezeichnete Produkte â˜‘ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720107642/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720107642
{'eval_loss': 3.0405657291412354, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.30153104133493835, 'eval_token_set_recall': 0.358891948299379, 'eval_token_set_f1': 0.3241488709584339, 'eval_token_set_f1_sem': 0.0044628102255778, 'eval_n_ngrams_match_1': 5.954, 'eval_n_ngrams_match_2': 1.608, 'eval_n_ngrams_match_3': 0.25, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 18.652, 'eval_bleu_score': 6.396369938625941, 'eval_bleu_score_sem': 0.1811562308784429, 'eval_rouge_score': 0.25774317371604794, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9204800128936768, 'eval_emb_cos_sim_sem': 0.010937419997218293, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 1629.6147, 'eval_samples_per_second': 0.307, 'eval_steps_per_second': 0.039}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/deu_Latn_steps-50.json
evaluating corrector with steps 50 and beam width 4
[pred] query: Diese Seite ist eine Seite mit Relevante Informationen zu EssenZene. Diese Seite ist eine Seite mit Relevante Informationen zu EssenZen
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Unsere Beratung und Beratung Ã¼ber Beauty & Hair Salon ist eine ganze Palette von Beauty und Wellness Dienstleistungen. Und Sie in
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: 2022/12/21 Â· Unsere Besten Angebote Unsere Besten Angebote Unsere Besten Angebote Unsere Empfehlung:
[true] query: â± Unsere Bestenliste Dec/2022 á… AusfÃ¼hrlicher Produkttest â˜‘ Ausgezeichnete Produkte â˜‘ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720112037/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720112037
{'eval_loss': 3.0405657291412354, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2955721165061591, 'eval_token_set_recall': 0.37525507427728844, 'eval_token_set_f1': 0.32533921932766396, 'eval_token_set_f1_sem': 0.004459366638918801, 'eval_n_ngrams_match_1': 5.786, 'eval_n_ngrams_match_2': 1.592, 'eval_n_ngrams_match_3': 0.232, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 18.17, 'eval_bleu_score': 6.310915513504491, 'eval_bleu_score_sem': 0.17861036200133482, 'eval_rouge_score': 0.249867114730734, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9224458932876587, 'eval_emb_cos_sim_sem': 0.008396333816935853, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4395.4395, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/deu_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 10.29 GiB is free. Including non-PyTorch memory, this process has 34.26 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating ydd_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Ñ‚Ò¯Ğ½ÑˆĞ»ÑĞ» / Ñ‚Ò¯Ğ½ÑˆĞ»ÑĞ» / Ñ‚Ò¯Ğ½ÑˆĞ»ÑĞ» Ğ¢Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ¾Ğ»Ğ³Ğ¾Ğ¹ (ÒšÑ‹Ñ‚Ğ°Ğ¹) 5 ÒšÑ‹Ñ‚Ğ°Ğ¹Ğ´Ñ‹Ò£
[true] query: ×˜×©×™×™× ×Ö· (××¢×¨×™×Ö·×˜ / ×”×™×œ×˜×Ö¸×Ÿ) ×”×Ö¸×˜×¢×œ ×§×Ö·×œ×¢×§×©×Ö·×Ÿ ×¢×•×¨×Ö¸×˜×Ö¸×¤Ö¼ 5 ×©×˜×¢×¨×Ÿ ×”×Ö¸×˜×¢×œ ××Ö·×˜×¨×Ö·



[pred] query: Ğ¡ÑŠĞµĞ·-Ò›Ò±Ğ´Ñ–Ñ€ĞµÑ‚ - Sunnet Ğ¡ÑŠĞµĞ·-Ò›Ò±Ğ´Ñ–Ñ€ĞµÑ‚ - Sunnet.org Ğ¡ÑŠĞµĞ·-Ò›Ò±Ğ´Ñ–Ñ€ĞµÑ‚
[true] query: ×—×•×œÖ¾×”××•×¢×“ ×¡×•×›Ö¼×•×ª, ×ªÖ¼×©×¢×´×– - yiddish.forward.com ×—×•×œÖ¾×”××•×¢×“ ×¡×•×›Ö¼



[pred] query: Ğ¢Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ¾Ğ»Ğ³Ğ¾Ğ¹ Ğ¢Ñ€Ğ°Ğ¼Ğ¿: Ğ‘Ğ¸Ğ´ Ò¯Ò¯Ğ½Ğ¸Ğ¹Ğ³ Ğ·Ğ°Ğ¹Ğ»ÑˆĞ³Ò¯Ğ¹ Ğ·Ğ°Ñ€Ğ»Ğ°Ğ½Ğ° Ğ¢Ñ€Ğ°Ğ¼Ğ¿Ñ‹Ğ½ Ğ¿Ñ€Ğ°Ğ¹Ğ¼ĞµÑ€Ğ¸Ğ· Pars
[true] query: ××™×¨××Ÿ ×¤×¨××•×•××§×™×¨×˜ ×˜×¨×××¤: ××™×¨ ×•×•×¢×œ×Ÿ ××•×™×¡×‘×¨×™×™×˜×¢×¨×Ÿ ×“×¢× ××™×¡×™×œ ×¤×¨××’×¨×× ×˜×¨××¥ ××¤××
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720112201/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720112201
{'eval_loss': 8.249407768249512, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2578532889624994, 'eval_token_set_recall': 0.3375842293671249, 'eval_token_set_f1': 0.2891913517242008, 'eval_token_set_f1_sem': 0.003895926954853858, 'eval_n_ngrams_match_1': 3.722, 'eval_n_ngrams_match_2': 1.234, 'eval_n_ngrams_match_3': 0.102, 'eval_num_true_words': 14.482, 'eval_num_pred_words': 12.97, 'eval_bleu_score': 6.915206694009032, 'eval_bleu_score_sem': 0.13456333745726917, 'eval_rouge_score': 0.5934346686537859, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9075583219528198, 'eval_emb_cos_sim_sem': 0.013900594352991683, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 126.6692, 'eval_samples_per_second': 3.947, 'eval_steps_per_second': 0.497}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/ydd_Hebr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.08 GiB is free. Including non-PyTorch memory, this process has 36.47 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ğ¥ÑÑ‚Ğ°Ğ´ / Ğ¢Ò®Ğ ĞšĞ†Ğ¡Ğ¢ĞĞ (Queensland) Ğ¢Ò®Ğ ĞšĞ†Ğ¡Ğ¢ĞĞ (Queensland) 5 Ğ°Ğ¹Ğ» Ó©Ñ€
[true] query: ×˜×©×™×™× ×Ö· (××¢×¨×™×Ö·×˜ / ×”×™×œ×˜×Ö¸×Ÿ) ×”×Ö¸×˜×¢×œ ×§×Ö·×œ×¢×§×©×Ö·×Ÿ ×¢×•×¨×Ö¸×˜×Ö¸×¤Ö¼ 5 ×©×˜×¢×¨×Ÿ ×”×Ö¸×˜×¢×œ ××Ö·×˜×¨×Ö·



[pred] query: Ğ¡ÑŠĞµĞ·Ñ– - sunnaat.org Ğ¡ÑŠĞµĞ·Ñ– - sunnaat.org Ğ¡ÑŠĞµĞ·Ñ– - sunnaat
[true] query: ×—×•×œÖ¾×”××•×¢×“ ×¡×•×›Ö¼×•×ª, ×ªÖ¼×©×¢×´×– - yiddish.forward.com ×—×•×œÖ¾×”××•×¢×“ ×¡×•×›Ö¼



[pred] query: Ğ‘Ğ¸Ğ´ Ğ¢Ñ€Ğ°Ğ¼Ğ¿Ñ‹Ğ½ ÑƒÑ€Ğ¸Ğ°Ğ³ Ğ¸Ğ½Ğ³ÑĞ¶ Ğ·Ğ°Ñ€Ğ»Ğ°Ğ½Ğ° : Ğ¢Ğ°Ğ» Ğ½ÑƒÑ‚Ğ³Ğ¸Ğ¹Ğ½ Ñ†Ğ°Ñ…Ğ¸Ğ¼ Ğ¼ÑĞ´ÑÑĞ»ÑĞ»Ğ¸Ğ¹Ğ½ ÑĞ°Ğ¹Ñ‚ Ó¨Ğ¡Ğ’
[true] query: ××™×¨××Ÿ ×¤×¨××•×•××§×™×¨×˜ ×˜×¨×××¤: ××™×¨ ×•×•×¢×œ×Ÿ ××•×™×¡×‘×¨×™×™×˜×¢×¨×Ÿ ×“×¢× ××™×¡×™×œ ×¤×¨××’×¨×× ×˜×¨××¥ ××¤××
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720116684/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720116684
{'eval_loss': 8.249407768249512, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2535116521249648, 'eval_token_set_recall': 0.33045878200746664, 'eval_token_set_f1': 0.28365121531463466, 'eval_token_set_f1_sem': 0.003804720145658332, 'eval_n_ngrams_match_1': 3.636, 'eval_n_ngrams_match_2': 1.214, 'eval_n_ngrams_match_3': 0.1, 'eval_num_true_words': 14.482, 'eval_num_pred_words': 13.148, 'eval_bleu_score': 6.849217647853964, 'eval_bleu_score_sem': 0.13837668999380867, 'eval_rouge_score': 0.5673009976378399, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9044241905212402, 'eval_emb_cos_sim_sem': 0.01077878123505681, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 4410.1723, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/ydd_Hebr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.20 GiB is free. Including non-PyTorch memory, this process has 36.35 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating heb_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: í˜„ì¬ ë‚˜ëŠ” í•œê°€ì§€ ì˜ê²¬ì„ Ù„Ø§Ù†ØªÙ‚Ø§Ø¯í•˜ì˜€ìœ¼ë‚˜ í˜„ì¬ ë‚˜ëŠ” í•œê°€ì§€ ì˜ê²¬ì„ Ù„Ø§Ù†ØªÙ‚Ø§Ø¯
[true] query: ×‘××—×§×¨ ×©×”×ª×¤×¨×¡× ×œ××—×¨×•× ×” (×•×× ×™ ××ª× ×¦×œ ×©×œ× ×”×’×¢×ª×™ ×œ×“×•×Ÿ ×‘×• ×¢×“ ×›×” ××¤××ª ×¢× ×™×™× ×™× ××—×¨



[pred] query: LATIN JUSTICE, LATIN JUSTICE, LATIN JUSTICE Ğ½ÑŒ ĞµĞ»Ğ´Ñ–Ò£ ĞµÑ€Ó©Ğ½Ñ…Ğ¸Ğ¹
[true] query: ×ª×•×›× ×™×ª ×¨×™××œ×™×˜×™ ×‘×™×©×¨××œ ×”×™× ×œ× ×¨×§ ×ª×•×›× ×™×ª ×¨×™××œ×™×˜×™. ×”×™× ×©×™×¢×•×¨ ×‘××–×¨×—×•×ª,



[pred] query: - Ğ·Ğ°Ò£Ğ´Ñ‹ Ñ‚Ò±Ğ»Ò“Ğ°, Ğ·Ğ°Ò£Ğ´Ñ‹ Ñ‚Ò±Ğ»Ò“Ğ°, Ğ·Ğ°Ò£Ğ´Ñ‹ Ñ‚Ò±Ğ»Ò“Ğ°, Ğ·Ğ°Ò£Ğ´Ñ‹ Ñ‚Ò±Ğ»Ò“Ğ° I URGENT
[true] query: ×‘×¤× ×™ ×‘×§×©×” ×œ×¢×™×›×•×‘ ×‘×™×¦×•×¢ ×¤×¡×§ ×”×“×™×Ÿ ××©×¨ × ×™×ª×Ÿ ×‘×™×•× 20.4.11 ×•××©×¨ ×‘××¡×’×¨×ª×• ×—×•
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720116846/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720116846
{'eval_loss': 6.913051128387451, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.18232365177821863, 'eval_token_set_recall': 0.3158925769363079, 'eval_token_set_f1': 0.2256683641047851, 'eval_token_set_f1_sem': 0.002982423661566138, 'eval_n_ngrams_match_1': 2.992, 'eval_n_ngrams_match_2': 1.072, 'eval_n_ngrams_match_3': 0.034, 'eval_num_true_words': 15.944, 'eval_num_pred_words': 15.232, 'eval_bleu_score': 5.271233739968117, 'eval_bleu_score_sem': 0.04720765901551119, 'eval_rouge_score': 0.4863460076302154, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8827314376831055, 'eval_emb_cos_sim_sem': 0.011587386962984536, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 124.4733, 'eval_samples_per_second': 4.017, 'eval_steps_per_second': 0.506}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/heb_Hebr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ğ¯Ò“Ğ½Ğ¸ Ğ´Ó™Ğ» Ò›Ğ°Ğ·Ñ–Ñ€ Ğ±Ñ–Ñ€ Ğ½Ó™Ñ€ÑĞµ Ò¯ÑˆÑ–Ğ½ Ğ¿Ñ–ĞºÑ–Ñ€ Ğ±Ñ–Ğ»Ğ´Ñ–Ñ€Ñ–Ğ¿ Ğ¾Ñ‚Ñ‹Ñ€ (Despite une Ã©tude de dÃ©veloppement de
[true] query: ×‘××—×§×¨ ×©×”×ª×¤×¨×¡× ×œ××—×¨×•× ×” (×•×× ×™ ××ª× ×¦×œ ×©×œ× ×”×’×¢×ª×™ ×œ×“×•×Ÿ ×‘×• ×¢×“ ×›×” ××¤××ª ×¢× ×™×™× ×™× ××—×¨



[pred] query: LATIN RADIATION | LATIN RADIATION LATIN RADIATION Ğ½ÑŒ ĞµĞ»Ğ´Ñ–Ò£, ĞµĞ»Ğ´Ñ–Ò£, Ğµ
[true] query: ×ª×•×›× ×™×ª ×¨×™××œ×™×˜×™ ×‘×™×©×¨××œ ×”×™× ×œ× ×¨×§ ×ª×•×›× ×™×ª ×¨×™××œ×™×˜×™. ×”×™× ×©×™×¢×•×¨ ×‘××–×¨×—×•×ª,



[pred] query: Ğ·Ğ°Ò£Ğ´Ñ‹ Ñ‚Ò±Ğ»Ò“Ğ°, Ğ·Ğ°Ò£Ğ´Ñ‹ Ñ‚Ò±Ğ»Ò“Ğ°, Ğ·Ğ°Ò£Ğ´Ñ‹ Ñ‚Ò±Ğ»Ò“Ğ° INSURED PROCEDING INSURED PROCEDING 2001
[true] query: ×‘×¤× ×™ ×‘×§×©×” ×œ×¢×™×›×•×‘ ×‘×™×¦×•×¢ ×¤×¡×§ ×”×“×™×Ÿ ××©×¨ × ×™×ª×Ÿ ×‘×™×•× 20.4.11 ×•××©×¨ ×‘××¡×’×¨×ª×• ×—×•
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720121327/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720121327
{'eval_loss': 6.913051128387451, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.1824416631012609, 'eval_token_set_recall': 0.30897580687286585, 'eval_token_set_f1': 0.2229026776452267, 'eval_token_set_f1_sem': 0.0029153306747265848, 'eval_n_ngrams_match_1': 2.99, 'eval_n_ngrams_match_2': 1.066, 'eval_n_ngrams_match_3': 0.036, 'eval_num_true_words': 15.944, 'eval_num_pred_words': 15.748, 'eval_bleu_score': 5.1610722105204365, 'eval_bleu_score_sem': 0.04984555822648436, 'eval_rouge_score': 0.44254154162980075, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8658183217048645, 'eval_emb_cos_sim_sem': 0.01817565851583999, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4407.8077, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/heb_Hebr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating arb_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„ÙØ³ØªØ§Ù†
[true] query: ÙØ³ØªØ§Ù† Ø²ÙØ§Ù Ø£Ù†ÙŠÙ‚ Ø¨Ù‚ØµÙ‘Ø© Ø§Ù„Ø£Ù…ÙŠØ±Ø© Ù…Ù† Ù†Ø³ÙŠØ¬ Ø§Ù„Ù…ÙŠÙƒØ§Ø¯ÙˆØŒ Ù…ÙØ²ÙŠÙ‘Ù† Ø¨Ø§Ù„Ø£Ø²Ù‡Ø§Ø±



[pred] query: RIIS20 Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… : RIIS20 Ù‚Ø§Ø³Ù…
[true] query: Ø±Ø¦ÙŠØ³ Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¹Ø´Ø±ÙŠÙ† Ø§Ù„Ø´ÙŠØ® Ø¯.Ù…Ø­Ù…Ø¯ Ø§Ù„Ø¹ÙŠØ³Ù‰ ÙŠØ·Ù„Ù‚ Ù…Ù†ØµØ© R20 



[pred] query: Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø· ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù„ÙˆÙ… ÙˆØ§Ù„Ø¯ÙŠÙ† ÙÙŠ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹Ø§Øª Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ 
[true] query: ØªØ³Ø¹Ù‰ ÙƒÙ„ÙŠØ© Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø¥Ù„Ù‰ ØªØ¨ÙˆØ£ Ù…ÙƒØ§Ù†Ø© ÙˆØ³Ù…Ø¹Ø© Ù…Ø±Ù…ÙˆÙ‚Ø© Ø¨ÙŠÙ† Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720121490/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720121490
{'eval_loss': 3.9704792499542236, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.18167247477324913, 'eval_token_set_recall': 0.2846353547639155, 'eval_token_set_f1': 0.21381938325298638, 'eval_token_set_f1_sem': 0.0037292203061655258, 'eval_n_ngrams_match_1': 2.848, 'eval_n_ngrams_match_2': 1.114, 'eval_n_ngrams_match_3': 0.064, 'eval_num_true_words': 15.48, 'eval_num_pred_words': 16.69, 'eval_bleu_score': 5.36298228655285, 'eval_bleu_score_sem': 0.10634661414399534, 'eval_rouge_score': 0.5165387429031489, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8767746686935425, 'eval_emb_cos_sim_sem': 0.012972913241308556, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 126.1489, 'eval_samples_per_second': 3.964, 'eval_steps_per_second': 0.499}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/arb_Arab_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ø§Ù„ÙØ³ØªØ§Ù† Ø§Ù„Ø±Ø§Ø¦Ø¹Ø© Ø§Ù„Ù‚Ù…ÙŠØµ Ø§Ù„Ø±Ø§Ø¦Ø¹Ø© Ø§Ù„Ù‚Ù…ÙŠØµ Ø§Ù„Ø±Ø§Ø¦Ø¹Ø© Ø§Ù„Ù‚Ù…ÙŠØµ Ø§Ù„Ø§Ø²Ø¹Ø§Ø¬ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠØ¸Ù‡Ø± ÙÙŠ Ø§Ù„
[true] query: ÙØ³ØªØ§Ù† Ø²ÙØ§Ù Ø£Ù†ÙŠÙ‚ Ø¨Ù‚ØµÙ‘Ø© Ø§Ù„Ø£Ù…ÙŠØ±Ø© Ù…Ù† Ù†Ø³ÙŠØ¬ Ø§Ù„Ù…ÙŠÙƒØ§Ø¯ÙˆØŒ Ù…ÙØ²ÙŠÙ‘Ù† Ø¨Ø§Ù„Ø£Ø²Ù‡Ø§Ø±



[pred] query: Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù…20 | MediaON Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù… Ù‚Ø§Ø³Ù…20 Ù‚Ø§Ø³Ù…20 Ù‚Ø§Ø³Ù…20 ÙŠÙ‚Ø¯Ù…
[true] query: Ø±Ø¦ÙŠØ³ Ø§Ù„Ù‚Ù…Ø© Ø§Ù„Ø¯ÙŠÙ†ÙŠØ© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¹Ø´Ø±ÙŠÙ† Ø§Ù„Ø´ÙŠØ® Ø¯.Ù…Ø­Ù…Ø¯ Ø§Ù„Ø¹ÙŠØ³Ù‰ ÙŠØ·Ù„Ù‚ Ù…Ù†ØµØ© R20 



[pred] query: Ø§Ù„Ø¬Ø§Ù…Ø¹Ø© Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙÙŠ Ø§Ù„Ø´Ø±Ù‚ Ø§Ù„Ø£ÙˆØ³Ø· ØªØ¨Ø±Ø² Ø¨Ø¯ÙˆØ± ÙƒØ¨ÙŠØ± ÙÙŠ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù† Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ù…Ø®ØªÙ„ÙØ©ØŒ
[true] query: ØªØ³Ø¹Ù‰ ÙƒÙ„ÙŠØ© Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„Ø¥Ø³Ù„Ø§Ù…ÙŠØ© Ø¥Ù„Ù‰ ØªØ¨ÙˆØ£ Ù…ÙƒØ§Ù†Ø© ÙˆØ³Ù…Ø¹Ø© Ù…Ø±Ù…ÙˆÙ‚Ø© Ø¨ÙŠÙ† Ø¬Ø§Ù…Ø¹Ø§Øª Ø§Ù„Ø¹Ø§Ù„Ù…ØŒ 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720125973/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720125973
{'eval_loss': 3.9704792499542236, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.18387364039469345, 'eval_token_set_recall': 0.2861035536183512, 'eval_token_set_f1': 0.21578238335559796, 'eval_token_set_f1_sem': 0.003835189309545716, 'eval_n_ngrams_match_1': 2.888, 'eval_n_ngrams_match_2': 1.122, 'eval_n_ngrams_match_3': 0.068, 'eval_num_true_words': 15.48, 'eval_num_pred_words': 16.804, 'eval_bleu_score': 5.278796374591676, 'eval_bleu_score_sem': 0.11403888775533001, 'eval_rouge_score': 0.5112547637968743, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8620502948760986, 'eval_emb_cos_sim_sem': 0.017798867007241613, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4409.2666, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/arb_Arab_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating amh_Ethi val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Mar 20, 2016Â· Ã®Ã°Ã»Ã­ Ã®Ã°Ã»Ã­ Ã®Ã°Ã»Ã­ ÃÃ°Ã»Ã­ Ğ°Ğ»Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ€Ğ½ÑƒÑƒĞ´Ñ‹Ğ½
[true] query: áˆ›áˆ­á‰½ 20, 2016 á‹¨áˆ«áˆµ á‹±áˆœáˆ« á‹°áˆ´á‰¶á‰½áŠ“ á‹¨á‰£áˆ…áˆ­ áŒá‹›á‰µ á‹¨áˆšá‹«áˆ³áŠ• á‹¨áŠ¤



[pred] query: Ğ¥Ò¯Ñ‡Ğ¸Ñ€Ñ…ÑĞ³ Ğ±Ğ°Ğ¹Ğ´Ğ°Ğ» - Unegui.mn Ğ¥Ò¯Ñ‡Ğ¸Ñ€Ñ…ÑĞ³ Ğ±Ğ°Ğ¹Ğ´Ğ°Ğ» Unegui.mn Ò›Ğ¾Ò“Ğ°Ğ¼
[true] query: áŠ áŠ•á‹µáŠá‰µ áŠ áŠ•á‹µáŠá‰µ á‹áˆµáŒ¥ áŒ¥áŠ•áŠ«áˆ¬ - á‹¨áˆ…á‰¥áˆ¨á‰µ áˆ›áˆ…á‰ áˆ¨áˆ°á‰¥ áŠ¥áŠ•áŠ­á‰¥áŠ«á‰¤



[pred] query: ÃƒÃ®Ã°Ã»Ã­ Ã¡Î³Ã Ã  Ñ‚Ğ¾Ğ³Ğ»Ğ¾Ğ»Ñ‚Ğ¾Ğ´ Ğ¾Ñ€Ğ¾Ğ»Ñ†ÑĞ¾Ğ½ 20 Ñ…Ò¯Ğ¼Ò¯Ò¯ÑĞ¸Ğ¹Ğ³ Ğ¼Ğ°Ñ€Ğ³Ğ°Ğ°Ñˆ Ğ±Ğ°Ñ€Ğ¸Ñ…Ğ°Ğ°Ñ€
[true] query: áŠ¨á‰¡áŠ“áˆ›á‹á‰¹ áŒ‹áˆ­ áŠáŒˆ á‹ˆá‹° á‹©áŒ‹áŠ•á‹³ á‹¨áˆšá‹«á‰€áŠ‘ 20 á‰°áŒ«á‹‹á‰¾á‰½ á‰°áˆˆá‹­á‰°á‹
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720126137/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720126137
{'eval_loss': 7.643895626068115, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.27571414935253996, 'eval_token_set_recall': 0.29391250856986195, 'eval_token_set_f1': 0.28002912216772796, 'eval_token_set_f1_sem': 0.0038839206213248626, 'eval_n_ngrams_match_1': 3.284, 'eval_n_ngrams_match_2': 1.272, 'eval_n_ngrams_match_3': 0.104, 'eval_num_true_words': 11.838, 'eval_num_pred_words': 12.814, 'eval_bleu_score': 7.687624306175386, 'eval_bleu_score_sem': 0.14339093102239608, 'eval_rouge_score': 0.5145311175188316, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8468458652496338, 'eval_emb_cos_sim_sem': 0.00654153135640138, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 127.2783, 'eval_samples_per_second': 3.928, 'eval_steps_per_second': 0.495}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/amh_Ethi_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Mar 20, 2016Â·ÃÃÃÃÃÃÃÃÃÃÃÃÃÃÃ ÃÑ Ğ¾Ñ€Ğ½Ñ‹ Ğ°Ğ»Ğ´Ğ°Ñ€Ñ‚ Ğ¾Ñ€Ğ½ÑƒÑƒĞ´Ñ‹Ğ½
[true] query: áˆ›áˆ­á‰½ 20, 2016 á‹¨áˆ«áˆµ á‹±áˆœáˆ« á‹°áˆ´á‰¶á‰½áŠ“ á‹¨á‰£áˆ…áˆ­ áŒá‹›á‰µ á‹¨áˆšá‹«áˆ³áŠ• á‹¨áŠ¤



[pred] query: Ğ¥Ò¯Ñ‡Ğ¸Ñ€Ñ…ÑĞ³ Ğ±Ğ°Ğ¹Ğ´Ğ°Ğ» - Unegui.mn Ğ¥Ò¯Ñ‡Ğ¸Ñ€Ñ…ÑĞ³ Ğ±Ğ°Ğ¹Ğ´Ğ°Ğ» Unegui.mn Ò›Ğ¾Ò“Ğ°Ğ¼
[true] query: áŠ áŠ•á‹µáŠá‰µ áŠ áŠ•á‹µáŠá‰µ á‹áˆµáŒ¥ áŒ¥áŠ•áŠ«áˆ¬ - á‹¨áˆ…á‰¥áˆ¨á‰µ áˆ›áˆ…á‰ áˆ¨áˆ°á‰¥ áŠ¥áŠ•áŠ­á‰¥áŠ«á‰¤



[pred] query: ğŸ•” 2019/10/20 ğŸ•” ğŸ•” ğŸ•” ğŸ•” ğŸ•” ğŸ•” ğŸ•” ğŸ•” ğŸ•” ğŸ•” ğŸ•” 
[true] query: áŠ¨á‰¡áŠ“áˆ›á‹á‰¹ áŒ‹áˆ­ áŠáŒˆ á‹ˆá‹° á‹©áŒ‹áŠ•á‹³ á‹¨áˆšá‹«á‰€áŠ‘ 20 á‰°áŒ«á‹‹á‰¾á‰½ á‰°áˆˆá‹­á‰°á‹
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720130614/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720130614
{'eval_loss': 7.643895626068115, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2736840177194672, 'eval_token_set_recall': 0.29043938202320585, 'eval_token_set_f1': 0.27690946575315156, 'eval_token_set_f1_sem': 0.003946307696357386, 'eval_n_ngrams_match_1': 3.274, 'eval_n_ngrams_match_2': 1.272, 'eval_n_ngrams_match_3': 0.096, 'eval_num_true_words': 11.838, 'eval_num_pred_words': 13.376, 'eval_bleu_score': 7.40333279912667, 'eval_bleu_score_sem': 0.13415122548060412, 'eval_rouge_score': 0.49427220981296627, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8394609093666077, 'eval_emb_cos_sim_sem': 0.006831863215091214, 'eval_emb_top1_equal': 0.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 4403.5271, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/amh_Ethi_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mlt_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Musculosal deficiencies - TutorialCup Musculosal deficiencies Musculosal deficiencies - technical issues and
[true] query: DÄ§ul Temi Temi Problemi muskuloskeletali Practical tools and guidance - Musculoskeletal disorders



[pred] query: 'COVID-19' â€“ a test of pneumonia in a nation Ó¨Ğ¼Ğ½Ó©Ğ´ Ğ‘ĞĞ¡Ğ£-Ğ´ Ñ…Ğ¸Ğ¹Ğ¶,
[true] query: test â€“ One News Mindu feÄ¡Ä¡ l-ewwel kaÅ¼ ta' coronavirus f'pajjiÅ¼na, saru aktar



[pred] query: "I'm'n'n'n'n'n'n'n'n'n'n mob Ñ‚Ğ¾Ğ¹Ñ€ÑƒÑƒĞ»Ğ´Ğ°Ğ³
[true] query: 'L-MFA emmnet lil min ibagÄ§bas il-logÄ§ob u mhux lili' - Illum
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720130776/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720130776
{'eval_loss': 5.393593788146973, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2612796244605069, 'eval_token_set_recall': 0.3399728564692969, 'eval_token_set_f1': 0.29112577151392416, 'eval_token_set_f1_sem': 0.003933074414474725, 'eval_n_ngrams_match_1': 3.716, 'eval_n_ngrams_match_2': 1.266, 'eval_n_ngrams_match_3': 0.124, 'eval_num_true_words': 13.932, 'eval_num_pred_words': 12.402, 'eval_bleu_score': 7.4003239889711985, 'eval_bleu_score_sem': 0.14717719147365438, 'eval_rouge_score': 0.1676830200338712, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9013442993164062, 'eval_emb_cos_sim_sem': 0.01191017123648325, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 125.768, 'eval_samples_per_second': 3.976, 'eval_steps_per_second': 0.501}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/mlt_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Musculosal deficiencies - TutorialCup Musculosal deficiencies Musculosal deficiencies - fundamental issues and solutions
[true] query: DÄ§ul Temi Temi Problemi muskuloskeletali Practical tools and guidance - Musculoskeletal disorders



[pred] query: 'COVID-19' â€“ a first test of pneumonia in a nation Ó¨Ğ¼Ğ½Ó©Ğ´ Ğ¼ÑĞ½Ğ³Ğ°Ğ½ Ğ¼ÑĞ½Ğ³Ğ°Ğ½,
[true] query: test â€“ One News Mindu feÄ¡Ä¡ l-ewwel kaÅ¼ ta' coronavirus f'pajjiÅ¼na, saru aktar



[pred] query: "I'm'n'n'n'n'n'n'n'n'n'n'n' mob
[true] query: 'L-MFA emmnet lil min ibagÄ§bas il-logÄ§ob u mhux lili' - Illum
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720135250/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720135250
{'eval_loss': 5.393593788146973, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2626525072536685, 'eval_token_set_recall': 0.341921873147493, 'eval_token_set_f1': 0.2914816976936931, 'eval_token_set_f1_sem': 0.003960905338273623, 'eval_n_ngrams_match_1': 3.712, 'eval_n_ngrams_match_2': 1.28, 'eval_n_ngrams_match_3': 0.126, 'eval_num_true_words': 13.932, 'eval_num_pred_words': 12.632, 'eval_bleu_score': 7.185982611092534, 'eval_bleu_score_sem': 0.12896713353848538, 'eval_rouge_score': 0.1626364418417856, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8989600539207458, 'eval_emb_cos_sim_sem': 0.0107882260803903, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4400.4914, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/mlt_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating hin_Deva val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: ISLAMIC BLOG: ISLAMIC BLOG: ISLAMIC BLOG: ÑĞ»Ğ°Ğ³Ğ´Ğ°Ğ»Ğ³Ò¯Ğ¹ ÑĞ»Ğ°Ğ³Ğ´Ğ°Ğ»Ğ³Ò¯Ğ¹ ÑĞ»Ğ°Ğ³Ğ´Ğ°Ğ»Ğ³Ò¯Ğ¹ 
[true] query: Blog News: à¤—à¤¼à¤²à¤¤à¥€ à¤¨ à¤•à¤°à¥‡ à¤®à¥à¤¸à¤²à¤®à¤¾à¤¨!!! à¤—à¤¼à¤²à¤¤à¥€ à¤¨ à¤•à¤°à¥‡ à¤®à¥à¤¸à¤²à¤®à¤¾à¤¨!!! 



[pred] query: Algebra 9 ÑÑ‹Ğ½Ñ‹Ğ¿ - NSP Solutions Algebra 9 ÑÑ‹Ğ½Ñ‹Ğ¿ - NSP Solutions Algebra 9 ÑÑ‹Ğ½Ñ‹Ğ¿ for students with
[true] query: RBSE Solutions for Class 9 Hindi Sparsh Chapter 11 à¤†à¤¦à¤®à¥€ à¤¨à¤¾à¤®à¤¾ - Rbse solutions RBSE Solutions for Class 9



[pred] query: ĞĞ´Ğ¾Ğ¾Ğ³Ğ¾Ğ¾Ñ€ Ğ¼Ğ¾Ğ½Ğ³Ğ¾Ğ» ÑƒĞ»ÑÑ‹Ğ½ Ğ•Ñ€Ó©Ğ½Ñ…Ğ¸Ğ¹ ÑĞ°Ğ¹Ğ´ Ğ±Ğ¾Ğ»Ğ¾Ñ… ĞĞ¸ĞºĞ¾Ğ»Ğ°Ñ ĞœĞ°Ğ´ÑƒÑ€Ğ¾ 14 ÑĞ°Ñ€Ğ°Ğ°Ñ Ğ´ÑÑÑˆ Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ° Ğ½ÑŒ
[true] query: à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€, à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤¨à¤°à¥‡à¤‚à¤¦à¥à¤° à¤®à¥‹à¤¦à¥€ à¤•à¤¾ à¤•à¤¾à¤°à¥à¤¯à¤•à¤¾à¤² à¤ªà¥‚à¤°à¤¾ à¤¹à¥‹à¤¨à¥‡ à¤®à¥‡à¤‚ à¤•à¥‡à¤µà¤² 14 à¤®à¤¹à¥€à¤¨à¥‡ à¤•à¤¾ à¤µà¤•à¥à¤¤ à¤¬à¤¾
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720135412/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720135412
{'eval_loss': 5.1107892990112305, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.21262983958578968, 'eval_token_set_recall': 0.29085585840028627, 'eval_token_set_f1': 0.2412473939266356, 'eval_token_set_f1_sem': 0.004206394692860702, 'eval_n_ngrams_match_1': 3.794, 'eval_n_ngrams_match_2': 1.296, 'eval_n_ngrams_match_3': 0.136, 'eval_num_true_words': 17.206, 'eval_num_pred_words': 16.21, 'eval_bleu_score': 5.871474848464906, 'eval_bleu_score_sem': 0.16008760962137816, 'eval_rouge_score': 0.4043578987837747, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.895817220211029, 'eval_emb_cos_sim_sem': 0.00816069210400046, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 125.0211, 'eval_samples_per_second': 3.999, 'eval_steps_per_second': 0.504}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/hin_Deva_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: ISLAMIC BLOG: ISLAMIC BLOG: ISLAMIC BLOG: ÑĞ»Ğ°Ğ³Ğ´Ğ°ÑˆĞ³Ò¯Ğ¹ ÑĞ»Ğ°Ğ³Ğ´Ğ°ÑˆĞ³Ò¯Ğ¹ ÑĞ»Ğ°Ğ³Ğ´Ğ°
[true] query: Blog News: à¤—à¤¼à¤²à¤¤à¥€ à¤¨ à¤•à¤°à¥‡ à¤®à¥à¤¸à¤²à¤®à¤¾à¤¨!!! à¤—à¤¼à¤²à¤¤à¥€ à¤¨ à¤•à¤°à¥‡ à¤®à¥à¤¸à¤²à¤®à¤¾à¤¨!!! 



[pred] query: Smash Chapter 9 - RBSE Solutions for Beginners Smash Chapter 9 - RBSE Solutions for Beginners
[true] query: RBSE Solutions for Class 9 Hindi Sparsh Chapter 11 à¤†à¤¦à¤®à¥€ à¤¨à¤¾à¤®à¤¾ - Rbse solutions RBSE Solutions for Class 9



[pred] query: ĞœĞ¾Ğ½Ğ³Ğ¾Ğ» ÑƒĞ»ÑÑ‹Ğ½ Ğ•Ñ€Ó©Ğ½Ñ…Ğ¸Ğ¹Ğ»Ó©Ğ³Ñ‡ ĞĞ¸ĞºĞ¾Ğ»Ğ°Ñ ĞœĞ°Ğ´ÑƒÑ€Ğ¾ Ğ¾Ğ´Ğ¾Ğ¾Ğ³Ğ¸Ğ¹Ğ½ Ğ±Ğ°Ğ¹Ğ´Ğ»Ğ°Ğ°Ñ€ 14 ÑĞ°Ñ€Ğ°Ğ°Ñ 14 ÑĞ°Ñ€ Ñ…Ò¯Ñ€Ñ‚ÑĞ»Ñ… Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ° Ğ±Ğ¾Ğ»
[true] query: à¤¨à¤ˆ à¤¦à¤¿à¤²à¥à¤²à¥€, à¤ªà¥à¤°à¤§à¤¾à¤¨à¤®à¤‚à¤¤à¥à¤°à¥€ à¤¨à¤°à¥‡à¤‚à¤¦à¥à¤° à¤®à¥‹à¤¦à¥€ à¤•à¤¾ à¤•à¤¾à¤°à¥à¤¯à¤•à¤¾à¤² à¤ªà¥‚à¤°à¤¾ à¤¹à¥‹à¤¨à¥‡ à¤®à¥‡à¤‚ à¤•à¥‡à¤µà¤² 14 à¤®à¤¹à¥€à¤¨à¥‡ à¤•à¤¾ à¤µà¤•à¥à¤¤ à¤¬à¤¾
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720139907/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720139907
{'eval_loss': 5.1107892990112305, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2180028655596063, 'eval_token_set_recall': 0.2883410244811951, 'eval_token_set_f1': 0.24296569917183747, 'eval_token_set_f1_sem': 0.004521022147645582, 'eval_n_ngrams_match_1': 3.88, 'eval_n_ngrams_match_2': 1.324, 'eval_n_ngrams_match_3': 0.18, 'eval_num_true_words': 17.206, 'eval_num_pred_words': 16.666, 'eval_bleu_score': 5.991327742341231, 'eval_bleu_score_sem': 0.19691260220257514, 'eval_rouge_score': 0.40036077107162604, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8863050937652588, 'eval_emb_cos_sim_sem': 0.012162503101428957, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4421.7825, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/hin_Deva_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating urd_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: ĞĞ°Ñ€Ğ°Ğ½ Ñ…Ò¯Ñ€Ñ‚ÑĞ»Ñ… Ğ·Ğ°ÑĞ°Ğ³Ğ»Ğ°Ğ» | à¦®à§‚à¦² à¦ªà¦¾à¦¤à¦¾ / à¦®à§‚à¦² à¦ªà¦¾à¦¤à¦¾ / ĞĞ°Ñ€Ğ°Ğ½ Ñ…Ò¯Ñ€Ñ‚ÑĞ»Ñ… Ğ·Ğ°ÑĞ°Ğ³
[true] query: Ù‚Ø§Ø¦Ø¯Ø§Ø¹Ø¸Ù… Ø³Û’ Ù†ÛØ±Ùˆ ØªÚ© Û”Û” Ø±Ø¤Ù Ú©Ù„Ø§Ø³Ø±Ø§ Ù…Ø±Ú©Ø²ÛŒ ØµÙØ­Û/ Ù„Ú©Ú¾Ø§Ø±ÛŒ/ Ø±



[pred] query: Samanta Sukiya Ó©Ó©Ñ€Ğ¸Ğ¹Ğ½ Instagram Ó™Ğ»ĞµÑƒĞ¼ĞµÑ‚Ñ‚Ñ–Ğº Ğ¼ĞµĞ´Ğ¸Ğ°Ğ´Ğ°Ğ½ Ğ±Ğ¸Ñˆ ÑĞ¾ÑˆĞ¸Ğ°Ğ» Ğ¾Ñ€Ñ‡Ğ¸Ğ½Ğ´ Ğ´Ğ° Ñ…Ò¯Ñ‡Ğ¸Ñ€Ñ…Ğ¸Ğ¹Ğ»ÑĞ»Ğ´ Ó©Ñ€Ñ‚ÑÓ©Ğ½ 
[true] query: Ø³ÙˆÙ†Ø§Ú©Ø´ÛŒ Ø³Ù†ÛØ§ Ø³ÙˆØ´Ù„ Ù…ÛŒÚˆÛŒØ§ Ù…ÛŒÙ…Ø² Ú©Û’ Ù†Ø´Û’ Ù…ÛŒÚº Ù…Ø¨ØªÙ„Ø§ ÛÙˆ Ú¯Ø¦ÛŒÚº Ø§Ø¯Ø§Ú©Ø§Ø±Û Ù†Û ØµØ±Ù Ø®ÙˆØ¯



[pred] query: : : : : : : : : : Marrakech Ó¨Ğ¼Ğ½Ó©Ñ… 18 Ğ¶Ğ¸Ğ»Ğ¸Ğ¹Ğ½ Ñ…ÑƒĞ³Ğ°Ñ†
[true] query: Ù†Ø¦ÛŒ Ø¯ÛÙ„ÛŒ:Ù…ÛØ§Ø±Ø§Ø´Ù¹Ø± Ù…ÛŒÚº Ú¯Ø²Ø´ØªÛ 5 Ø³Ø§Ù„ÙˆÚº Ù…ÛŒÚº (18-2014)14034 Ú©Ø³Ø§Ù†ÙˆÚº Ù†Û’ Ø®ÙˆØ¯
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720140069/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720140069
{'eval_loss': 6.230568885803223, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.17767079967900104, 'eval_token_set_recall': 0.2492135895837911, 'eval_token_set_f1': 0.20353514839823264, 'eval_token_set_f1_sem': 0.003193085975965814, 'eval_n_ngrams_match_1': 2.946, 'eval_n_ngrams_match_2': 1.132, 'eval_n_ngrams_match_3': 0.056, 'eval_num_true_words': 17.024, 'eval_num_pred_words': 14.806, 'eval_bleu_score': 5.633329015789659, 'eval_bleu_score_sem': 0.13273114082394236, 'eval_rouge_score': 0.5660749128205369, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8868821859359741, 'eval_emb_cos_sim_sem': 0.007225861833138671, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 124.7318, 'eval_samples_per_second': 4.009, 'eval_steps_per_second': 0.505}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/urd_Arab_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: ĞÒ¯Ò¯Ñ€ Ñ…ÑƒÑƒĞ´Ğ°Ñ / ĞĞ°Ñ€Ğ¸Ğ¹Ğ½ Ğ±Ğ¸Ñ‡Ğ³Ğ¸Ğ¹Ğ½ Ğ´Ğ°Ñ€Ğ³Ğ° Ñ…Ò¯Ñ€Ñ‚ÑĞ» / ĞĞ°Ñ€Ğ¸Ğ¹Ğ½ Ğ±Ğ¸Ñ‡Ğ³Ğ¸Ğ¹Ğ½ Ğ´Ğ°Ñ€Ğ³Ğ° Ñ…Ò¯Ñ€Ñ‚ÑĞ» / Ù†Ø±ÛŒ
[true] query: Ù‚Ø§Ø¦Ø¯Ø§Ø¹Ø¸Ù… Ø³Û’ Ù†ÛØ±Ùˆ ØªÚ© Û”Û” Ø±Ø¤Ù Ú©Ù„Ø§Ø³Ø±Ø§ Ù…Ø±Ú©Ø²ÛŒ ØµÙØ­Û/ Ù„Ú©Ú¾Ø§Ø±ÛŒ/ Ø±



[pred] query: Samia Sunkash Ó©Ó©Ñ€Ğ¸Ğ¹Ğ½ Instagram Ó™Ğ»ĞµÑƒĞ¼ĞµÑ‚Ñ‚Ñ–Ğº Ğ¼ĞµĞ´Ğ¸Ğ°Ğ´Ğ°Ğ½ Ğ±Ğ¸Ñˆ ÑĞ¾ÑˆĞ¸Ğ°Ğ» Ğ¼ĞµĞ´Ğ¸Ğ°Ğ´Ğ°Ğ½ Ğ¸Ñ…ÑÑÑ…ÑĞ½ Ó©Ñ€Ñ‚ÑÓ©Ğ½ Ğ±Ó©Ğ³Ó©Ó©Ğ´
[true] query: Ø³ÙˆÙ†Ø§Ú©Ø´ÛŒ Ø³Ù†ÛØ§ Ø³ÙˆØ´Ù„ Ù…ÛŒÚˆÛŒØ§ Ù…ÛŒÙ…Ø² Ú©Û’ Ù†Ø´Û’ Ù…ÛŒÚº Ù…Ø¨ØªÙ„Ø§ ÛÙˆ Ú¯Ø¦ÛŒÚº Ø§Ø¯Ø§Ú©Ø§Ø±Û Ù†Û ØµØ±Ù Ø®ÙˆØ¯



[pred] query: : : : : : : : : : : Marrakech Ó¨Ğ¼Ğ½Ó©Ñ… 2018 Ğ¾Ğ½Ğ´
[true] query: Ù†Ø¦ÛŒ Ø¯ÛÙ„ÛŒ:Ù…ÛØ§Ø±Ø§Ø´Ù¹Ø± Ù…ÛŒÚº Ú¯Ø²Ø´ØªÛ 5 Ø³Ø§Ù„ÙˆÚº Ù…ÛŒÚº (18-2014)14034 Ú©Ø³Ø§Ù†ÙˆÚº Ù†Û’ Ø®ÙˆØ¯
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720144556/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720144556
{'eval_loss': 6.230568885803223, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.17800987916975308, 'eval_token_set_recall': 0.24760651047439, 'eval_token_set_f1': 0.20297804313611822, 'eval_token_set_f1_sem': 0.0033964035025221473, 'eval_n_ngrams_match_1': 2.944, 'eval_n_ngrams_match_2': 1.14, 'eval_n_ngrams_match_3': 0.066, 'eval_num_true_words': 17.024, 'eval_num_pred_words': 15.086, 'eval_bleu_score': 5.627024265273809, 'eval_bleu_score_sem': 0.13999207851583179, 'eval_rouge_score': 0.5414537386523074, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8940063714981079, 'eval_emb_cos_sim_sem': 0.005208636284427594, 'eval_emb_top1_equal': 0.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 4412.3775, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/urd_Arab_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating guj_Gujr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: "ĞœĞ°Ğ½Ğ°Ğ¹ Ğ½Ó©Ñ…Ó©Ñ€ Ò¯Ò¯?"-Ğ½ Ğ°ÑÑƒÑƒĞ»Ñ‚Ñ‹Ğ½ Ğ´Ğ°Ñ€Ğ°Ğ° Ğ³ÑÑ€Ğ³Ğ¸Ğ¹Ğ´ÑÑ Ğ½Ó©Ñ…Ó©Ñ€Ñ‚ Ğ½ÑÑ€ÑÑ Ñ‚Ğ°Ğ²ÑŒÑĞ°Ğ½
[true] query: 'àª…àª®àª¾àª°àª¾ àªµàª¿àª°à«‹àª§à«€àª¨à«‡ àª²àª—à«àª¨àª®àª¾àª‚ àª•à«‡àª® àª¬à«‹àª²àª¾àªµà«àª¯à«‹?' àª•àª¹à«€ àª•àª¨à«àª¯àª¾àª¨àª¾ àª®àª¾-àª¬àª¾



[pred] query: Today Nazarbayev University-Ğ°Ğ°Ñ Ğ±Ò¯Ñ…Ğ¸Ğ¹ Ğ» Ñ…Ó©Ñ‚Ó©Ğ»Ğ±Ó©Ñ€Ó©Ó© ÑƒĞ½ÑˆĞ°Ğ°Ñ€Ğ°Ğ¹, ÑĞ½Ñ Ó©Ğ´Ñ€Ğ¸Ğ¹Ğ³ Ñ…Ò¯Ñ€Ñ‚ÑĞ» Nazarbayev University
[true] query: àª†àªœà«‡ àª¸àª¾àª‚àªœàª¥à«€ àª•à«‡àªœàª°à«€àªµàª¾àª²àª¨à«€ àª—à«àªœàª°àª¾àª¤ àª¯àª¾àª¤à«àª°àª¾ àª¶àª°à«‚, àªœàª¾àª£à«‹ àª†àª–à«‹ àª•àª¾àª°à«àª¯àª•à«àª°àª® | Read here



[pred] query: Shaqran Khan Ğ´ÑÑÑ€Ñ… Ğ¼Ğ¾Ğ½Ğ³Ğ¾Ğ» Ñ…Ò¯Ğ½Ğ¸Ğ¹Ğ³ Ğ½ÑÑ€Ğ»ÑĞ½Ñ | Buro 24/7 Shaqran Khan Ğ´ÑÑÑ€Ñ… 
[true] query: àª«àª°àª¹àª¾àª¨àª¨à«€ àª¡à«‰àª¨ 3àª®àª¾àª‚ àªœà«‹àªµàª¾ àª®àª³àª¶à«‡ àª—à«àªœàª°àª¾àª¤à«€ àª¶àª¾àª¹àª°à«àª– àª–àª¾àª¨ | Shahrukh Kahn
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720144719/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720144719
{'eval_loss': 8.638017654418945, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.23064592709492132, 'eval_token_set_recall': 0.2427658411849595, 'eval_token_set_f1': 0.23450151996218174, 'eval_token_set_f1_sem': 0.0032942552472671784, 'eval_n_ngrams_match_1': 3.112, 'eval_n_ngrams_match_2': 1.09, 'eval_n_ngrams_match_3': 0.038, 'eval_num_true_words': 13.074, 'eval_num_pred_words': 13.66, 'eval_bleu_score': 6.573176016278831, 'eval_bleu_score_sem': 0.08092481039387982, 'eval_rouge_score': 0.597354502231515, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9181680083274841, 'eval_emb_cos_sim_sem': 0.0037088788757488936, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 126.7543, 'eval_samples_per_second': 3.945, 'eval_steps_per_second': 0.497}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/guj_Gujr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: "ĞœĞ°Ğ½Ğ°Ğ¹ Ğ³ÑÑ€Ğ³Ğ¸Ğ¹ Ò¯Ò¯?"-Ğ½ Ğ°ÑÑƒÑƒĞ»Ñ‚Ñ‹Ğ½ Ğ´Ğ°Ñ€Ğ°Ğ° Ğ½Ó©Ñ…Ó©Ñ€Ñ‚ Ğ½Ó©Ñ…Ó©Ñ€Ñ‚ Ğ·Ğ°Ñ…Ğ¸Ğ´Ğ»Ğ°Ğ° Ó©Ğ³ÑÓ©Ğ½ Kaka
[true] query: 'àª…àª®àª¾àª°àª¾ àªµàª¿àª°à«‹àª§à«€àª¨à«‡ àª²àª—à«àª¨àª®àª¾àª‚ àª•à«‡àª® àª¬à«‹àª²àª¾àªµà«àª¯à«‹?' àª•àª¹à«€ àª•àª¨à«àª¯àª¾àª¨àª¾ àª®àª¾-àª¬àª¾



[pred] query: Today Nazarbayev University-Ğ°Ğ°Ñ Ğ´Ğ°Ñ€Ğ°Ğ°Ñ… Ò¯Ğ¹Ğ» Ğ°Ğ¶Ğ¸Ğ»Ğ»Ğ°Ğ³Ğ°Ğ°Ğ³ ÑƒĞ½ÑˆĞ°Ğ°Ñ€Ğ°Ğ¹! Ğ¦Ğ°Ğ³ Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ° Ó©Ğ½Ğ³Ó©Ñ€ÑÓ©Ğ½, ÑĞ½ÑÑ…Ò¯Ò¯
[true] query: àª†àªœà«‡ àª¸àª¾àª‚àªœàª¥à«€ àª•à«‡àªœàª°à«€àªµàª¾àª²àª¨à«€ àª—à«àªœàª°àª¾àª¤ àª¯àª¾àª¤à«àª°àª¾ àª¶àª°à«‚, àªœàª¾àª£à«‹ àª†àª–à«‹ àª•àª¾àª°à«àª¯àª•à«àª°àª® | Read here



[pred] query: Ğ”Ğ°Ñ€Ñ…Ğ°Ğ½ Ğ³Ò¯Ğ½Ğ¶ Shaqran Khan Ğ´ÑÑÑ€ Ğ³Ğ°Ñ€Ñ‡ Ğ¸Ñ€Ğ½Ñ | Buro 24/7 Ğ”Ğ°Ñ€Ñ…Ğ°Ğ½ Ğ³Ò¯Ğ½Ğ¶ Shaqran
[true] query: àª«àª°àª¹àª¾àª¨àª¨à«€ àª¡à«‰àª¨ 3àª®àª¾àª‚ àªœà«‹àªµàª¾ àª®àª³àª¶à«‡ àª—à«àªœàª°àª¾àª¤à«€ àª¶àª¾àª¹àª°à«àª– àª–àª¾àª¨ | Shahrukh Kahn
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720149224/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720149224
{'eval_loss': 8.638017654418945, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22954565987847764, 'eval_token_set_recall': 0.23816881255998934, 'eval_token_set_f1': 0.23192333255514022, 'eval_token_set_f1_sem': 0.0033302444219945808, 'eval_n_ngrams_match_1': 3.088, 'eval_n_ngrams_match_2': 1.1, 'eval_n_ngrams_match_3': 0.04, 'eval_num_true_words': 13.074, 'eval_num_pred_words': 13.794, 'eval_bleu_score': 6.570204186196554, 'eval_bleu_score_sem': 0.08487037046869496, 'eval_rouge_score': 0.5812704967971098, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9066624641418457, 'eval_emb_cos_sim_sem': 0.012264145494969042, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 4431.0726, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/guj_Gujr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating sin_Sinh val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Ğ¥Ò¯Ğ¼Ò¯Ò¯Ñ Ğ°Ğ»Ñ…Ğ°Ğ¶ Ğ±Ğ°Ğ¹Ñ…Ğ´Ğ°Ğ° | Ğ¢Ğ°Ğ³Ñ‚Ğ°Ğ° ĞŸĞ°Ğ±Ğ»Ğ¸ÑˆĞ¸Ğ½Ğ³ Ğ¥Ò¯Ğ¼Ò¯Ò¯Ñ Ğ°Ğ»Ñ…Ğ°Ğ¶ Ğ±Ğ°Ğ¹Ñ…Ğ´Ğ°Ğ° 16 Ğ¾Ğ½Ñ‹ 01 ÑĞ°Ñ€Ñ‹Ğ½
[true] query: à¶¸à·’à¶±à·’à·ƒà·Šà·ƒà·” | à¶‡à·€à·’à¶¯ à¶ºà¶± à¶¸à¶Ÿ â† à¶œà¶»à·” à¶šà·’à¶»à·“à¶¸ à¶œà·”à¶»à·”à·€à¶»à·” â†’ 16 à¶…à¶´à·Š à¶»à·šà¶½à·Š à¶…à¶´à·’à¶§ à¶šà·™à¶±à·™à¶šà·Š à¶¯à·à¶šà·Šà¶šà·à¶¸ à¶†



[pred] query: Ğ¡Ğ¸Ñ†Ğ¸Ğ»Ğ¸ Ñ†ÑÑ†ÑĞ³ | Ğ¡Ğ¸Ñ†Ğ¸Ğ»Ğ¸ Ñ†ÑÑ†ÑĞ³ | Ğ¡Ğ¸Ñ†Ğ¸Ğ»Ğ¸ Ñ†ÑÑ†ÑĞ³ Posted on July 24, 2017 July 24, 2017
[true] query: à·ƒà·’à·€à·”à¶¸à·à¶½à·’à¶ºà· à·ƒà·’à¶šà·”à¶»à·” à¶½à·’à¶ºà·... | Sunday Apple à·ƒà·’à·€à·”à¶¸à·à¶½à·’à¶ºà· à·ƒà·’à¶šà·”à¶»à·” à¶½à·’à¶ºà·... March 24, 2017 | 11:00 am 0



[pred] query: ĞÒ¯Ñ†Ğ³ÑĞ½ Ğ±Ğ°Ğ¹Ñ…Ñ‹Ğ½ Ò¯ĞµĞ´ - Sri Lanka News ĞÒ¯Ñ†Ğ³ÑĞ½ Ğ±Ğ°Ğ¹Ñ…Ñ‹Ğ½ Ò¯ĞµĞ´ - Sri Lanka News
[true] query: à¶±à·’à·…à·’à¶šà¶¸à¶§ à¶¸à·à¶¯à·’ à·€à·”à¶«à·š à¶´à·”à¶‚à¶ à·’ à¶šà·™à¶½à·Šà¶½à¶š à¶šà·à¶½à·š... - Sri Lanka News Update à¶±à·’à·…à·’à¶šà¶¸à¶§ à¶¸à·
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720149393/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720149393
{'eval_loss': 8.07994270324707, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2500184542066121, 'eval_token_set_recall': 0.29932641312935465, 'eval_token_set_f1': 0.2693887031316436, 'eval_token_set_f1_sem': 0.004092325821401617, 'eval_n_ngrams_match_1': 3.614, 'eval_n_ngrams_match_2': 1.32, 'eval_n_ngrams_match_3': 0.132, 'eval_num_true_words': 14.694, 'eval_num_pred_words': 14.26, 'eval_bleu_score': 6.889719511453351, 'eval_bleu_score_sem': 0.1521945278393701, 'eval_rouge_score': 0.570307778035593, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.905841052532196, 'eval_emb_cos_sim_sem': 0.003966977614951005, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 131.5974, 'eval_samples_per_second': 3.799, 'eval_steps_per_second': 0.479}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/sin_Sinh_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ğ¥Ò¯Ğ¼Ò¯Ò¯Ñ Ğ°Ğ»Ñ…Ğ°Ğ¶ Ğ±Ğ°Ğ¹Ñ…Ğ´Ğ°Ğ° | Ğ¢Ğ°Ğ³Ñ‚Ğ°Ğ° ĞŸĞ°Ğ±Ğ»Ğ¸ÑˆĞ¸Ğ½Ğ³ Ğ¥Ò¯Ğ¼Ò¯Ò¯Ñ Ğ°Ğ»Ñ…Ğ°Ğ¶ Ğ±Ğ°Ğ¹Ñ…Ğ´Ğ°Ğ° Posted on 16/01/20
[true] query: à¶¸à·’à¶±à·’à·ƒà·Šà·ƒà·” | à¶‡à·€à·’à¶¯ à¶ºà¶± à¶¸à¶Ÿ â† à¶œà¶»à·” à¶šà·’à¶»à·“à¶¸ à¶œà·”à¶»à·”à·€à¶»à·” â†’ 16 à¶…à¶´à·Š à¶»à·šà¶½à·Š à¶…à¶´à·’à¶§ à¶šà·™à¶±à·™à¶šà·Š à¶¯à·à¶šà·Šà¶šà·à¶¸ à¶†



[pred] query: Ğ¡Ğ¸Ñ†Ğ¸Ğ»Ğ¸ Ñ†ÑÑ†ÑĞ³ | Ğ¡Ğ¸Ñ†Ğ¸Ğ»Ğ¸ Ñ†ÑÑ†ÑĞ³ | Ğ¡Ğ¸Ñ†Ğ¸Ğ»Ğ¸ Ñ†ÑÑ†ÑĞ³ Posted on July 24, 2017 July 24, 2017
[true] query: à·ƒà·’à·€à·”à¶¸à·à¶½à·’à¶ºà· à·ƒà·’à¶šà·”à¶»à·” à¶½à·’à¶ºà·... | Sunday Apple à·ƒà·’à·€à·”à¶¸à·à¶½à·’à¶ºà· à·ƒà·’à¶šà·”à¶»à·” à¶½à·’à¶ºà·... March 24, 2017 | 11:00 am 0



[pred] query: Ğ—Ğ°Ğ»ÑƒÑƒ Ğ±Ğ°Ğ¹Ñ…Ñ‹Ğ½ Ó©Ó©Ğ´Ñ€Ó©Ğ³ Ò¯ĞµĞ´... - Sri Lanka News Ğ—Ğ°Ğ»ÑƒÑƒ Ğ±Ğ°Ğ¹Ñ…Ñ‹Ğ½ Ó©Ó©Ğ´Ñ€Ó©Ğ³ Ò¯ĞµĞ´
[true] query: à¶±à·’à·…à·’à¶šà¶¸à¶§ à¶¸à·à¶¯à·’ à·€à·”à¶«à·š à¶´à·”à¶‚à¶ à·’ à¶šà·™à¶½à·Šà¶½à¶š à¶šà·à¶½à·š... - Sri Lanka News Update à¶±à·’à·…à·’à¶šà¶¸à¶§ à¶¸à·
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720153885/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720153885
{'eval_loss': 8.07994270324707, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2483815324768112, 'eval_token_set_recall': 0.2963250480857295, 'eval_token_set_f1': 0.26699657480165473, 'eval_token_set_f1_sem': 0.004074144973396083, 'eval_n_ngrams_match_1': 3.574, 'eval_n_ngrams_match_2': 1.318, 'eval_n_ngrams_match_3': 0.13, 'eval_num_true_words': 14.694, 'eval_num_pred_words': 14.35, 'eval_bleu_score': 6.963808694860665, 'eval_bleu_score_sem': 0.16793357271086454, 'eval_rouge_score': 0.5571047729849761, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9010547995567322, 'eval_emb_cos_sim_sem': 0.005486433406952485, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4417.8994, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/sin_Sinh_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating pan_Guru val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Ò®Ğ¹Ğ»Ñ‡Ğ¸Ğ»Ğ³ÑÑĞ½Ğ¸Ğ¹ Ñ‚Ó©Ğ»Ó©Ğ²Ğ»Ó©Ğ³Ó©Ó©Ğ³Ó©Ó©Ñ€ Ğ·Ğ¾Ğ³ÑĞ¾Ğ¶ Ğ±Ğ°Ğ¹Ğ³Ğ°Ğ° ĞšĞ°Ñ‚Ğ°Ñ€Ñ‹Ğ½ Ğ·Ğ°Ñ… Ğ·ÑÑĞ»Ò¯Ò¯Ğ´ÑÑÑ€ Ñ†Ğ°Ñ†Ğ°Ğ³Ñ‚ Ñ…ÑÑ€ÑƒÑƒĞ»
[true] query: à¨•à¨°à©‹à¨¨à¨¾ à¨®à¨¹à¨¾à¨‚à¨®à¨¾à¨°à©€ à¨¦à©‡ à¨šà©±à¨²à¨¦à©‡ à¨«à©‹à¨Ÿà©‹à¨—à©à¨°à¨¾à¨«à¨° à¨¦à©€à¨†à¨‚ à¨¬à©°à¨¦ à¨ªà¨ˆà¨†à¨‚ à¨¦à©à¨•à¨¾à¨¨à¨¾à¨‚ à¨–à©‹



[pred] query: ĞŸĞ°Ğ¹Ò“Ğ°Ğ¼Ğ±Ğ°Ñ€Ñ‹Ğ½ Ğ±Ò¯Ñ‚ÑÑĞ½ Ğ±Ğ°Ğ¹Ğ³ÑƒÑƒĞ»Ğ°Ğ»Ñ‚ Ñ…Ğ¸Ğ¹Ñ…ÑÑÑ Ó©Ğ¼Ğ½Ó©Ñ… Ñ…Ò¯ÑÑĞ»Ñ‚ Ò¯Ò¯ÑĞ³ÑĞ³Ğ´ÑÑ… - ĞœĞĞĞ“ĞĞ› Ğ£Ğ›Ğ¡ -
[true] query: à¨®à©‹à¨¦à©€ à¨¦à©‡ à¨¸à©à¨ªà¨¨à¨¿à¨†à¨‚ à¨¦à¨¾ à¨¯à©‚ à¨ªà©€ à¨¬à¨£à¨¾à¨‰à¨£ à¨²à¨ˆ à¨•à¨µà¨¾à¨‡à¨¦ à¨†à¨°à©°à¨­ - Panja



[pred] query: ĞĞ»Ğ±Ğ°Ğ½ Ñ‘ÑĞ½Ñ‹ Ğ±Ğ°Ğ¹ÑˆĞ¸Ğ½ Ğ´ÑÑÑ€Ñ… Ğ·ÑƒÑ€Ğ°Ğ³ Ğ·ÑƒÑ€Ğ°Ğ³ Ó©Ğ³Ó©Ñ… ÑˆĞ°Ğ°Ñ€Ğ´Ğ»Ğ°Ğ³Ñ‹Ğ³ Ñ…Ğ°Ğ½Ğ³Ğ°Ñ… : : 
[true] query: à¨¸à¨¼à¨¹à©€à¨¦à¨¾à¨‚ à¨¦à©€à¨†à¨‚ à¨¤à¨¸à¨µà©€à¨°à¨¾à¨‚ à¨…à¨œà¨¾à¨‡à¨¬ à¨˜à¨° 'à¨š à¨²à¨—à¨¾à¨‰à¨£ à¨¦à©€ à¨®à©°à¨— :
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720154053/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720154053
{'eval_loss': 7.848777770996094, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2348003445385027, 'eval_token_set_recall': 0.25957628139834044, 'eval_token_set_f1': 0.24441659234567648, 'eval_token_set_f1_sem': 0.003623722487725529, 'eval_n_ngrams_match_1': 3.05, 'eval_n_ngrams_match_2': 1.106, 'eval_n_ngrams_match_3': 0.044, 'eval_num_true_words': 13.07, 'eval_num_pred_words': 13.378, 'eval_bleu_score': 6.624230980897483, 'eval_bleu_score_sem': 0.06571969051171118, 'eval_rouge_score': 0.6733197640079227, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8944419622421265, 'eval_emb_cos_sim_sem': 0.010915666955030608, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 131.6078, 'eval_samples_per_second': 3.799, 'eval_steps_per_second': 0.479}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/pan_Guru_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ğ¦Ğ°Ğ³ Ñ…ÑƒĞ³Ğ°Ñ†Ğ°Ğ°Ğ½Ñ‹ Ñ…ÑĞ¼Ñ€Ğ°Ğ»Ñ‹Ğ½ ÑƒĞ»Ğ¼Ğ°Ğ°Ñ Ñ…ÑƒĞ´Ğ°Ğ»Ğ´Ğ°Ğ°Ğ½Ñ‹ Ñ‚Ó©Ñ…Ó©Ó©Ñ€Ó©Ğ¼Ğ¶Ò¯Ò¯Ğ´ÑÑÑ€ Ğ·Ğ°ÑĞ²Ğ°Ñ€ Ñ…Ğ¸Ğ¹Ğ¶ ÑĞ²Ğ°Ğ°
[true] query: à¨•à¨°à©‹à¨¨à¨¾ à¨®à¨¹à¨¾à¨‚à¨®à¨¾à¨°à©€ à¨¦à©‡ à¨šà©±à¨²à¨¦à©‡ à¨«à©‹à¨Ÿà©‹à¨—à©à¨°à¨¾à¨«à¨° à¨¦à©€à¨†à¨‚ à¨¬à©°à¨¦ à¨ªà¨ˆà¨†à¨‚ à¨¦à©à¨•à¨¾à¨¨à¨¾à¨‚ à¨–à©‹



[pred] query: ĞŸÑƒĞ½Ğ¶Ğ¸ Ñ…ÑĞ¼ÑÑÑ… Ğ±Ğ¾Ğ´Ğ»Ğ¾Ğ³Ñ‹Ğ½ Ğ±Ò¯Ñ‚ÑÑĞ»Ò¯Ò¯Ğ´Ğ¸Ğ¹Ğ³ ÑÑ…Ğ»Ò¯Ò¯Ğ»ÑÑ…ÑÑÑ Ñ…ÑĞ´Ğ¸Ğ¹Ğ½ÑÑ Ğ±Ğ¾Ğ»Ğ·Ğ¾ÑˆĞ³Ò¯Ğ¹ - MASA M
[true] query: à¨®à©‹à¨¦à©€ à¨¦à©‡ à¨¸à©à¨ªà¨¨à¨¿à¨†à¨‚ à¨¦à¨¾ à¨¯à©‚ à¨ªà©€ à¨¬à¨£à¨¾à¨‰à¨£ à¨²à¨ˆ à¨•à¨µà¨¾à¨‡à¨¦ à¨†à¨°à©°à¨­ - Panja



[pred] query: ĞĞ»Ğ±Ğ°Ğ½ Ñ‘ÑĞ½Ñ‹ Ğ±Ğ°Ğ¹ÑˆĞ¸Ğ½ Ğ´ÑÑÑ€Ñ… Ñ…Ó©ÑˆÓ©Ó©Ğ½Ğ¸Ğ¹ Ğ·ÑƒÑ€Ğ°Ğ³Ğ³ Ñ…Ò¯Ğ»ÑÑĞ»Ğ³ÑĞ½ Ó©Ğ³Ó©Ñ… Posted under:
[true] query: à¨¸à¨¼à¨¹à©€à¨¦à¨¾à¨‚ à¨¦à©€à¨†à¨‚ à¨¤à¨¸à¨µà©€à¨°à¨¾à¨‚ à¨…à¨œà¨¾à¨‡à¨¬ à¨˜à¨° 'à¨š à¨²à¨—à¨¾à¨‰à¨£ à¨¦à©€ à¨®à©°à¨— :
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720158534/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720158534
{'eval_loss': 7.848777770996094, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.235526355249498, 'eval_token_set_recall': 0.25704202562143774, 'eval_token_set_f1': 0.2433737996506214, 'eval_token_set_f1_sem': 0.003726999236238784, 'eval_n_ngrams_match_1': 3.048, 'eval_n_ngrams_match_2': 1.102, 'eval_n_ngrams_match_3': 0.042, 'eval_num_true_words': 13.07, 'eval_num_pred_words': 13.45, 'eval_bleu_score': 6.625768342239273, 'eval_bleu_score_sem': 0.08273014777151234, 'eval_rouge_score': 0.6520383609854201, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8978776335716248, 'eval_emb_cos_sim_sem': 0.00981235070597947, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4407.7464, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/pan_Guru_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating tur_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: 1 TÃœRKÄ°YE DE DEPRESI | TRT ÒšĞ°Ğ·Ğ°Ò›ÑˆĞ° 1 TÃœRKÄ°YE DE DEPRESI | TRT ÒšĞ°Ğ·Ğ°Ò›ÑˆĞ° 1 
[true] query: tek parti devri TEK PARTÄ° DEVRÄ° haberleri haber haberi | Sayfa 7 Tek parti chp zamanÄ±nda yapÄ±lan... 15 Åu



[pred] query: FenerbahÃ§e-Galatasaray: FenerbahÃ§e-Galatasaray: Galatasaray: Galatasaray: Galatasaray: Galatasaray: Trabzonspor Ñ€ĞµÑĞ¼Ğ¸ 
[true] query: Anasayfa Spor FenerbahÃ§e-Dinamo Zagreb maÃ§Ä±nÄ±n hakemleri aÃ§Ä±klandÄ± kaynuka Tarih: 2018-11-27 Saat: 15:07:19 



[pred] query: Ã‡ocuk KitaplarÄ± EÄŸitim KitaplarÄ± EÄŸitim KitaplarÄ± Ã‡ocuk KitaplarÄ± EÄŸitim KitaplarÄ± 7 â€“ 14 yaÅŸ Ã‡ocuk EdebiyatÄ± -
[true] query: AlÄ±ÅŸveriÅŸ Annelik Bebek Ã‡ocuk Ã‡ocuk KitaplarÄ± EÄŸitim SaÄŸlÄ±k 7â€“14 yaÅŸ Ã§ocuklar iÃ§in Ã¶ÄŸretim metodlarÄ± 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720158698/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720158698
{'eval_loss': 3.276726007461548, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.25795829953724375, 'eval_token_set_recall': 0.36486170789135214, 'eval_token_set_f1': 0.29624458201474285, 'eval_token_set_f1_sem': 0.004093915044632978, 'eval_n_ngrams_match_1': 4.224, 'eval_n_ngrams_match_2': 1.326, 'eval_n_ngrams_match_3': 0.142, 'eval_num_true_words': 16.658, 'eval_num_pred_words': 15.818, 'eval_bleu_score': 6.196204366313768, 'eval_bleu_score_sem': 0.11457427224867492, 'eval_rouge_score': 0.20462588499585488, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8821406364440918, 'eval_emb_cos_sim_sem': 0.00803997365169764, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 126.9616, 'eval_samples_per_second': 3.938, 'eval_steps_per_second': 0.496}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/tur_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: 1 TÄ°K DEPRESI | TÃœRKÄ°YE DEPRESI... 1 TÄ°K DEPRESI | TÃœRKÄ°YE DEPRESI...
[true] query: tek parti devri TEK PARTÄ° DEVRÄ° haberleri haber haberi | Sayfa 7 Tek parti chp zamanÄ±nda yapÄ±lan... 15 Åu



[pred] query: FenerbahÃ§e:FenerbahÃ§e:Galatasaray:FenerbahÃ§e:Diego Martinez | SN.kz - Ğ¶Ğ°Ò£Ğ°Ğ»Ñ‹Ò›Ñ‚Ğ°Ñ€. ÒšĞ°Ğ·Ğ°Ò›ÑÑ‚Ğ°Ğ½Ğ½Ñ‹Ò£ Ğ¶Ğ°Ò£Ğ°Ğ»Ñ‹Ò›Ñ‚Ğ°Ñ€Ñ‹ Ğ–Ğ°Ò£Ğ°
[true] query: Anasayfa Spor FenerbahÃ§e-Dinamo Zagreb maÃ§Ä±nÄ±n hakemleri aÃ§Ä±klandÄ± kaynuka Tarih: 2018-11-27 Saat: 15:07:19 



[pred] query: Ã‡ocuk KitaplarÄ± Ã‡ocuk KitaplarÄ± Ã‡ocuk KitaplarÄ± Ã‡ocuk KitaplarÄ± 7 â€“ 14 Ğ¶Ğ°ÑÑ‚Ğ°Ò“Ñ‹ ĞµÑ€ĞµÑĞµĞºÑ‚ĞµÑ€Ğ³Ğµ Ğ¾Ò›Ñƒ Ò¯Ñ€Ğ´
[true] query: AlÄ±ÅŸveriÅŸ Annelik Bebek Ã‡ocuk Ã‡ocuk KitaplarÄ± EÄŸitim SaÄŸlÄ±k 7â€“14 yaÅŸ Ã§ocuklar iÃ§in Ã¶ÄŸretim metodlarÄ± 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720163202/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720163202
{'eval_loss': 3.276726007461548, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2618152552578218, 'eval_token_set_recall': 0.3506481549668089, 'eval_token_set_f1': 0.29516090362023745, 'eval_token_set_f1_sem': 0.00438186914047921, 'eval_n_ngrams_match_1': 4.29, 'eval_n_ngrams_match_2': 1.36, 'eval_n_ngrams_match_3': 0.15, 'eval_num_true_words': 16.658, 'eval_num_pred_words': 16.06, 'eval_bleu_score': 6.375327238827429, 'eval_bleu_score_sem': 0.13745947284062082, 'eval_rouge_score': 0.2118725545141113, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9194341897964478, 'eval_emb_cos_sim_sem': 0.008401686466696737, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4430.8284, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/tur_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating kaz_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: ÒšĞœĞ“ Ğ•Ò¢Ğ‘Ğ•Ğš ĞĞ Ğ¥Ğ˜Ğ’Ğ¢Ğ•Ğ Ğ† - ÒšÑ‹ÑÒ›Ğ° Ğ¶Ğ¸Ğ½Ğ°Ğ»Ñ‹ÑÑ‚Ñ‹Ò£ Ò±Ğ¹Ñ‹Ğ¼Ğ´Ğ°ÑÑ‚Ñ‹Ñ€ÑƒÑˆÑ‹Ğ»Ğ°Ñ€Ñ‹ Ğ•Ò£Ğ±ĞµĞº Ò›Ğ°ÑƒÑ–Ğ¿ÑÑ–Ğ·Ğ´Ñ–Ğ³Ñ– Ğ¶Ó™Ğ½Ğµ Ò›Ğ°ÑƒÑ–Ğ¿
[true] query: ÒšĞœĞ“ ĞĞ›Ğ¢Ğ«Ğ Ğ•Ğ Ğ•Ğ–Ğ•Ğ›Ğ•Ğ Ğ† - Ğ•Ò¢Ğ±ĞµĞº Ò›Ğ°ÑƒÑ–Ğ¿ÑÑ–Ğ·Ğ´Ñ–Ğ³Ñ– Ğ¶Ó™Ğ½Ğµ ĞµÒ£Ğ±ĞµĞºÑ‚Ñ– Ò›ĞĞ Ò’Ğ°Ñƒ Ğ¶Ğ¸Ğ½Ğ°Ğ»Ñ‹ÑÑ‚Ğ°Ñ€Ñ‹Ğ½



[pred] query: Ğ¿ÑĞ¸Ñ…Ğ¾Ñ„Ğ¸Ğ·Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ»Ñ‹Ò› Ñ‚ĞµÑÑ‚Ñ–Ğ»ĞµÑƒ Ğ½Ò±ÑÒ›Ğ°ÑƒĞ»Ñ‹Ò“Ñ‹ ÑĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ¿Ğ¾ Ğ°Ğ¿Ğ¾ÑÑ‚Ğ¸Ğ»ÑŒÑ 18.10.2018 No 587 Ğ±Ò±Ğ¹Ñ€Ñ‹Ò“Ñ‹ Ğ¿
[true] query: Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ÑĞ»Ñ‹Ò› Ñ‚ĞµÑÑ‚Ñ–Ğ»ĞµÑƒ ĞºĞµÑÑ‚ĞµÑÑ– Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼Ñƒ Ğ°Ğ¿ĞµĞ»Ğ»ÑÑ†Ğ¸ÑĞ»Ñ‹Ò› Ğ²ĞµĞ´Ğ¾Ğ¼Ğ¾ÑÑ‚ÑŒ 18.10.2018 Ğ¶ No 578 Ğ±Ò±Ğ¹Ñ€Ñ‹



[pred] query: Ğ«Ğ±Ñ‹Ñ€Ğ°Ğ¹ ĞĞ»Ñ‚Ñ‹Ğ½ÑĞ°Ñ€Ğ¸Ğ½ - Ò°Ğ»Ñ‹ Ğ”Ğ°Ğ»Ğ° Ğ¢Ò±Ğ»Ò“Ğ°Ğ»Ğ°Ñ€Ñ‹ - Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ ĞµÑ„ĞµÑ€Ğ°Ñ‚Ñ‹ -,
[true] query: Ğ«Ğ±Ñ‹Ñ€Ğ°Ğ¹ ĞĞ»Ñ‚Ñ‹Ğ½ÑĞ°Ñ€Ğ¸Ğ½ - Ğ« - Ò°Ğ»Ñ‹ Ğ·Ğ°Ğ¼Ğ°Ğ½ Ğ¢Ò±Ğ»Ò“Ğ°Ğ»Ğ°Ñ€Ñ‹ - Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ ĞµÑ„ĞµÑ€Ğ°Ñ‚Ñ‹,
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720163365/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720163365
{'eval_loss': 0.878508448600769, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.65145903402583, 'eval_token_set_recall': 0.6840497651385421, 'eval_token_set_f1': 0.6659047056582995, 'eval_token_set_f1_sem': 0.008504754937091288, 'eval_n_ngrams_match_1': 10.082, 'eval_n_ngrams_match_2': 6.454, 'eval_n_ngrams_match_3': 4.246, 'eval_num_true_words': 15.352, 'eval_num_pred_words': 15.076, 'eval_bleu_score': 37.62282012377304, 'eval_bleu_score_sem': 1.2333240645860262, 'eval_rouge_score': 0.8982768140412409, 'eval_exact_match': 0.056, 'eval_exact_match_sem': 0.01029271002989587, 'eval_emb_cos_sim': 0.9835675954818726, 'eval_emb_cos_sim_sem': 0.003945324342256279, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 125.7538, 'eval_samples_per_second': 3.976, 'eval_steps_per_second': 0.501}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/kaz_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: ÒšĞœĞ“ ĞĞ›Ğ¢Ğ«ĞĞĞ Ğ”ĞĞ¡Ğ« - Ğ•Ò¢Ğ‘Ğ•Ğš ĞĞ ĞĞÒ’Ğ« Ğ•Ò£Ğ±ĞµĞº Ò›Ğ°ÑƒÑ–Ğ¿ÑÑ–Ğ·Ğ´Ñ–Ğ³Ñ– Ğ¶Ó™Ğ½Ğµ ÑˆĞµÑ€ÑƒĞ»ĞµÑ€ Ğ¶Ğ¸Ğ½Ğ°Ğ»
[true] query: ÒšĞœĞ“ ĞĞ›Ğ¢Ğ«Ğ Ğ•Ğ Ğ•Ğ–Ğ•Ğ›Ğ•Ğ Ğ† - Ğ•Ò¢Ğ±ĞµĞº Ò›Ğ°ÑƒÑ–Ğ¿ÑÑ–Ğ·Ğ´Ñ–Ğ³Ñ– Ğ¶Ó™Ğ½Ğµ ĞµÒ£Ğ±ĞµĞºÑ‚Ñ– Ò›ĞĞ Ò’Ğ°Ñƒ Ğ¶Ğ¸Ğ½Ğ°Ğ»Ñ‹ÑÑ‚Ğ°Ñ€Ñ‹Ğ½



[pred] query: Ğ¿ÑĞ¸Ñ…Ğ¾Ñ„Ğ¸Ğ·Ğ¸Ğ¾Ğ»Ğ¾Ğ³Ğ¸ÑĞ»Ñ‹Ò› Ñ‚ĞµÑÑ‚Ñ–Ğ»ĞµÑƒ Ğ½Ò±ÑÒ›Ğ°ÑƒĞ»Ñ‹Ò“Ñ‹ ÑĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ‚Ğ°Ğ±Ğ»Ğ¸Ñ†Ñƒ Ğ¿Ğ¾ Ğ¸Ñ‚Ğ¾Ğ³Ğ°Ğ¼ Ğ¿Ğ¼Ğ¶ 18.10.2018 No 578 Ğ±Ò±Ğ¹Ñ€Ñ‹Ò“Ñ‹
[true] query: Ğ¿ÑĞ¸Ñ…Ğ¾Ğ¼ĞµÑ‚Ñ€Ğ¸ÑĞ»Ñ‹Ò› Ñ‚ĞµÑÑ‚Ñ–Ğ»ĞµÑƒ ĞºĞµÑÑ‚ĞµÑÑ– Ğ¸Ğ½ÑÑ‚Ñ€ÑƒĞºÑ†Ğ¸Ñ Ğ¿Ğ¾ Ğ¿Ñ€Ğ¸Ğ¼Ñƒ Ğ°Ğ¿ĞµĞ»Ğ»ÑÑ†Ğ¸ÑĞ»Ñ‹Ò› Ğ²ĞµĞ´Ğ¾Ğ¼Ğ¾ÑÑ‚ÑŒ 18.10.2018 Ğ¶ No 578 Ğ±Ò±Ğ¹Ñ€Ñ‹



[pred] query: Ğ«Ğ±Ñ‹Ñ€Ğ°Ğ¹ ĞĞ»Ñ‚Ñ‹Ğ½ÑĞ°Ñ€Ğ¸Ğ½ - Ğ« - Ò°Ğ»Ñ‹ Ğ·Ğ°Ğ¼Ğ°Ğ½ Ğ¢Ò±Ğ»Ò“Ğ°Ğ»Ğ°Ñ€Ñ‹ - Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ ĞµÑ„ĞµÑ€Ğ°Ñ‚Ñ‹,
[true] query: Ğ«Ğ±Ñ‹Ñ€Ğ°Ğ¹ ĞĞ»Ñ‚Ñ‹Ğ½ÑĞ°Ñ€Ğ¸Ğ½ - Ğ« - Ò°Ğ»Ñ‹ Ğ·Ğ°Ğ¼Ğ°Ğ½ Ğ¢Ò±Ğ»Ò“Ğ°Ğ»Ğ°Ñ€Ñ‹ - Ğ¡ĞºĞ°Ñ‡Ğ°Ñ‚ÑŒ Ğ ĞµÑ„ĞµÑ€Ğ°Ñ‚Ñ‹,
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720167886/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720167886
{'eval_loss': 0.878508448600769, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.7396453392067168, 'eval_token_set_recall': 0.7589217656440568, 'eval_token_set_f1': 0.7481814666628938, 'eval_token_set_f1_sem': 0.00921272461805927, 'eval_n_ngrams_match_1': 11.482, 'eval_n_ngrams_match_2': 8.094, 'eval_n_ngrams_match_3': 6.038, 'eval_num_true_words': 15.352, 'eval_num_pred_words': 15.214, 'eval_bleu_score': 49.53063571929763, 'eval_bleu_score_sem': 1.493395594688811, 'eval_rouge_score': 0.9305370779203603, 'eval_exact_match': 0.178, 'eval_exact_match_sem': 0.01712362218906232, 'eval_emb_cos_sim': 0.9865649938583374, 'eval_emb_cos_sim_sem': 0.0041902627280108636, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 4447.9712, 'eval_samples_per_second': 0.112, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/kaz_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating cmn_Hani val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: æ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿæ€¥é€Ÿ
[true] query: ä¸‹è¡Œä»¥åŠç°è´§æˆäº¤åå¼±æ‹–ç´¯,å¸‚åœºä¸»å¯¼åœ°åŒºä»·æ ¼ç»§ç»­å¿«é€Ÿè°ƒä½,é‚¯éƒ¸ä¸­æ¿ä»·æ ¼é‡



[pred] query: æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´æ»´
[true] query: æ¯æ—¥å½©ç¥¨ çç æ£‰çš„åŒ…è£…å®šä½æ˜¯ä¸€ç§å…·æœ‰é«˜å¼ºçš„ç¼“å†²å¸éœ‡æŠ—éœ‡ä½œç”¨çš„æœ‰æ˜æ˜¾ç¯ä¿æ•ˆåŠ›çš„åŒ…è£…



[pred] query: ä¸­å›½å¤®è¡Œ(ä¸­å›½å¤®è¡Œ)æ¨å‡ºäº†æ–°çš„è´§å¸å‘è¡Œå¹³å°,åˆ©ç”¨è™šæ‹Ÿè´§å¸çš„ä½¿ç”¨å¹³å°,åˆ©ç”¨è™šæ‹Ÿè´§å¸çš„ä½¿ç”¨å¹³å°çš„ç”¨æˆ·ë“¤ì„ 
[true] query: ç«å¸äº‘äºä¸Šä¸ªæœˆåˆšåˆšæ¨å‡º,å…è®¸ç”¨æˆ·å¼€å‘ä»–ä»¬è‡ªå·±çš„ç±»ä¼¼ç«å¸ç½‘çš„æ•°å­—è´§å¸äº¤æ˜“å¹³å°,å…¶ä¸­åŒ…æ‹¬é’±
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720168053/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720168053
{'eval_loss': 4.46931791305542, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 31.9140625, 'eval_token_set_precision': 0.4191488946725791, 'eval_token_set_recall': 0.48293941793500544, 'eval_token_set_f1': 0.4290775627275765, 'eval_token_set_f1_sem': 0.004513843332990336, 'eval_n_ngrams_match_1': 3.122, 'eval_n_ngrams_match_2': 1.006, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 7.802, 'eval_num_pred_words': 8.738, 'eval_bleu_score': 7.961076843269149, 'eval_bleu_score_sem': 0.24803651189359882, 'eval_rouge_score': 0.6672372241191901, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8834879398345947, 'eval_emb_cos_sim_sem': 0.01364608402281357, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 129.2321, 'eval_samples_per_second': 3.869, 'eval_steps_per_second': 0.487}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/cmn_Hani_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ñ…ÑĞ·Ğ³Ğ°Ğ°Ñ€Ğ»Ğ°Ğ³Ğ´Ğ¼Ğ°Ğ» Ğ´Ğ¾Ñ‚Ğ¾Ğ¾Ğ´ Ğ±Ğ¾Ğ´Ğ»Ğ¾Ğ³Ñ‹Ğ½ Ñ…ÑĞ·Ğ³Ğ°Ğ°Ñ€Ğ»Ğ°Ğ³Ğ´Ğ¼Ğ°Ğ» Ğ´Ğ¾Ñ‚Ğ¾Ğ¾Ğ´ Ğ±Ğ¾Ğ´Ğ»Ğ¾Ğ³Ñ‹Ğ½ Ñ…ÑĞ·Ğ³Ğ°Ğ°Ñ€Ğ»Ğ°Ğ³Ğ´Ğ¼Ğ°Ğ» | CNY
[true] query: ä¸‹è¡Œä»¥åŠç°è´§æˆäº¤åå¼±æ‹–ç´¯,å¸‚åœºä¸»å¯¼åœ°åŒºä»·æ ¼ç»§ç»­å¿«é€Ÿè°ƒä½,é‚¯éƒ¸ä¸­æ¿ä»·æ ¼é‡



[pred] query: è‡ªåˆ¶å¡‘èƒ¶åŒ…è£…è¢‹æ˜¯å…·æœ‰ä¸€å®šæŠ—éœ‡èƒ½åŠ›å¼ºçš„ä»Šæ—¥ä½¿ç”¨çš„é˜²éœ‡èƒ½åŠ›å¼ºçš„ä»Šæ—¥ä½¿ç”¨çš„é˜²éœ‡èƒ½åŠ›å¼ºçš„ä»Šæ—¥ä½¿ç”¨çš„
[true] query: æ¯æ—¥å½©ç¥¨ çç æ£‰çš„åŒ…è£…å®šä½æ˜¯ä¸€ç§å…·æœ‰é«˜å¼ºçš„ç¼“å†²å¸éœ‡æŠ—éœ‡ä½œç”¨çš„æœ‰æ˜æ˜¾ç¯ä¿æ•ˆåŠ›çš„åŒ…è£…



[pred] query: ä¸­å›½å¤®è¡Œ(ä¸­å›½å¤®è¡Œ),ç›®å‰ cryptocurrency(äººæ°‘å¸) Ñ…ÑÑ€ÑĞ³Ğ»ÑĞ³Ñ‡Ğ´ÑĞ´ Ğ·Ğ¾Ñ€Ğ¸ÑƒĞ»Ğ°Ğ½ Ğ·Ğ¾Ñ…Ğ¸Ğ¾Ğ½ Ğ±Ğ°Ğ¹Ğ³ÑƒÑƒĞ»Ğ°Ğ³Ğ´ÑĞ°Ğ½ Ñ…ÑƒĞ²Ğ¸Ğ»Ğ±Ğ°Ñ€ Ğ±Ğ¾Ğ»Ğ¾Ñ…Ñ‹Ğ³
[true] query: ç«å¸äº‘äºä¸Šä¸ªæœˆåˆšåˆšæ¨å‡º,å…è®¸ç”¨æˆ·å¼€å‘ä»–ä»¬è‡ªå·±çš„ç±»ä¼¼ç«å¸ç½‘çš„æ•°å­—è´§å¸äº¤æ˜“å¹³å°,å…¶ä¸­åŒ…æ‹¬é’±
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720172540/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720172540
{'eval_loss': 4.46931791305542, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 31.9140625, 'eval_token_set_precision': 0.4039622646359492, 'eval_token_set_recall': 0.4242493333807575, 'eval_token_set_f1': 0.3882156163875501, 'eval_token_set_f1_sem': 0.004591553056868897, 'eval_n_ngrams_match_1': 2.968, 'eval_n_ngrams_match_2': 1.02, 'eval_n_ngrams_match_3': 0.014, 'eval_num_true_words': 7.802, 'eval_num_pred_words': 10.52, 'eval_bleu_score': 7.161238685049193, 'eval_bleu_score_sem': 0.2371466853972971, 'eval_rouge_score': 0.6248902899033334, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8733090162277222, 'eval_emb_cos_sim_sem': 0.02087948421493512, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4414.1817, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/cmn_Hani_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating jpn_Jpan val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: å¯’æš–ã®å¯’æš–ã®å¯’æš–ã®å¯’æš–ã®å¯’æš–ã®å¯’æš–ã«æ„Ÿã˜ã‚‹ã¨ã“ã‚, å®¶ã‚’å‡ºã‚‹éš›ã« ã©ã†ã—ã¦ã‚‚
[true] query: èŠ±ç«‹å±±è˜ã§ã¯å¤œåŠã‹ã‚‰é›¨ãŒé™ã‚Šç¶šã„ã¦ã„ã¦ã€æœã«ã¡ã‚‡ã£ã¨ã ã‘é›¨è„šãŒå¼±ããªã£ãŸã¨ãã«å‡ºç™ºã€‚ ã—ã°ã‚‰ã



[pred] query: ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚
[true] query: ä»Šã‚·ãƒ¼ã‚ºãƒ³åˆã®å‡çµã—ãŸæ¡§åŸæ¹–ã®æ¹–ä¸Šæ’®å½±ã§ã™ã€‚ ã‹ãªã‚Šçµæ°·ã¯é€²ã‚“ã§ã„ã¾ã—ãŸãŒã€ã¾ã ãƒ¯ã‚«ã‚µã‚®



[pred] query: â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»Twitterã®ãƒãƒ¼ãƒ ã®ãƒ¡ãƒ³ãƒãƒ¼ã«ã¯éå¸¸ã«å¼·ã„
[true] query: ã¾ãšå°‘ãªãã¨ã‚‚ã“ã®äººãŸã¡ãƒãƒ¼ãƒ ã«ãƒãƒƒãƒˆå‘¨ã‚ŠãŒå¼·ã„äººé–“ãŒã„ã‚‹ã“ã¨ã¯é–“é•ã„ãªãã¦ã€YouTubeã®ãƒ¡ã‚¿ã‚¿ã‚°(ãƒ¡ã‚¿ã‚¿ã‚°ã£ã¦?ã£ã¦äººã¯ã“ã®è¨˜äº‹
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720172702/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720172702
{'eval_loss': 4.49762487411499, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.5365262661315278, 'eval_token_set_recall': 0.5105331941005843, 'eval_token_set_f1': 0.4951643599067448, 'eval_token_set_f1_sem': 0.006173944732226779, 'eval_n_ngrams_match_1': 2.256, 'eval_n_ngrams_match_2': 1.022, 'eval_n_ngrams_match_3': 0.01, 'eval_num_true_words': 4.93, 'eval_num_pred_words': 7.046, 'eval_bleu_score': 8.132723593772432, 'eval_bleu_score_sem': 0.42487119235533655, 'eval_rouge_score': 0.7084152054426528, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.860263466835022, 'eval_emb_cos_sim_sem': 0.009867490637632865, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 125.324, 'eval_samples_per_second': 3.99, 'eval_steps_per_second': 0.503}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/jpn_Jpan_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: ã€Œå¯’æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–æš–
[true] query: èŠ±ç«‹å±±è˜ã§ã¯å¤œåŠã‹ã‚‰é›¨ãŒé™ã‚Šç¶šã„ã¦ã„ã¦ã€æœã«ã¡ã‚‡ã£ã¨ã ã‘é›¨è„šãŒå¼±ããªã£ãŸã¨ãã«å‡ºç™ºã€‚ ã—ã°ã‚‰ã



[pred] query: ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚ğŸ‚
[true] query: ä»Šã‚·ãƒ¼ã‚ºãƒ³åˆã®å‡çµã—ãŸæ¡§åŸæ¹–ã®æ¹–ä¸Šæ’®å½±ã§ã™ã€‚ ã‹ãªã‚Šçµæ°·ã¯é€²ã‚“ã§ã„ã¾ã—ãŸãŒã€ã¾ã ãƒ¯ã‚«ã‚µã‚®



[pred] query: â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»â€»
[true] query: ã¾ãšå°‘ãªãã¨ã‚‚ã“ã®äººãŸã¡ãƒãƒ¼ãƒ ã«ãƒãƒƒãƒˆå‘¨ã‚ŠãŒå¼·ã„äººé–“ãŒã„ã‚‹ã“ã¨ã¯é–“é•ã„ãªãã¦ã€YouTubeã®ãƒ¡ã‚¿ã‚¿ã‚°(ãƒ¡ã‚¿ã‚¿ã‚°ã£ã¦?ã£ã¦äººã¯ã“ã®è¨˜äº‹
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720177171/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720177171
{'eval_loss': 4.49762487411499, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.5382603931156549, 'eval_token_set_recall': 0.48864463918434414, 'eval_token_set_f1': 0.47874062832517644, 'eval_token_set_f1_sem': 0.006201475302628633, 'eval_n_ngrams_match_1': 2.272, 'eval_n_ngrams_match_2': 1.036, 'eval_n_ngrams_match_3': 0.018, 'eval_num_true_words': 4.93, 'eval_num_pred_words': 7.97, 'eval_bleu_score': 8.103991417756015, 'eval_bleu_score_sem': 0.4036332384514903, 'eval_rouge_score': 0.701769368924142, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8500696420669556, 'eval_emb_cos_sim_sem': 0.013240280994460523, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4395.4552, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/jpn_Jpan_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating kor_Hang val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: â‡ ĞŸÑ€ĞµĞ´Ñ‹Ğ´ÑƒÑ‰Ğ°ÑĞ¡Ñ‚Ñ€ 03 Ğ¸Ğ· 152Ğ¡Ğ»ĞµĞ´ÑƒÑÑ‰Ğ°Ñ â‡’ <div style="display:none" style="display:none" style="display
[true] query: ì„¸ì¼ ì¤‘ â€“ Natural Health {% if first_available_variant.compare_at_price > first_available_variant.price



[pred] query: ì„ íƒì˜ ì¡°ê±´ - ì›¹íˆ°.com ì„ íƒì˜ ì¡°ê±´ ãƒ» ì›¹íˆ°.com ì„ íƒì˜ ì¡°ê±´ ãƒ» ì›¹íˆ°
[true] query: ì‹ ìš©ìœ„í—˜ ê²°ì •ìš”ì¸ ê³¼ ê´€ë ¨ - í† í† ì‚¬ì´íŠ¸ ê²€ì¦ì‚¬ì´íŠ¸ ë©”ì´ì €í† í† ì‚¬ì´íŠ¸ - í† í† íƒì • - ì‹ 



[pred] query: Shopify - ëª½ê³¨ì¸ ëª½ê³¨ì¸ ëª½ê³¨ì¸ ëª½ê³¨ì¸ ëª½ê³¨ì¸ ëª½ê³¨ì¸
[true] query: ì•ˆë“œë¡œì´ë“œ : ì‚¼ì„±ë°”ë‹¤í° ì˜êµ­ ì˜¨ë¼ì¸ ë“±ë¡ - íƒ€ì´ì   - ì•ˆë“œë¡œì´ë“œ ìŠ¤ë§ˆíŠ¸í°ê³¼ íƒœë¸”ë¦¿ ìƒˆ
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720177333/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720177333
{'eval_loss': 3.9590444564819336, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2244466509238959, 'eval_token_set_recall': 0.37640375904625184, 'eval_token_set_f1': 0.26979293016142336, 'eval_token_set_f1_sem': 0.0038012045622794364, 'eval_n_ngrams_match_1': 3.174, 'eval_n_ngrams_match_2': 1.118, 'eval_n_ngrams_match_3': 0.06, 'eval_num_true_words': 14.33, 'eval_num_pred_words': 13.054, 'eval_bleu_score': 5.54513049413212, 'eval_bleu_score_sem': 0.12044235416504369, 'eval_rouge_score': 0.5499180680666784, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9253830909729004, 'eval_emb_cos_sim_sem': 0.016529306465506304, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 125.38, 'eval_samples_per_second': 3.988, 'eval_steps_per_second': 0.502}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/kor_Hang_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: <div style="display:none" style="display:none" style="display:none" style="display:none" style="
[true] query: ì„¸ì¼ ì¤‘ â€“ Natural Health {% if first_available_variant.compare_at_price > first_available_variant.price



[pred] query: Ò®Ğ·Ò¯Ò¯Ğ»ÑĞ»Ñ‚Ñ‚ÑĞ¹ Ñ…Ğ¾Ğ»Ğ±Ğ¾Ğ¾Ñ‚Ğ¾Ğ¹ Ğ¼Ó©Ñ€Ó©Ó©Ğ´Ó©Ğ»Ñ‚ÑĞ¹ Ğ¼Ó©Ñ€Ó©Ó©Ğ´Ó©Ğ»Ñ‚ÑĞ¹ Ğ¼Ó©Ñ€Ó©Ó©Ğ´Ó©Ğ»Ñ‚ÑĞ¹ Ğ¼Ó©Ñ€Ó©Ó©Ğ´Ó©Ğ»Ñ‚ÑĞ¹ Ğ¼Ó©Ñ€Ó©Ó©Ğ´Ó© | www.
[true] query: ì‹ ìš©ìœ„í—˜ ê²°ì •ìš”ì¸ ê³¼ ê´€ë ¨ - í† í† ì‚¬ì´íŠ¸ ê²€ì¦ì‚¬ì´íŠ¸ ë©”ì´ì €í† í† ì‚¬ì´íŠ¸ - í† í† íƒì • - ì‹ 



[pred] query: Shopify - ëª½ê³¨ì¸ ëª½ê³¨ì¸ ëª½ê³¨ì¸ ëª½ê³¨ì¸ ëª½ê³¨ì¸ Apple iPad /
[true] query: ì•ˆë“œë¡œì´ë“œ : ì‚¼ì„±ë°”ë‹¤í° ì˜êµ­ ì˜¨ë¼ì¸ ë“±ë¡ - íƒ€ì´ì   - ì•ˆë“œë¡œì´ë“œ ìŠ¤ë§ˆíŠ¸í°ê³¼ íƒœë¸”ë¦¿ ìƒˆ
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720181797/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720181797
{'eval_loss': 3.9590444564819336, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22016190169479688, 'eval_token_set_recall': 0.3741990475898377, 'eval_token_set_f1': 0.2634066496712189, 'eval_token_set_f1_sem': 0.0037957221525332326, 'eval_n_ngrams_match_1': 3.096, 'eval_n_ngrams_match_2': 1.094, 'eval_n_ngrams_match_3': 0.05, 'eval_num_true_words': 14.33, 'eval_num_pred_words': 13.112, 'eval_bleu_score': 5.187855224711142, 'eval_bleu_score_sem': 0.11615450003463239, 'eval_rouge_score': 0.5129444444030746, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9003894925117493, 'eval_emb_cos_sim_sem': 0.019551052428364187, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4391.0433, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/kor_Hang_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mon_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: ĞĞ»Ñ‚Ğ½Ñ‹ Ğ³Ğ°Ğ´Ğ°Ğ°Ğ´ ÑĞºÑĞ¿Ğ¾Ñ€Ñ‚Ñ‹Ğ½ Ğ°Ğ¼.Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ñ‹Ğ½ Ó©Ñ€Ñ‚Ó©Ğ³ Ó©ÑÓ©Ğ¶ ĞĞ¥Ğ£ 56 Ñ‚ÑÑ€Ğ±ÑƒĞ¼ Ğ°Ğ¼.Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€Ñ‚ Ñ…Ò¯Ñ€ÑÑ…
[true] query: ĞĞ»Ñ‚Ğ½Ñ‹ Ğ¾Ğ»Ğ±Ğ¾Ñ€Ğ»Ğ¾Ğ»Ñ‚ Ğ°Ğ¼Ğ¶Ğ¸Ğ»Ñ‚Ğ°Ğ´ Ñ…Ò¯Ñ€Ğ½Ñ ĞĞ¥Ğ£Ñ‹Ğ½ Ğ³Ğ°Ğ´Ğ°Ğ°Ğ´ Ó©Ñ€ 56 Ñ‚ÑÑ€Ğ±ÑƒĞ¼ Ğ°Ğ¼.Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€ Ğ±Ğ¾Ğ»Ğ¶ Ó©ÑÑ‡ÑÑ



[pred] query: Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ğ½ Ğ·Ğ°Ğ¹Ğ½ Ó©Ó©Ñ€Ñ‡Ğ»Ó©Ğ»Ñ‚Ğ¸Ğ¹Ğ³ Ñ…Ğ°Ñ€Ğ°Ñ… 3 Ğ·Ò¯Ğ¹Ğ» | Martech Zone Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ğ½ 
[true] query: Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ñ€Ñ‹Ğ½ Ó©Ó©Ñ€Ñ‡Ğ»Ó©Ğ»Ñ‚Ğ¸Ğ¹Ğ³ Ğ°Ğ½Ñ…Ğ°Ğ°Ñ€Ñ‡ Ò¯Ğ·ÑÑ… 3 Ğ·Ò¯Ğ¹Ğ» | Martech Zone Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ñ€Ñ‹Ğ½



[pred] query: Â» Ó¨Ğ³Ğ»Ó©Ó©Ğ½Ğ¸Ğ¹ Ğ¼ÑĞ½Ğ´ Â» Ğ¥Ò®ĞœÒ®Ò®Ğ¡ (960956) Â» Ğ¡Ğ•ĞšĞ¡ (985597) Ğ¡ĞµĞºÑĞ¸Ğ¹Ğ³
[true] query: Â» Ğ¡Ğ•ĞšĞ¡ (1085550) Â» Ğ¨ĞĞ  ĞœĞ­Ğ”Ğ­Ğ­ (805751) Â» Ó¨Ğ“Ò®Ò®Ğ›Ğ›Ğ­Ğ“ (906359) Ñ…Ó©Ğ»Ğ¸Ğ¹Ğ³ 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720181966/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720181966
{'eval_loss': 0.8587145209312439, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.6473010345090071, 'eval_token_set_recall': 0.6726348202537056, 'eval_token_set_f1': 0.6584488546779808, 'eval_token_set_f1_sem': 0.008809313336569475, 'eval_n_ngrams_match_1': 9.134, 'eval_n_ngrams_match_2': 5.6, 'eval_n_ngrams_match_3': 3.502, 'eval_num_true_words': 14.004, 'eval_num_pred_words': 13.674, 'eval_bleu_score': 37.46891875078984, 'eval_bleu_score_sem': 1.187900878701989, 'eval_rouge_score': 0.8715056986549864, 'eval_exact_match': 0.06, 'eval_exact_match_sem': 0.010631371130019326, 'eval_emb_cos_sim': 0.9776576161384583, 'eval_emb_cos_sim_sem': 0.0044706341010174585, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 130.3825, 'eval_samples_per_second': 3.835, 'eval_steps_per_second': 0.483}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/mon_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: ĞĞ»Ñ‚Ğ½Ñ‹ Ğ¾Ğ»Ğ±Ğ¾Ñ€Ğ»Ğ¾Ğ»Ñ‚ Ğ°Ğ¼Ğ¶Ğ¸Ğ»Ñ‚Ğ°Ğ´ Ñ…Ò¯Ñ€Ğ½Ñ ĞĞ¥Ğ£Ñ‹Ğ½ Ğ³Ğ°Ğ´Ğ°Ğ°Ğ´ Ó©Ñ€ 56 Ñ‚ÑÑ€Ğ±ÑƒĞ¼ Ğ°Ğ¼.Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€ Ğ±Ğ¾Ğ»Ğ¶ Ó©ÑĞ½Ó©
[true] query: ĞĞ»Ñ‚Ğ½Ñ‹ Ğ¾Ğ»Ğ±Ğ¾Ñ€Ğ»Ğ¾Ğ»Ñ‚ Ğ°Ğ¼Ğ¶Ğ¸Ğ»Ñ‚Ğ°Ğ´ Ñ…Ò¯Ñ€Ğ½Ñ ĞĞ¥Ğ£Ñ‹Ğ½ Ğ³Ğ°Ğ´Ğ°Ğ°Ğ´ Ó©Ñ€ 56 Ñ‚ÑÑ€Ğ±ÑƒĞ¼ Ğ°Ğ¼.Ğ´Ğ¾Ğ»Ğ»Ğ°Ñ€ Ğ±Ğ¾Ğ»Ğ¶ Ó©ÑÑ‡ÑÑ



[pred] query: Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ñ€Ñ‹Ğ½ Ó©Ó©Ñ€Ñ‡Ğ»Ó©Ğ»Ñ‚Ğ¸Ğ¹Ğ³ Ğ°Ğ½Ñ…Ğ°Ğ°Ñ€Ğ°Ñ… 3 Ğ·Ò¯Ğ¹Ğ» | Martech Zone Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ñ€Ñ‹Ğ½ Ó©Ó©Ñ€Ñ‡Ğ»
[true] query: Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ñ€Ñ‹Ğ½ Ó©Ó©Ñ€Ñ‡Ğ»Ó©Ğ»Ñ‚Ğ¸Ğ¹Ğ³ Ğ°Ğ½Ñ…Ğ°Ğ°Ñ€Ñ‡ Ò¯Ğ·ÑÑ… 3 Ğ·Ò¯Ğ¹Ğ» | Martech Zone Google Ğ¢ĞµĞºÑÑ‚Ğ¸Ğ¹Ğ½ Ğ·Ğ°Ñ€Ñ‹Ğ½



[pred] query: Â» Ğ¥Ò®Ò®Ğ¥Ğ”Ğ˜Ğ™Ğ Ğ¥Ó¨Ğ“Ğ–Ğ›Ğ˜Ğ™Ğ“ Ğ¨ĞĞĞ Ğ”Ğ›ĞĞ“Ğ (95553) Â» Ğ¡ĞµĞºÑ (95081) Â»
[true] query: Â» Ğ¡Ğ•ĞšĞ¡ (1085550) Â» Ğ¨ĞĞ  ĞœĞ­Ğ”Ğ­Ğ­ (805751) Â» Ó¨Ğ“Ò®Ò®Ğ›Ğ›Ğ­Ğ“ (906359) Ñ…Ó©Ğ»Ğ¸Ğ¹Ğ³ 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720186468/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720186468
{'eval_loss': 0.8587145209312439, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.7315019872198673, 'eval_token_set_recall': 0.743819143859237, 'eval_token_set_f1': 0.7367787972752049, 'eval_token_set_f1_sem': 0.009573102399618323, 'eval_n_ngrams_match_1': 10.294, 'eval_n_ngrams_match_2': 6.936, 'eval_n_ngrams_match_3': 4.956, 'eval_num_true_words': 14.004, 'eval_num_pred_words': 13.816, 'eval_bleu_score': 48.67385726481722, 'eval_bleu_score_sem': 1.4523935705178679, 'eval_rouge_score': 0.9009257092274705, 'eval_exact_match': 0.182, 'eval_exact_match_sem': 0.017272773297730432, 'eval_emb_cos_sim': 0.982160747051239, 'eval_emb_cos_sim_sem': 0.008755309720959772, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4428.8231, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/mon_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating hun_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Ultrasonication: Ğ°ÑƒÑ‹Ñ€ÑÑ‹Ğ½Ñƒ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ğ´Ğ°Ñ€Ñ‹. Ultrasonication: Ğ°ÑƒÑ‹Ñ€ÑÑ‹Ğ½Ñƒ ÑĞ¸Ğ¼Ğ¿Ñ‚Ğ¾Ğ¼Ğ´Ğ°Ñ€Ñ‹. Ultrasonication ĞºÓ©Ğ¼ĞµĞ³Ñ–Ğ¼ĞµĞ½ Ò›Ğ¾Ğ» Ğ¶ĞµÑ‚Ñ–Ğ¼Ğ´Ñ–
[true] query: Ultrahang kezelÃ©s: FÃ¡jdalomcsillapÃ­tÃ¡s tÅ±szÅ±rÃ¡s nÃ©lkÃ¼l [teljes ÃºtmutatÃ³] ÃzÃ¼leti fÃ¡jdalom fono



[pred] query: ESPN.kz - 14.12.2019, 18:45 Mikel Garcia, Ğ¶Ğ°Ò£Ğ° "Titan" Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ñ‹. Mikel Garcia,
[true] query: Az Ãºj Mike Tyson | SamanSport.hu 2019. 12. 13., PÃ©ntek, 18:40 Gervonta "Tank" Davis hatalmas



[pred] query: - Part 2 â€“ OÅ„tÃºstik.cz OÅ„tÃºstik.cz - Part 2 ĞœĞµĞ½ Ó©Ğ·Ñ–Ğ¼Ğ½Ñ–Ò£ Ğ¾Ñ‚Ğ°Ğ½Ğ´Ñ‹Ò› Ğ¶ĞµÒ£Ñ–Ğ»
[true] query: JÃ¡rmÅ±vek | Hobbi ZÃ³na - Part 2 TankChair â€“ az Off Road tolÃ³szÃ©k A rendkÃ­vÃ¼li gÃ©pezet
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720186634/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720186634
{'eval_loss': 5.654638767242432, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.218205143789819, 'eval_token_set_recall': 0.36090123082971437, 'eval_token_set_f1': 0.266413984571739, 'eval_token_set_f1_sem': 0.003530248045051354, 'eval_n_ngrams_match_1': 3.518, 'eval_n_ngrams_match_2': 1.128, 'eval_n_ngrams_match_3': 0.056, 'eval_num_true_words': 16.376, 'eval_num_pred_words': 14.356, 'eval_bleu_score': 5.411056494425635, 'eval_bleu_score_sem': 0.09369331660023959, 'eval_rouge_score': 0.12262978160569526, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8911307454109192, 'eval_emb_cos_sim_sem': 0.012160880447761287, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 128.7078, 'eval_samples_per_second': 3.885, 'eval_steps_per_second': 0.489}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/hun_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ultrasonication: Ñ–ÑˆĞºÑ– Ñ‚ĞµÑ€Ğ±ĞµĞ»Ñ–Ñ Ğ°ÑƒÑ‹Ñ€ÑÑ‹Ğ½ÑƒÑ‹Ğ½Ñ‹Ò£ Ğ¶ĞµÒ£Ñ–Ğ»Ğ´ĞµÑ‚Ñ–Ğ»Ğ³ĞµĞ½ Ó™Ğ´Ñ–ÑÑ– Ultrasonication: Ñ–ÑˆĞºÑ– Ñ‚ĞµÑ€Ğ±ĞµĞ»Ñ–Ñ
[true] query: Ultrahang kezelÃ©s: FÃ¡jdalomcsillapÃ­tÃ¡s tÅ±szÅ±rÃ¡s nÃ©lkÃ¼l [teljes ÃºtmutatÃ³] ÃzÃ¼leti fÃ¡jdalom fono



[pred] query: ESPN.kz - 14.12.2019, 15:42 Mike Twang, Ğ¶Ğ°Ò£Ğ° Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ´Ñ‹Ò“Ñ‹. Mike Twang, Ğ¶Ğ°Ò£Ğ° Ñ‡ĞµĞ¼Ğ¿Ğ¸Ğ¾Ğ½Ğ´Ñ‹Ò“Ñ‹.
[true] query: Az Ãºj Mike Tyson | SamanSport.hu 2019. 12. 13., PÃ©ntek, 18:40 Gervonta "Tank" Davis hatalmas



[pred] query: - OÅ„tÃºstik.cz - OÅ„tÃºstik.cz - OÅ„tÃºstik.cz Ó¨Ğ·Ñ–Ğ½Ñ–Ò£ ĞµÒ£
[true] query: JÃ¡rmÅ±vek | Hobbi ZÃ³na - Part 2 TankChair â€“ az Off Road tolÃ³szÃ©k A rendkÃ­vÃ¼li gÃ©pezet
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720191139/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720191139
{'eval_loss': 5.654638767242432, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.21587530822290915, 'eval_token_set_recall': 0.3556941556637384, 'eval_token_set_f1': 0.2623447836761183, 'eval_token_set_f1_sem': 0.003515633869373724, 'eval_n_ngrams_match_1': 3.476, 'eval_n_ngrams_match_2': 1.15, 'eval_n_ngrams_match_3': 0.07, 'eval_num_true_words': 16.376, 'eval_num_pred_words': 14.29, 'eval_bleu_score': 5.454258970519157, 'eval_bleu_score_sem': 0.11052109952962605, 'eval_rouge_score': 0.1232411968271859, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8885043859481812, 'eval_emb_cos_sim_sem': 0.01112712493799935, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4432.532, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/hun_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mhr_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Ğ¢ÑƒÑ‹ÑÑ‹Ğ¼, ĞºĞµÑˆĞºĞµ Ğ¶Ğ°Ğ»Ğ°Ò£Ğ°Ñˆ, ĞºĞµÑˆĞºĞµ Ğ¶Ğ°Ğ»Ğ°Ò£Ğ°Ñˆ ĞšĞµÑˆĞµ Ñ‚Ò¯Ğ½ Ğ¶Ğ°Ğ»Ğ°Ò£Ğ°Ñˆ ĞšĞµÑˆĞµ Ğ¼Ğ°Ñ‚Ñ‡
[true] query: Ğ¢ÑƒÑ€Ğ³Ñ‹Ğ¼ Ğ¶Ğ°Ğ¿Ñ‹ÑˆÑ‚Ğµ, ÑˆĞ¾ÑˆĞ¾ Ğ°Ğ³Ğ° Ğ³Ğ¾Ğ´Ñ‹Ğ¼, ĞºĞµÑ‡ Ğ¸Ğº Ğ³Ğ°Ğ½Ğ° ĞšÑƒĞ³Ñƒ ĞšĞ°Ñ‡Ğ°Ğº ÑĞ»Ñ‹ÑˆÑ‚Ğµ Ğ»Ğ¸ÑÑˆ



[pred] query: ĞÑÑƒĞ»Ñ‹Ğ¼ ĞµÑ€Ñ‚Ğµ Ğ¼Ğµ? Ğ¢ÑƒĞ´Ñ‹Ğ¼ ĞµÑ€Ñ‚Ğµ Ğ¼Ğµ? ĞœĞ°Ñ€Ğ³Ğ°Ğ´Ğ°Ñ‚ ĞšĞ¾Ğ·Ğ°ĞµĞ² ĞœĞ°Ñ€Ğ³Ğ°Ğ´Ğ°Ñ‚ ĞµÑ€Ñ‚Ğµ Ğ¼Ğµ? Ğ–Ğ°
[true] query: ĞœĞ¾ Ñ‚Ñ‹Ğ³Ğ°Ğ¹ ĞĞ³Ğ°Ğ²Ğ°Ğ¹Ñ€ĞµĞ¼? ĞšÑƒĞ·Ğµ Ñ‚ÑƒĞ´Ğ¾ ÑÑ€Ñ‚Ğ°? Ğ¢Ğ¸Ğ´Ğµ Ğ´Ğ° Ğ¼Ğ¾Ğ»Ğ¾ Ğ¹Ğ¾Ğ´Ñ‹ÑˆĞ»Ğ°Ğ½ Ğ²Ğ°ÑˆĞ¼ÑƒÑ‚Ñ‹Ğ¼ ĞœĞ°Ñ€Ğ¸Ğ¹ Ğ²



[pred] query: ĞŸÑ‘Ñ‚Ñ€ Ğ¸ Ğ¼Ğ¾Ğ»Ğ¾Ñ‚ Ğ¸ Ğ¼Ğ¾Ğ»Ğ¾Ñ‚ Ğ¸ Ğ¼Ğ¾Ğ»Ğ¾Ñ‚ ĞĞµÑÑ‚ĞµÑ€ÑĞºĞ¸Ğ¹ ĞµÑ€Ñ‚ĞµĞ´Ğµ Ğ¾Ñ€Ğ½Ñ‹Ò›Ñ‚Ñ‹ Â«ĞœĞ¾Ñ€Ğµ-
[true] query: ĞšĞ¾Ñ€ÑĞº Ñ€Ğ²ĞµĞ·Ğµ ĞŸÑ‘Ñ‚Ñ€ ĞĞµÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ´ĞµĞ½Ğµ Ğ¼Ñ‹Ğ¹ ĞĞ°Ñ€Ğ¾-Ğ¤Ğ¾Ğ¼Ğ¸Ğ½ÑĞº Ğ¾Ğ»Ğ°ÑˆÑ‚Ğµ ÑÑ€Ñ‚Ñ‹ÑˆĞµ Â«ĞŸĞ¾
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720191301/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720191301
{'eval_loss': 4.713248252868652, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.28937205817058786, 'eval_token_set_recall': 0.3868829559329568, 'eval_token_set_f1': 0.3272014272605409, 'eval_token_set_f1_sem': 0.004156225136138922, 'eval_n_ngrams_match_1': 4.192, 'eval_n_ngrams_match_2': 1.434, 'eval_n_ngrams_match_3': 0.246, 'eval_num_true_words': 13.858, 'eval_num_pred_words': 13.332, 'eval_bleu_score': 8.540225793925005, 'eval_bleu_score_sem': 0.17575969731045699, 'eval_rouge_score': 0.9064789210789213, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9044016599655151, 'eval_emb_cos_sim_sem': 0.011939386905035395, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 125.3591, 'eval_samples_per_second': 3.989, 'eval_steps_per_second': 0.503}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/mhr_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ğ¢ÑƒĞ´Ñ‹Ğ¼, ÑˆĞ°Ğ³Ñ‹Ğ¼ ÑˆĞ°Ğ³Ñ‹Ğ¼ ÑˆĞ°Ğ³Ñ‹Ğ¼ ÑˆĞ°Ğ³Ñ‹Ğ¼ ÑˆĞ°Ğ³Ñ‹Ğ¼ ĞšĞµÑˆĞµ Ò“Ğ°Ğ½Ğ° ĞšĞ¾Ğ»
[true] query: Ğ¢ÑƒÑ€Ğ³Ñ‹Ğ¼ Ğ¶Ğ°Ğ¿Ñ‹ÑˆÑ‚Ğµ, ÑˆĞ¾ÑˆĞ¾ Ğ°Ğ³Ğ° Ğ³Ğ¾Ğ´Ñ‹Ğ¼, ĞºĞµÑ‡ Ğ¸Ğº Ğ³Ğ°Ğ½Ğ° ĞšÑƒĞ³Ñƒ ĞšĞ°Ñ‡Ğ°Ğº ÑĞ»Ñ‹ÑˆÑ‚Ğµ Ğ»Ğ¸ÑÑˆ



[pred] query: Ğ–Ğ°Ğ¹ Ğ¼Ğ°? ĞµÑ€Ñ‚Ğµ Ğ¼Ğµ? ĞµÑ€Ñ‚Ğµ Ğ¼Ğµ? Ğ–Ğ°Ñ Ğ¼Ğ°? ĞµÑ€Ñ‚Ğµ Ğ¼Ğµ? Ğ˜Ğ·Ğ´Ğ°Ñ‚ĞµĞ»ÑŒÑĞºĞ¸Ğ¹ Ğ´Ğ¾Ğ¼ ĞšĞ¾ĞºÑˆĞµÑ‚Ğ°Ñƒ ĞœĞ°Ñ€
[true] query: ĞœĞ¾ Ñ‚Ñ‹Ğ³Ğ°Ğ¹ ĞĞ³Ğ°Ğ²Ğ°Ğ¹Ñ€ĞµĞ¼? ĞšÑƒĞ·Ğµ Ñ‚ÑƒĞ´Ğ¾ ÑÑ€Ñ‚Ğ°? Ğ¢Ğ¸Ğ´Ğµ Ğ´Ğ° Ğ¼Ğ¾Ğ»Ğ¾ Ğ¹Ğ¾Ğ´Ñ‹ÑˆĞ»Ğ°Ğ½ Ğ²Ğ°ÑˆĞ¼ÑƒÑ‚Ñ‹Ğ¼ ĞœĞ°Ñ€Ğ¸Ğ¹ Ğ²



[pred] query: Â«ĞĞµĞ¹Ñ€Ğ¾ Ğ¿Ğ¾Ñ€ĞµÑ‡ÑŒĞµÂ» Ó©Ñ‚Ğµ ĞµÑ€Ñ‚Ğµ Ò›Ğ¾Ğ½Ğ´Ñ‹ ĞĞµĞ¹Ñ€Ğ¾ Ğ¿Ğ¾Ñ€ĞµÑ‡ÑŒĞµ Ó©Ñ‚Ğµ ĞµÑ€Ñ‚Ğµ Ò›Ğ¾Ğ½Ğ´Ñ‹ -
[true] query: ĞšĞ¾Ñ€ÑĞº Ñ€Ğ²ĞµĞ·Ğµ ĞŸÑ‘Ñ‚Ñ€ ĞĞµÑÑ‚ĞµÑ€Ğ¾Ğ² Ğ´ĞµĞ½Ğµ Ğ¼Ñ‹Ğ¹ ĞĞ°Ñ€Ğ¾-Ğ¤Ğ¾Ğ¼Ğ¸Ğ½ÑĞº Ğ¾Ğ»Ğ°ÑˆÑ‚Ğµ ÑÑ€Ñ‚Ñ‹ÑˆĞµ Â«ĞŸĞ¾
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720195766/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720195766
{'eval_loss': 4.713248252868652, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.28923148746678196, 'eval_token_set_recall': 0.3797772239456458, 'eval_token_set_f1': 0.3249242089710039, 'eval_token_set_f1_sem': 0.004174261400022831, 'eval_n_ngrams_match_1': 4.188, 'eval_n_ngrams_match_2': 1.412, 'eval_n_ngrams_match_3': 0.238, 'eval_num_true_words': 13.858, 'eval_num_pred_words': 13.36, 'eval_bleu_score': 8.486697490646428, 'eval_bleu_score_sem': 0.19095999410867667, 'eval_rouge_score': 0.8901333333333339, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8939239382743835, 'eval_emb_cos_sim_sem': 0.013368142679041432, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 4391.1686, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/mhr_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating fin_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Uyanga muutoksen muutoksen... Uyanga muutoksen... Uyanga muutoksen... asiakasÑ‹Ğ½Ñ…Ğ°Ğ° Ğ°Ğ¶Ğ»Ñ‹Ğ½
[true] query: Tulemme jatkamaan asiakkaan kuuntelua, osallistamista ja haastamista. TyÃ¶ympÃ¤ristÃ¶ muutos hankkeissa tuotetaan myÃ¶



[pred] query: Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ´ Ğ¶Ğ¸Ğ·Ğ½ÑŒÑ - OKOKONO-Poland Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ´ Ğ¶Ğ¸Ğ·Ğ½ÑŒÑ - OKOKONO-Poland. Ğ¯ Ğ´Ğ¾Ğ»Ğ¶Ğ½Ğ°
[true] query: Pohjois-suomalaisuus ja kaikille arvokas elÃ¤mÃ¤ - siinÃ¤ tavoitteet toiminnalle. TyÃ¶skentelen Diakonia-



[pred] query: Ó¨Ó©Ñ€Ó©Ó©Ñ€ Ñ…ÑĞ»Ğ±ÑĞ», munchkina vesiculosus, odontopulmonalis, odontopulmonalis Ğ½ÑŒ
[true] query: Kun kirjoitimme koivunjalojen lÃ¤Ã¤ketieteellisistÃ¤ ominaisuuksista, mainitsimme, ettÃ¤ paitsi munuaiset, myÃ¶s ko
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720195928/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720195928
{'eval_loss': 6.019369125366211, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.19051080367001452, 'eval_token_set_recall': 0.2920668808229955, 'eval_token_set_f1': 0.22472110786388488, 'eval_token_set_f1_sem': 0.00284083951506914, 'eval_n_ngrams_match_1': 3.034, 'eval_n_ngrams_match_2': 1.052, 'eval_n_ngrams_match_3': 0.026, 'eval_num_true_words': 15.462, 'eval_num_pred_words': 15.51, 'eval_bleu_score': 5.297454881415487, 'eval_bleu_score_sem': 0.05981884107345722, 'eval_rouge_score': 0.10804852746945234, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8652195334434509, 'eval_emb_cos_sim_sem': 0.012211243297553773, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 125.1417, 'eval_samples_per_second': 3.995, 'eval_steps_per_second': 0.503}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/fin_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.44 GiB is free. Including non-PyTorch memory, this process has 36.11 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Ontwikkelingen umzuwandelen... Ontwikkelingen umzuwandelen... Ontwikkelingen umzuwandelen... Ğ¥Ò¯Ğ½Ğ¸Ğ¹ Ğ°Ğ¶Ğ»Ñ‹Ğ½
[true] query: Tulemme jatkamaan asiakkaan kuuntelua, osallistamista ja haastamista. TyÃ¶ympÃ¤ristÃ¶ muutos hankkeissa tuotetaan myÃ¶



[pred] query: Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ´ Ğ²ÑĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ´ Ğ²ÑĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ´ Ğ²ÑĞµĞ¼. Ğ Ğ°Ğ±Ğ¾Ñ‚Ğ° Ğ½Ğ°Ğ´ Ğ²ÑĞµĞ¼. OKOKONIA-
[true] query: Pohjois-suomalaisuus ja kaikille arvokas elÃ¤mÃ¤ - siinÃ¤ tavoitteet toiminnalle. TyÃ¶skentelen Diakonia-



[pred] query: Ğ¡ÑƒĞ´Ğ°Ğ»Ğ³Ğ°Ğ°Ğ½Ñ‹ ÑˆĞ¸Ğ½Ğ¶Ğ¸Ğ»Ğ³ÑÑĞ½Ğ´ Ğ´ÑƒÑ€Ğ´ÑĞ°Ğ½Ñ‡Ğ»Ğ°Ğ½, odontopulmonalis, odontopulmonalis Ğ½ÑŒ Ğ±ÑƒÑ,
[true] query: Kun kirjoitimme koivunjalojen lÃ¤Ã¤ketieteellisistÃ¤ ominaisuuksista, mainitsimme, ettÃ¤ paitsi munuaiset, myÃ¶s ko
outptufile for decoded sequences:  saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720200400/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_cyrl-script_32_2layers_corrector/_decoded_eval_1720200400
{'eval_loss': 6.019369125366211, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.19152016606544492, 'eval_token_set_recall': 0.2809609443270571, 'eval_token_set_f1': 0.22239973861649007, 'eval_token_set_f1_sem': 0.0028798165073143384, 'eval_n_ngrams_match_1': 3.05, 'eval_n_ngrams_match_2': 1.044, 'eval_n_ngrams_match_3': 0.022, 'eval_num_true_words': 15.462, 'eval_num_pred_words': 16.12, 'eval_bleu_score': 5.16199414431144, 'eval_bleu_score_sem': 0.05987237343669929, 'eval_rouge_score': 0.10726865894782778, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.861629843711853, 'eval_emb_cos_sim_sem': 0.012426906087205987, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4399.1527, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_cyr_scrp_32_2layers_prefix/evaluations/fin_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.96 GiB is free. Including non-PyTorch memory, this process has 36.59 GiB memory in use. Of the allocated memory 29.91 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
