working directory /home/cs.aau.dk/ng78zb/vec2text_exp
sif /home/cs.aau.dk/ng78zb/pytorch_23.10-py3.sif
launch evaluation yiyic/mt5_me5_indo-aryan-fami_32_2layers_corrector
loading experiment and trainer from yiyic/mt5_me5_indo-aryan-fami_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector
on rank 0, output dir: saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_inverter
on rank 0, output dir: saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_inverter
adding embedding type to dataset args.
Loading datasets with TOKENIZERS_PARALLELISM = False
loading train dataset from path: /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/563419ae595a6ce34101b4f4bb0ae282.arrow
loaded dict of val datasets from /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/b7c2d88873c9bb4061ee54b83d4d22ce.arrow
07/05/2024 09:57:31 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
07/05/2024 09:58:34 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
data arguments for experiment: DataArguments(dataset_name='mt-ms_ind_fami', max_eval_samples=500, use_less_data=3000)
model yiyic/mt5_me5_indo-aryan-fami_32_2layers_corrector parameters 890566656
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
Using Frozen Embeddings as Input -- Val datasets
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '24e7b7ef1b0606ddc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7cf9deee5c6ee34dc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7d3c7a3fdc899099c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a808a07212534aa6c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '1835610d5c7fc595c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '74575508bfab4c9cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'af6bef1b30e3c5dbc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'bfa2f55e85bb2562c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd839c9a8871bbce8c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '67555e67fd16afc9c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '48ec325196197014c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a20c740ae5a0d86bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'cc2235accdffe49bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '104320b250ecc6cac111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd7b432ef61da1e93c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'c06e214a198bcc2cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '3ae3df71733802abc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '18a40ab4beb7f210c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '96cd5258a58b461bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7ee1e04e96a3faa2c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
output dir ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations
evaluating deu_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/deu_Latn_steps-1.json already exists
evaluating corrector with steps 20
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/deu_Latn_steps-20.json already exists
evaluating corrector with steps 50
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/deu_Latn_steps-50.json already exists
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/deu_Latn_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: Allerbeste artikelen in Allerbeste artikelen in Allerbeste artikelen in Allerbeste artikelen in Allerbeste artikelen You are here:
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Unser Salon offre a große Palette von Dienstleistungen für Beauty, Beauty & Hair. Unser Salon offre a
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: ✅ Ausgewählte Bestsellerprodukte Anfang 2021 ✅ Ausgewählte Bestsellerprodukte Anfang 2021 ✅ Ausgewählte Bestsellerprodukte A
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720183157/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720183157
{'eval_loss': 3.5206332206726074, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.25078923053360425, 'eval_token_set_recall': 0.3944361528702527, 'eval_token_set_f1': 0.29868720036525764, 'eval_token_set_f1_sem': 0.004774573992163241, 'eval_n_ngrams_match_1': 4.978, 'eval_n_ngrams_match_2': 1.426, 'eval_n_ngrams_match_3': 0.172, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 17.784, 'eval_bleu_score': 5.7160701976466575, 'eval_bleu_score_sem': 0.17681959083549167, 'eval_rouge_score': 0.20677653994750056, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9125074148178101, 'eval_emb_cos_sim_sem': 0.015390544716857506, 'eval_emb_top1_equal': 0.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 16724.6858, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/deu_Latn_steps-50_sbeam-8.json
evaluating ydd_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/ydd_Hebr_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 3.91 GiB is free. Including non-PyTorch memory, this process has 40.64 GiB memory in use. Of the allocated memory 35.29 GiB is allocated by PyTorch, and 5.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.43 GiB is free. Including non-PyTorch memory, this process has 39.12 GiB memory in use. Of the allocated memory 35.29 GiB is allocated by PyTorch, and 3.52 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/ydd_Hebr_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: ટાઇટેન / ચીન (હિન્દુસ્તાન) ટાઇટેન પાલિકાએ 5 ક્વે
[true] query: טשיינאַ (מעריאַט / הילטאָן) האָטעל קאַלעקשאַן עוראָטאָפּ 5 שטערן האָטעל מאַטראַ



[pred] query: اذا تسجد، اذا تسجد، اذا تسجد - Geofumadas اذا تسجد، اذا تسجد
[true] query: חול־המועד סוכּות, תּשע״ז - yiddish.forward.com חול־המועד סוכּ



[pred] query: તમે મમતા ટ્રમ્પને અટકાવશે : INTERVIEW તમે મમતા ટ્રમ્પને અટકાવશે
[true] query: איראן פראוואקירט טראמפ: מיר וועלן אויסברייטערן דעם מיסיל פראגראם טראץ אפמא
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720199845/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720199845
{'eval_loss': 8.041914939880371, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.24664870571464995, 'eval_token_set_recall': 0.3123888235947062, 'eval_token_set_f1': 0.2731458071648522, 'eval_token_set_f1_sem': 0.003837196068483793, 'eval_n_ngrams_match_1': 3.544, 'eval_n_ngrams_match_2': 1.22, 'eval_n_ngrams_match_3': 0.104, 'eval_num_true_words': 14.482, 'eval_num_pred_words': 13.328, 'eval_bleu_score': 6.919597989466735, 'eval_bleu_score_sem': 0.15765172626069285, 'eval_rouge_score': 0.6837006439552884, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8660213351249695, 'eval_emb_cos_sim_sem': 0.053627312870473474, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 16554.015, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/ydd_Hebr_steps-50_sbeam-8.json
evaluating heb_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/heb_Hebr_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.72 GiB is free. Including non-PyTorch memory, this process has 38.83 GiB memory in use. Of the allocated memory 35.29 GiB is allocated by PyTorch, and 3.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/heb_Hebr_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: لقد وضعت حديث حول موضوع بحث في فترات سابقه ("لقد وضعت حديث في فت
[true] query: במחקר שהתפרסם לאחרונה (ואני מתנצל שלא הגעתי לדון בו עד כה מפאת עניינים אחר



[pred] query:.......... इस कार्यक्रम की पहचान सीमित
[true] query: תוכנית ריאליטי בישראל היא לא רק תוכנית ריאליטי. היא שיעור באזרחות,



[pred] query: درخواست اجازت ، درخواست اجازت ، درخواست اجازت مورخه 20.04.2011 بموجب قانون
[true] query: בפני בקשה לעיכוב ביצוע פסק הדין אשר ניתן ביום 20.4.11 ואשר במסגרתו חו
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720216616/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720216616
{'eval_loss': 6.787135601043701, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.16796377792314357, 'eval_token_set_recall': 0.2395788597797118, 'eval_token_set_f1': 0.19312166227757854, 'eval_token_set_f1_sem': 0.0027218543400651125, 'eval_n_ngrams_match_1': 2.678, 'eval_n_ngrams_match_2': 1.058, 'eval_n_ngrams_match_3': 0.032, 'eval_num_true_words': 15.944, 'eval_num_pred_words': 16.332, 'eval_bleu_score': 5.119799858499721, 'eval_bleu_score_sem': 0.04958075121962351, 'eval_rouge_score': 0.7936208680208676, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8400965929031372, 'eval_emb_cos_sim_sem': 0.03574800596867079, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.49999999144286444, 'eval_runtime': 16638.6814, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/heb_Hebr_steps-50_sbeam-8.json
evaluating arb_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/arb_Arab_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/arb_Arab_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: فستان يزين بُزُود، يزين بُزُود از مكة المكرمة Floral Designs
[true] query: فستان زفاف أنيق بقصّة الأميرة من نسيج الميكادو، مُزيّن بالأزهار



[pred] query: رئيس مجلس القرآن الكريم محمد رؤية العثمانية يقدم برنامج القسط20 من الشبكة
[true] query: رئيس القمة الدينية لمجموعة العشرين الشيخ د.محمد العيسى يطلق منصة R20 



[pred] query: جامعة العلوم الإسلامية يوسع شعبة الدعوة الكبيرة بين السنوات والسمعة في العالم،
[true] query: تسعى كلية العلوم الإسلامية إلى تبوأ مكانة وسمعة مرموقة بين جامعات العالم، 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720233413/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720233413
{'eval_loss': 2.979206085205078, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2627956696983789, 'eval_token_set_recall': 0.33830103877890316, 'eval_token_set_f1': 0.29147128091543173, 'eval_token_set_f1_sem': 0.00429897404475096, 'eval_n_ngrams_match_1': 4.136, 'eval_n_ngrams_match_2': 1.308, 'eval_n_ngrams_match_3': 0.126, 'eval_num_true_words': 15.48, 'eval_num_pred_words': 15.22, 'eval_bleu_score': 6.888019664774849, 'eval_bleu_score_sem': 0.14200771142414548, 'eval_rouge_score': 0.8925904249767526, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9397159814834595, 'eval_emb_cos_sim_sem': 0.01539301821002949, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.49999999144286444, 'eval_runtime': 16664.656, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/arb_Arab_steps-50_sbeam-8.json
evaluating amh_Ethi val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/amh_Ethi_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/amh_Ethi_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: ਮਈ 20, 2016 ਜ਼ਿਲ•ੇ ਦੀ ਡਾਮਿਰੇ ਅਤੇ ਹਸਪਤਾਲ ਦੀ RayHaber ਮੁੱਖਤੁ
[true] query: ማርች 20, 2016 የራስ ዱሜራ ደሴቶችና የባህር ግዛት የሚያሳን የኤ



[pred] query: ਜਥੇਬੰਦੀ ਦੀ ਸ਼ਮੂਲੀਅਤ - Strength of Unite ਜਥੇਬੰਦੀ ਦੀ ਸ਼ਮੂਲ
[true] query: አንድነት አንድነት ውስጥ ጥንካሬ - የህብረት ማህበረሰብ እንክብካቤ



[pred] query: Yugoslavia 'ਤੇ ਖੇਡਣ ਵਾਲੇ 20 ਖਿਡਾਰੀਆਂ ਦੀ ਖ਼ੁਦਕੁਸ਼ੀ ਸੋਮਵਾਰ ਹੋ
[true] query: ከቡናማዎቹ ጋር ነገ ወደ ዩጋንዳ የሚያቀኑ 20 ተጫዋቾች ተለይተው
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720250105/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720250105
{'eval_loss': 8.068629264831543, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.28310399949208037, 'eval_token_set_recall': 0.28234415649709826, 'eval_token_set_f1': 0.2803549115342211, 'eval_token_set_f1_sem': 0.004024218687472945, 'eval_n_ngrams_match_1': 3.388, 'eval_n_ngrams_match_2': 1.354, 'eval_n_ngrams_match_3': 0.164, 'eval_num_true_words': 11.838, 'eval_num_pred_words': 12.85, 'eval_bleu_score': 8.306952110448156, 'eval_bleu_score_sem': 0.1914097418528602, 'eval_rouge_score': 0.6135073782168986, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8622118234634399, 'eval_emb_cos_sim_sem': 0.007809251266236879, 'eval_emb_top1_equal': 0.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 16559.2303, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/amh_Ethi_steps-50_sbeam-8.json
evaluating mlt_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/mlt_Latn_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/mlt_Latn_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: Musculul Disorders and Triggers - Technical Guides Musculul Disorders and Triggers मुख्यपृ
[true] query: Dħul Temi Temi Problemi muskuloskeletali Practical tools and guidance - Musculoskeletal disorders



[pred] query: - Khabar-e-Jiddish News a first test of coronavirus’s a claim in Moldova, 
[true] query: test – One News Mindu feġġ l-ewwel każ ta' coronavirus f'pajjiżna, saru aktar



[pred] query: 'Mf appelle l'interdiction de l'iGaming...' Mf appelle l'interdiction
[true] query: 'L-MFA emmnet lil min ibagħbas il-logħob u mhux lili' - Illum
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720266817/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720266817
{'eval_loss': 5.298429489135742, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.25376925644874576, 'eval_token_set_recall': 0.3392265187942437, 'eval_token_set_f1': 0.2845690859427, 'eval_token_set_f1_sem': 0.004027364914810536, 'eval_n_ngrams_match_1': 3.588, 'eval_n_ngrams_match_2': 1.244, 'eval_n_ngrams_match_3': 0.124, 'eval_num_true_words': 13.932, 'eval_num_pred_words': 12.89, 'eval_bleu_score': 7.076748549833961, 'eval_bleu_score_sem': 0.14878764795435767, 'eval_rouge_score': 0.15334759860843306, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9122253656387329, 'eval_emb_cos_sim_sem': 0.03361567836449322, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.49999999144286444, 'eval_runtime': 16580.4451, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/mlt_Latn_steps-50_sbeam-8.json
evaluating hin_Deva val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/hin_Deva_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/hin_Deva_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: NEWS BLOG: ग़लत न करे मुसलमान!! ग़लत न करे मुसलमान!! ग़लत
[true] query: Blog News: ग़लती न करे मुसलमान!!! ग़लती न करे मुसलमान!!! 



[pred] query: RBSE Class 9 Solutions for Class 11 Names Hindi - RBSE Solutions RBSE Class 9 Solutions for Class 11 Names Hindi रा
[true] query: RBSE Solutions for Class 9 Hindi Sparsh Chapter 11 आदमी नामा - Rbse solutions RBSE Solutions for Class 9



[pred] query: नई दिल्ली, प्रधानमंत्री नरेंद्र मोदी का कार्यकाल पूरा होने का समय में सिर्फ 14 महीने बा
[true] query: नई दिल्ली, प्रधानमंत्री नरेंद्र मोदी का कार्यकाल पूरा होने में केवल 14 महीने का वक्त बा
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720283812/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720283812
{'eval_loss': 1.0759841203689575, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.7541772326949905, 'eval_token_set_recall': 0.7692829511905361, 'eval_token_set_f1': 0.7606485695392912, 'eval_token_set_f1_sem': 0.008201132616194, 'eval_n_ngrams_match_1': 13.098, 'eval_n_ngrams_match_2': 8.258, 'eval_n_ngrams_match_3': 5.71, 'eval_num_true_words': 17.206, 'eval_num_pred_words': 17.182, 'eval_bleu_score': 42.638876545616114, 'eval_bleu_score_sem': 1.3806795748008869, 'eval_rouge_score': 0.8440450684041529, 'eval_exact_match': 0.122, 'eval_exact_match_sem': 0.01465132494504475, 'eval_emb_cos_sim': 0.974837601184845, 'eval_emb_cos_sim_sem': 0.012311637090706724, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 16863.2028, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/hin_Deva_steps-50_sbeam-8.json
evaluating urd_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/urd_Arab_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/urd_Arab_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: قائداعظم سے لکھاری تک ۔۔ رؤف کلاسرا صفحہ مرکزی / نادرا /
[true] query: قائداعظم سے نہرو تک ۔۔ رؤف کلاسرا مرکزی صفحہ/ لکھاری/ ر



[pred] query: سوناکشی سنہا سوشل میڈیا میمز کے نشے میں مبتلا ہو گئیں نہ صرف اداکارہ خود
[true] query: سوناکشی سنہا سوشل میڈیا میمز کے نشے میں مبتلا ہو گئیں اداکارہ نہ صرف خود



[pred] query: نئی دہلی:مہاراشٹر میں گزشتہ 5 سالوں میں 142400 کسانوں (2017-18) نے خود
[true] query: نئی دہلی:مہاراشٹر میں گزشتہ 5 سالوں میں (18-2014)14034 کسانوں نے خود
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720300653/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720300653
{'eval_loss': 0.9809061288833618, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.7697938542313489, 'eval_token_set_recall': 0.785313671239247, 'eval_token_set_f1': 0.7766753374205929, 'eval_token_set_f1_sem': 0.00817293531417541, 'eval_n_ngrams_match_1': 13.23, 'eval_n_ngrams_match_2': 8.838, 'eval_n_ngrams_match_3': 6.384, 'eval_num_true_words': 17.024, 'eval_num_pred_words': 16.922, 'eval_bleu_score': 47.71323674184085, 'eval_bleu_score_sem': 1.385710338295116, 'eval_rouge_score': 0.9118565247446058, 'eval_exact_match': 0.142, 'eval_exact_match_sem': 0.01562563024781025, 'eval_emb_cos_sim': 0.9832106828689575, 'eval_emb_cos_sim_sem': 0.012846111813385498, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.49999999144286444, 'eval_runtime': 16708.1172, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/urd_Arab_steps-50_sbeam-8.json
evaluating guj_Gujr val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/guj_Gujr_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/guj_Gujr_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: 'અમારા લગ્નમાં વિરોધી કેમ બોલાવ્યાં?' : કન્યા માતાના કહેવાથી
[true] query: 'અમારા વિરોધીને લગ્નમાં કેમ બોલાવ્યો?' કહી કન્યાના મા-બા



[pred] query: આજથી સાંજે કેજરીવાલની ગુજરાત યાત્રા શરૂ, જાણો આખો કાર્યક્રમ | Read here
[true] query: આજે સાંજથી કેજરીવાલની ગુજરાત યાત્રા શરૂ, જાણો આખો કાર્યક્રમ | Read here



[pred] query: શાહરુખ ખાનની ગુજરાતી ડૉન ફરહાન 3માં જોવા મળશે | Shahrukh Khann
[true] query: ફરહાનની ડૉન 3માં જોવા મળશે ગુજરાતી શાહરુખ ખાન | Shahrukh Kahn
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720317469/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720317469
{'eval_loss': 0.9072681665420532, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.7287862165612168, 'eval_token_set_recall': 0.7402286587595424, 'eval_token_set_f1': 0.733789474056853, 'eval_token_set_f1_sem': 0.009238929955238824, 'eval_n_ngrams_match_1': 9.668, 'eval_n_ngrams_match_2': 6.186, 'eval_n_ngrams_match_3': 4.28, 'eval_num_true_words': 13.074, 'eval_num_pred_words': 12.94, 'eval_bleu_score': 43.48031710520364, 'eval_bleu_score_sem': 1.44887494296916, 'eval_rouge_score': 0.9479498313382526, 'eval_exact_match': 0.154, 'eval_exact_match_sem': 0.016158285192455334, 'eval_emb_cos_sim': 0.9952843189239502, 'eval_emb_cos_sim_sem': 0.0014307498467762078, 'eval_emb_top1_equal': 0.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 16684.489, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/guj_Gujr_steps-50_sbeam-8.json
evaluating sin_Sinh val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/sin_Sinh_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/sin_Sinh_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
[pred] query: ਦਰਸ਼ਨ ਕਰਨ ਵਾਲੇ ਲੋਕ | Youth Guru 16 ਮਾਰਚ → ਜਦੋਂ ਅਸੀਂ ਕਿਸੇ ਨੂੰ ਦੇਖ
[true] query: මිනිස්සු | ඇවිද යන මඟ ← ගරු කිරීම ගුරුවරු → 16 අප් රේල් අපිට කෙනෙක් දැක්කාම ආ



[pred] query: सिमौलिया चिड़िया... | चिट्ठा सिमौलिया चिड़िया... Published March 24, 2017 March 24, 2017
[true] query: සිවුමැලියා සිකුරු ලියා... | Sunday Apple සිවුමැලියා සිකුරු ලියා... March 24, 2017 | 11:00 am 0



[pred] query: ਅੰਦੋਲਨ ਦੇ ਵੇਲੇ ਹੋਈ ਸੀ ਲੜਕੀ... - Sri Lanka News ਅੰਦੋਲਨ ਦੇ ਵੇਲੇ
[true] query: නිළිකමට මැදි වුණේ පුංචි කෙල්ලක කාලේ... - Sri Lanka News Update නිළිකමට මැ
outptufile for decoded sequences:  saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720334246/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_indo-aryan-fami_32_2layers_corrector/_decoded_eval_1720334246
{'eval_loss': 8.262213706970215, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.26246963705369303, 'eval_token_set_recall': 0.3040544459668303, 'eval_token_set_f1': 0.2795572028188946, 'eval_token_set_f1_sem': 0.004471384937725435, 'eval_n_ngrams_match_1': 3.812, 'eval_n_ngrams_match_2': 1.426, 'eval_n_ngrams_match_3': 0.196, 'eval_num_true_words': 14.694, 'eval_num_pred_words': 13.828, 'eval_bleu_score': 7.8534206146855565, 'eval_bleu_score_sem': 0.24629869705546575, 'eval_rouge_score': 0.7297862933677952, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9323107004165649, 'eval_emb_cos_sim_sem': 0.008444160611828098, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 16645.0918, 'eval_samples_per_second': 0.03, 'eval_steps_per_second': 0.015}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/sin_Sinh_steps-50_sbeam-8.json
evaluating pan_Guru val_dataset
evaluating corrector 
evaluating corrector with steps 1
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/pan_Guru_steps-1.json already exists
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 7.39 GiB. GPU 0 has a total capacty of 44.56 GiB of which 5.96 GiB is free. Including non-PyTorch memory, this process has 38.59 GiB memory in use. Of the allocated memory 27.89 GiB is allocated by PyTorch, and 10.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with beam width 4
./saves/correctors/mt5_multilingual_e5_base_mt-ms_ind_fami_32_2layers_prefix/evaluations/pan_Guru_steps-50_sbeam-4.json already exists
evaluating corrector with beam width 8
