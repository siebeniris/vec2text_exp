working directory /home/cs.aau.dk/ng78zb/vec2text_exp
sif /home/cs.aau.dk/ng78zb/pytorch_23.10-py3.sif
launch evaluation yiyic/mt5_me5_heb_Hebr_32_2layers_corrector with batch size 8
loading experiment and trainer from yiyic/mt5_me5_heb_Hebr_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector
on rank 0, output dir: saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector
num_workers 7
Set num workers to 7
Experiment output_dir = saves/yiyic__mt5_me5_heb_Hebr_32_2layers_inverter
on rank 0, output dir: saves/yiyic__mt5_me5_heb_Hebr_32_2layers_inverter
adding embedding type to dataset args.
Loading datasets with TOKENIZERS_PARALLELISM = False
loading train dataset from path: /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/af6f02deb324db2c00c681a36bd8e08c.arrow
loaded dict of val datasets from /home/cs.aau.dk/ng78zb/vec2text_exp/.cache/inversion/b7c2d88873c9bb4061ee54b83d4d22ce.arrow
07/04/2024 14:33:21 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
07/04/2024 14:34:24 - WARNING - accelerate.utils.other - Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.
data arguments for experiment: DataArguments(dataset_name='mt-ms_heb_Hebr', max_eval_samples=500, use_less_data=3000)
model yiyic/mt5_me5_heb_Hebr_32_2layers_corrector parameters 890566656
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
dataset kwargs: {'remove_columns': ['text'], 'batched': True, 'batch_size': 1024, 'num_proc': 6, 'desc': 'Running tokenizer on dataset'}
Using Frozen Embeddings as Input -- Val datasets
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '24e7b7ef1b0606ddc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7cf9deee5c6ee34dc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7d3c7a3fdc899099c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a808a07212534aa6c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '1835610d5c7fc595c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '74575508bfab4c9cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'af6bef1b30e3c5dbc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'bfa2f55e85bb2562c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd839c9a8871bbce8c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '67555e67fd16afc9c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '48ec325196197014c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'a20c740ae5a0d86bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'cc2235accdffe49bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '104320b250ecc6cac111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'd7b432ef61da1e93c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': 'c06e214a198bcc2cc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '3ae3df71733802abc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '18a40ab4beb7f210c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '96cd5258a58b461bc111194716da8091da112d4db2a7a08e', 'num_proc': 1}
dataset kwargs: {'batched': True, 'batch_size': 128, 'new_fingerprint': '7ee1e04e96a3faa2c111194716da8091da112d4db2a7a08e', 'num_proc': 1}
output dir ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations
evaluating deu_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: בכל item in the Menu, מופיעות קטגוריות: Sparesort und Qualität. Contents in the Menu הופ
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Our Beauty & Spa Center מתמחה בתחום טיפולי שיער, בריאות וטיפול. ואת כולו, 
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: דף הבית › מוצר / מוצר / מוצר / מוצר / Best-end tested 2019/20
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720096764/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720096764
{'eval_loss': 3.7577028274536133, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.218752284398048, 'eval_token_set_recall': 0.2817972506016591, 'eval_token_set_f1': 0.24329979986110223, 'eval_token_set_f1_sem': 0.003668969870088624, 'eval_n_ngrams_match_1': 4.298, 'eval_n_ngrams_match_2': 1.18, 'eval_n_ngrams_match_3': 0.07, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 17.182, 'eval_bleu_score': 4.9936635989283245, 'eval_bleu_score_sem': 0.06966469617780617, 'eval_rouge_score': 0.17161735466775255, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8845418691635132, 'eval_emb_cos_sim_sem': 0.011223225020960252, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 120.0249, 'eval_samples_per_second': 4.166, 'eval_steps_per_second': 0.525}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/deu_Latn_steps-1.json
evaluating corrector with steps 20
[pred] query: content in the page: Choose the right selection and the right selection. content in the page: Sparesorts in Stuttgart
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Our Beauty & Spa Center מתמחה בתחום טיפולי שיער, בריאות ובריאות. ואת כולו, 
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: Bestsellers 2019 / Bestsellers 2019 / Bestsellers 2019 / Bestsellers 2019 / Bestsellers 
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720097527/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720097527
{'eval_loss': 3.7577028274536133, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22448750963332026, 'eval_token_set_recall': 0.2905779551713896, 'eval_token_set_f1': 0.24962679255010806, 'eval_token_set_f1_sem': 0.0037228028647951538, 'eval_n_ngrams_match_1': 4.398, 'eval_n_ngrams_match_2': 1.24, 'eval_n_ngrams_match_3': 0.092, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 17.588, 'eval_bleu_score': 5.093522279365589, 'eval_bleu_score_sem': 0.07651030441310326, 'eval_rouge_score': 0.1735481313357923, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8865172863006592, 'eval_emb_cos_sim_sem': 0.015977226213951118, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 761.909, 'eval_samples_per_second': 0.656, 'eval_steps_per_second': 0.083}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/deu_Latn_steps-20.json
evaluating corrector with steps 50
[pred] query: content in the page: Choose the right selection and the right selection. content in the page: Sparesorts in Stuttgart
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Our Beauty & Spa Center מתמחה בתחום טיפולי שיער, בריאות ובריאות. ואת כולו, 
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: Bestsellers 2019 / Bestsellers 2019 / Bestsellers 2019 / Bestsellers 2019 / Bestsellers 
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720099202/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720099202
{'eval_loss': 3.7577028274536133, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22448750963332026, 'eval_token_set_recall': 0.2906446218380563, 'eval_token_set_f1': 0.24965536397867946, 'eval_token_set_f1_sem': 0.0037234674762498905, 'eval_n_ngrams_match_1': 4.398, 'eval_n_ngrams_match_2': 1.24, 'eval_n_ngrams_match_3': 0.092, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 17.586, 'eval_bleu_score': 5.093481252959889, 'eval_bleu_score_sem': 0.07651112776467622, 'eval_rouge_score': 0.17355666792115815, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8865172863006592, 'eval_emb_cos_sim_sem': 0.015977226213951118, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 1675.5852, 'eval_samples_per_second': 0.298, 'eval_steps_per_second': 0.038}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/deu_Latn_steps-50.json
evaluating corrector with steps 50 and beam width 4
[pred] query: בכל קטגוריה, תוכלו למצוא את הקטגוריה "Besten Preis und Qualität in Zustellung". Rubriken
[true] query: In unserem Themenverzeichnis finden Sie alle wichtigen Informationen zum Thema Conveniencestore. Die Artikel sind nach Relevanz sort



[pred] query: Our Beauty & Wellness Center מתמחה בתחום טיפולי שיער עם מעל 20,000 שירותים. ואתם,
[true] query: Unser Hair and Beauty Studio bietet Ihnen eine grosse Palette an Dienstleistungen rund um Wellness und Beauty. Dabei geht



[pred] query: דף הבית / Bestsellers 2020/2021 / Bestsellers 2020/2021 / מוצר מצוין מצטיין 
[true] query: ❱ Unsere Bestenliste Dec/2022 ᐅ Ausführlicher Produkttest ☑ Ausgezeichnete Produkte ☑ Aktuelle
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720103649/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720103649
{'eval_loss': 3.7577028274536133, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.217467742807439, 'eval_token_set_recall': 0.2913992111947222, 'eval_token_set_f1': 0.24480454707108876, 'eval_token_set_f1_sem': 0.0038468443086918124, 'eval_n_ngrams_match_1': 4.278, 'eval_n_ngrams_match_2': 1.208, 'eval_n_ngrams_match_3': 0.082, 'eval_num_true_words': 19.254, 'eval_num_pred_words': 16.908, 'eval_bleu_score': 4.976712763621582, 'eval_bleu_score_sem': 0.08820651225398801, 'eval_rouge_score': 0.1706830613236923, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8910987377166748, 'eval_emb_cos_sim_sem': 0.011429000423369614, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4446.859, 'eval_samples_per_second': 0.112, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/deu_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 10.46 GiB is free. Including non-PyTorch memory, this process has 34.09 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 3.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating ydd_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: טיוואן / קליבלנד / קליבלנד / קליבלנד / קליבלנד השיק 5 ערים
[true] query: טשיינאַ (מעריאַט / הילטאָן) האָטעל קאַלעקשאַן עוראָטאָפּ 5 שטערן האָטעל מאַטראַ



[pred] query: חול-הסוכות. חול-הסוכות. חול-הסוכות. חול-הסוכות המו
[true] query: חול־המועד סוכּות, תּשע״ז - yiddish.forward.com חול־המועד סוכּ



[pred] query: אנחנו תצטרכו להמיר את טראמפ: מירוץ מירוץ מירוץ מירוץ פאלאס פאלאס
[true] query: איראן פראוואקירט טראמפ: מיר וועלן אויסברייטערן דעם מיסיל פראגראם טראץ אפמא
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720103820/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720103820
{'eval_loss': 4.415439128875732, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2512882732592485, 'eval_token_set_recall': 0.36918930807754397, 'eval_token_set_f1': 0.29096045264947085, 'eval_token_set_f1_sem': 0.00394981826427146, 'eval_n_ngrams_match_1': 3.63, 'eval_n_ngrams_match_2': 1.148, 'eval_n_ngrams_match_3': 0.066, 'eval_num_true_words': 14.482, 'eval_num_pred_words': 14.398, 'eval_bleu_score': 6.3399781288172115, 'eval_bleu_score_sem': 0.08862710124374287, 'eval_rouge_score': 0.7686179565360682, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9195551872253418, 'eval_emb_cos_sim_sem': 0.012099497514172218, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 132.3863, 'eval_samples_per_second': 3.777, 'eval_steps_per_second': 0.476}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/ydd_Hebr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 9.16 GiB is free. Including non-PyTorch memory, this process has 35.39 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.85 GiB is free. Including non-PyTorch memory, this process has 36.70 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: קליבלנד / קליבלנד / קליבלנד / קליבלנד / קליבלנד משלב ערים 
[true] query: טשיינאַ (מעריאַט / הילטאָן) האָטעל קאַלעקשאַן עוראָטאָפּ 5 שטערן האָטעל מאַטראַ



[pred] query: חול-הסוכות. חול-הסוכות. חול-הסוכות. חול-הסוכות יו
[true] query: חול־המועד סוכּות, תּשע״ז - yiddish.forward.com חול־המועד סוכּ



[pred] query: אנחנו מאמינים: טראמפ תסיר את טירוף פראנסוואט פראנסוואט פראנסוואט פאר
[true] query: איראן פראוואקירט טראמפ: מיר וועלן אויסברייטערן דעם מיסיל פראגראם טראץ אפמא
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720108330/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720108330
{'eval_loss': 4.415439128875732, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2553209347154393, 'eval_token_set_recall': 0.3718803577055906, 'eval_token_set_f1': 0.292930466525246, 'eval_token_set_f1_sem': 0.003964315672285283, 'eval_n_ngrams_match_1': 3.686, 'eval_n_ngrams_match_2': 1.168, 'eval_n_ngrams_match_3': 0.08, 'eval_num_true_words': 14.482, 'eval_num_pred_words': 14.68, 'eval_bleu_score': 6.415157960619598, 'eval_bleu_score_sem': 0.09698113129379195, 'eval_rouge_score': 0.7659373429365317, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9166430234909058, 'eval_emb_cos_sim_sem': 0.013270222379060465, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4433.3209, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/ydd_Hebr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating heb_Hebr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: למרות שהתפרסם באחרונה בכתבה חדשה (אני מודה שעדיין לא נתקלתי בנושאים אחר
[true] query: במחקר שהתפרסם לאחרונה (ואני מתנצל שלא הגעתי לדון בו עד כה מפאת עניינים אחר



[pred] query: תוכנית ריאליטי בישראל היא לא רק מעמד במדינת ישראל. היא תוכנית ריאליטי
[true] query: תוכנית ריאליטי בישראל היא לא רק תוכנית ריאליטי. היא שיעור באזרחות,



[pred] query: שלום רב בקשת רשות ערעור אשר ניתן לעיכוב החלטתו ביום 20.11.20 וזאת לפי
[true] query: בפני בקשה לעיכוב ביצוע פסק הדין אשר ניתן ביום 20.4.11 ואשר במסגרתו חו
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720108495/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720108495
{'eval_loss': 1.4604164361953735, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.48072156245027775, 'eval_token_set_recall': 0.5179460826754941, 'eval_token_set_f1': 0.49722489844231, 'eval_token_set_f1_sem': 0.006270283335140089, 'eval_n_ngrams_match_1': 7.82, 'eval_n_ngrams_match_2': 3.442, 'eval_n_ngrams_match_3': 1.484, 'eval_num_true_words': 15.944, 'eval_num_pred_words': 15.504, 'eval_bleu_score': 16.318948510255918, 'eval_bleu_score_sem': 0.548003575123794, 'eval_rouge_score': 0.9230136943973902, 'eval_exact_match': 0.004, 'eval_exact_match_sem': 0.002825591608118863, 'eval_emb_cos_sim': 0.9587972164154053, 'eval_emb_cos_sim_sem': 0.01134826681795798, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 127.8387, 'eval_samples_per_second': 3.911, 'eval_steps_per_second': 0.493}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/heb_Hebr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: למרות שהתפרסם לאחרונה בכתבה חדשה (אני מודה שעדיין לא נתקלתי בנושאים אחר
[true] query: במחקר שהתפרסם לאחרונה (ואני מתנצל שלא הגעתי לדון בו עד כה מפאת עניינים אחר



[pred] query: תוכנית ריאליטי בישראל היא לא רק רמת מעמד במדינת ישראל. היא תרבות,
[true] query: תוכנית ריאליטי בישראל היא לא רק תוכנית ריאליטי. היא שיעור באזרחות,



[pred] query: שלום רב בקשת רשות ערעור אשר ניתן לעיכוב החלטתו ביום 20.11.20 ולאחר מ
[true] query: בפני בקשה לעיכוב ביצוע פסק הדין אשר ניתן ביום 20.4.11 ואשר במסגרתו חו
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720112961/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720112961
{'eval_loss': 1.4604164361953735, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.502625171543283, 'eval_token_set_recall': 0.5283032981724156, 'eval_token_set_f1': 0.513950757535441, 'eval_token_set_f1_sem': 0.006935128627007958, 'eval_n_ngrams_match_1': 8.144, 'eval_n_ngrams_match_2': 3.696, 'eval_n_ngrams_match_3': 1.722, 'eval_num_true_words': 15.944, 'eval_num_pred_words': 15.576, 'eval_bleu_score': 17.906454374169044, 'eval_bleu_score_sem': 0.6471035624341718, 'eval_rouge_score': 0.9207126290376295, 'eval_exact_match': 0.008, 'eval_exact_match_sem': 0.003987957825156938, 'eval_emb_cos_sim': 0.9604732394218445, 'eval_emb_cos_sim_sem': 0.011699979634603359, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4392.3534, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/heb_Hebr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating arb_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: חתונת הנישואים ע"י אשת מקסיקו, העשויה ע"י Moroccan Jewels
[true] query: فستان زفاف أنيق بقصّة الأميرة من نسيج الميكادو، مُزيّن بالأزهار



[pred] query: הרב ר' יהודה בן עמי, מנכ"ל העמותה החדשה להכנס למשרדי 20
[true] query: رئيس القمة الدينية لمجموعة العشرين الشيخ د.محمد العيسى يطلق منصة R20 



[pred] query: התכנית שואפת להרחיב את תעשיית לימודי השכלה ביהודה ושומרון, worldwide
[true] query: تسعى كلية العلوم الإسلامية إلى تبوأ مكانة وسمعة مرموقة بين جامعات العالم، 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720113123/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720113123
{'eval_loss': 5.165796756744385, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.15973915845936015, 'eval_token_set_recall': 0.187042644807893, 'eval_token_set_f1': 0.1699614939560558, 'eval_token_set_f1_sem': 0.002276701940649839, 'eval_n_ngrams_match_1': 2.49, 'eval_n_ngrams_match_2': 1.034, 'eval_n_ngrams_match_3': 0.016, 'eval_num_true_words': 15.48, 'eval_num_pred_words': 14.888, 'eval_bleu_score': 5.386590421939352, 'eval_bleu_score_sem': 0.08023342866089533, 'eval_rouge_score': 0.735010697377833, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8841069936752319, 'eval_emb_cos_sim_sem': 0.009515458426901114, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 125.3201, 'eval_samples_per_second': 3.99, 'eval_steps_per_second': 0.503}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/arb_Arab_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: חתונת הנישואין, אשר עשויה ע"י אפרוחים מקסיקני Moroccan Abaya.
[true] query: فستان زفاف أنيق بقصّة الأميرة من نسيج الميكادو، مُزيّن بالأزهار



[pred] query: הרב ר' יהודה בן עמי, מנכ"ל Islamic Revolution פותח ערוץ 20 למשכנ
[true] query: رئيس القمة الدينية لمجموعة العشرين الشيخ د.محمد العيسى يطلق منصة R20 



[pred] query: התכנית שואפת להרחיב את תחום הלימודים במזרח התיכון ותמצא פופולאריות גדולה
[true] query: تسعى كلية العلوم الإسلامية إلى تبوأ مكانة وسمعة مرموقة بين جامعات العالم، 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720117594/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720117594
{'eval_loss': 5.165796756744385, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.1592286647047487, 'eval_token_set_recall': 0.18132638365246792, 'eval_token_set_f1': 0.1676521145707656, 'eval_token_set_f1_sem': 0.0021558724574180874, 'eval_n_ngrams_match_1': 2.472, 'eval_n_ngrams_match_2': 1.032, 'eval_n_ngrams_match_3': 0.012, 'eval_num_true_words': 15.48, 'eval_num_pred_words': 15.022, 'eval_bleu_score': 5.331884149274609, 'eval_bleu_score_sem': 0.07965339993081393, 'eval_rouge_score': 0.7094480582564395, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8863122463226318, 'eval_emb_cos_sim_sem': 0.00827464664566179, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4396.8298, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/arb_Arab_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating amh_Ethi val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: אפריל 2020 אפריל 2020 אפריל 2020 אפריל 2020 עמוד ארכיוני עמק האורות ואת
[true] query: ማርች 20, 2016 የራስ ዱሜራ ደሴቶችና የባህር ግዛት የሚያሳን የኤ



[pred] query: חדשנות - כוח האדם בקהילה - כוח האדם בקהילה - האינטראקציה 
[true] query: አንድነት አንድነት ውስጥ ጥንካሬ - የህብረት ማህበረሰብ እንክብካቤ



[pred] query: 20 חודשים לפני הנסיעה, 20 חודשים לפני שהחלטתם להשתתף ב"הונגריה
[true] query: ከቡናማዎቹ ጋር ነገ ወደ ዩጋንዳ የሚያቀኑ 20 ተጫዋቾች ተለይተው
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720117758/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720117758
{'eval_loss': 8.358094215393066, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2345729866882351, 'eval_token_set_recall': 0.27835125985126047, 'eval_token_set_f1': 0.24773000408851792, 'eval_token_set_f1_sem': 0.003060769310078808, 'eval_n_ngrams_match_1': 2.762, 'eval_n_ngrams_match_2': 1.092, 'eval_n_ngrams_match_3': 0.036, 'eval_num_true_words': 11.838, 'eval_num_pred_words': 13.852, 'eval_bleu_score': 6.478298225500827, 'eval_bleu_score_sem': 0.09900442038174893, 'eval_rouge_score': 0.6727115023865025, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8554414510726929, 'eval_emb_cos_sim_sem': 0.007757413935291019, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 126.7576, 'eval_samples_per_second': 3.945, 'eval_steps_per_second': 0.497}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/amh_Ethi_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: אפריל 18, 2017 אפריל 18, 2017 אפריל 18, 2017 עמוד ארכיוני אהבות ואתרי 
[true] query: ማርች 20, 2016 የራስ ዱሜራ ደሴቶችና የባህር ግዛት የሚያሳን የኤ



[pred] query: חדשנות - כוח האדם במערכת האינטראקציה - כוח האדם במערכת האינטראק
[true] query: አንድነት አንድነት ውስጥ ጥንካሬ - የህብረት ማህበረሰብ እንክብካቤ



[pred] query: 20 חודשים לפני הנסיעה, 20 חודשים לפני שהשחקנים ילכו לענקני ה-E
[true] query: ከቡናማዎቹ ጋር ነገ ወደ ዩጋንዳ የሚያቀኑ 20 ተጫዋቾች ተለይተው
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720122216/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720122216
{'eval_loss': 8.358094215393066, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.231438314660916, 'eval_token_set_recall': 0.27493338099808745, 'eval_token_set_f1': 0.24424950141361867, 'eval_token_set_f1_sem': 0.002937839075282369, 'eval_n_ngrams_match_1': 2.724, 'eval_n_ngrams_match_2': 1.082, 'eval_n_ngrams_match_3': 0.036, 'eval_num_true_words': 11.838, 'eval_num_pred_words': 13.906, 'eval_bleu_score': 6.42308911715672, 'eval_bleu_score_sem': 0.09486705723986914, 'eval_rouge_score': 0.66503743813731, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8523471355438232, 'eval_emb_cos_sim_sem': 0.008903058791137251, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4383.7038, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/amh_Ethi_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mlt_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: Problems and Necessary Techniques - Multidisciplinary Exercises and Exercises for Beginners - Masala
[true] query: Dħul Temi Temi Problemi muskuloskeletali Practical tools and guidance - Musculoskeletal disorders



[pred] query: 'The first test in the world' – Neuvostoliiton, שטען על רקע מחלת ה- COVID-19 -
[true] query: test – One News Mindu feġġ l-ewwel każ ta' coronavirus f'pajjiżna, saru aktar



[pred] query: "Mlc - a player who doesn't afford him to play and a player who doesn
[true] query: 'L-MFA emmnet lil min ibagħbas il-logħob u mhux lili' - Illum
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720122379/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720122379
{'eval_loss': 5.749851703643799, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22886052290502767, 'eval_token_set_recall': 0.33915822978191446, 'eval_token_set_f1': 0.2623429219289549, 'eval_token_set_f1_sem': 0.0036439171445948734, 'eval_n_ngrams_match_1': 3.19, 'eval_n_ngrams_match_2': 1.14, 'eval_n_ngrams_match_3': 0.066, 'eval_num_true_words': 13.932, 'eval_num_pred_words': 13.95, 'eval_bleu_score': 5.763609052103131, 'eval_bleu_score_sem': 0.0965377654666845, 'eval_rouge_score': 0.12587676265932574, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8843809366226196, 'eval_emb_cos_sim_sem': 0.014241545235287608, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 126.601, 'eval_samples_per_second': 3.949, 'eval_steps_per_second': 0.498}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/mlt_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: Problems and challenges - M.D.M.C - Exercises and Techniques for problematic muscle 
[true] query: Dħul Temi Temi Problemi muskuloskeletali Practical tools and guidance - Musculoskeletal disorders



[pred] query: 'One of the first symptoms in the world' – Neuvostoliiton warned that a virus causes pneumonia
[true] query: test – One News Mindu feġġ l-ewwel każ ta' coronavirus f'pajjiżna, saru aktar



[pred] query: "Mlc acknowledges that a player who nominated me and anyone who nominated me" - 
[true] query: 'L-MFA emmnet lil min ibagħbas il-logħob u mhux lili' - Illum
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720126834/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720126834
{'eval_loss': 5.749851703643799, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22625123027127683, 'eval_token_set_recall': 0.34906460848756304, 'eval_token_set_f1': 0.26246400196801123, 'eval_token_set_f1_sem': 0.0034188446669215144, 'eval_n_ngrams_match_1': 3.152, 'eval_n_ngrams_match_2': 1.142, 'eval_n_ngrams_match_3': 0.06, 'eval_num_true_words': 13.932, 'eval_num_pred_words': 14.326, 'eval_bleu_score': 5.492905825657303, 'eval_bleu_score_sem': 0.09990517127637699, 'eval_rouge_score': 0.12428581764541466, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8891509771347046, 'eval_emb_cos_sim_sem': 0.01446336541608946, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4380.5062, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/mlt_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating hin_Deva val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: : : : : : : : : : : : לא להיות חכמים
[true] query: Blog News: ग़लती न करे मुसलमान!!! ग़लती न करे मुसलमान!!! 



[pred] query: Answers for Class A - Answers for Class A - Answers for Class A - Answers for Class A -
[true] query: RBSE Solutions for Class 9 Hindi Sparsh Chapter 11 आदमी नामा - Rbse solutions RBSE Solutions for Class 9



[pred] query: כדי להיות ראש ממשלת מצרים, דוד אדום בן 14 חודשים הוא 14 חודשים משנת
[true] query: नई दिल्ली, प्रधानमंत्री नरेंद्र मोदी का कार्यकाल पूरा होने में केवल 14 महीने का वक्त बा
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720127000/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720127000
{'eval_loss': 6.30627965927124, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.1922246237889633, 'eval_token_set_recall': 0.29002697872303146, 'eval_token_set_f1': 0.22532591517597714, 'eval_token_set_f1_sem': 0.003580006594578361, 'eval_n_ngrams_match_1': 3.41, 'eval_n_ngrams_match_2': 1.15, 'eval_n_ngrams_match_3': 0.056, 'eval_num_true_words': 17.206, 'eval_num_pred_words': 15.854, 'eval_bleu_score': 5.255708338585068, 'eval_bleu_score_sem': 0.08246801374820716, 'eval_rouge_score': 0.5308741038974175, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8775343298912048, 'eval_emb_cos_sim_sem': 0.01774854235303059, 'eval_emb_top1_equal': 0.125, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 128.7162, 'eval_samples_per_second': 3.885, 'eval_steps_per_second': 0.489}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/hin_Deva_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: : : : : : : לא להיות חכמים : לא להיות חכמים :
[true] query: Blog News: ग़लती न करे मुसलमान!!! ग़लती न करे मुसलमान!!! 



[pred] query: Answer for Class A - Answer for Class A - Answer for Class A - Answer for Class A - Answer for Class A
[true] query: RBSE Solutions for Class 9 Hindi Sparsh Chapter 11 आदमी नामा - Rbse solutions RBSE Solutions for Class 9



[pred] query: אלא אם כן, ניצן מאיר בן 24 חודשים ממדינת מצרים הוא 4 חודשים
[true] query: नई दिल्ली, प्रधानमंत्री नरेंद्र मोदी का कार्यकाल पूरा होने में केवल 14 महीने का वक्त बा
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720131458/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720131458
{'eval_loss': 6.30627965927124, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.19338257768278344, 'eval_token_set_recall': 0.2832957472524036, 'eval_token_set_f1': 0.22437571350677463, 'eval_token_set_f1_sem': 0.0037617745066459386, 'eval_n_ngrams_match_1': 3.414, 'eval_n_ngrams_match_2': 1.166, 'eval_n_ngrams_match_3': 0.072, 'eval_num_true_words': 17.206, 'eval_num_pred_words': 16.11, 'eval_bleu_score': 5.313742454594425, 'eval_bleu_score_sem': 0.09957408450932649, 'eval_rouge_score': 0.5133550728784344, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8715324401855469, 'eval_emb_cos_sim_sem': 0.019604100822660227, 'eval_emb_top1_equal': 0.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 4382.99, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/hin_Deva_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating urd_Arab val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: ממקור ראשון ועד מקור ראשון / אלפא / רהרה / נשיא / נשיא 
[true] query: قائداعظم سے نہرو تک ۔۔ رؤف کلاسرا مرکزی صفحہ/ لکھاری/ ر



[pred] query: סנאי סנאי התבטא בעצמה אין יותר ממיליוני אינסטגרם בעצמה עצמה
[true] query: سوناکشی سنہا سوشل میڈیا میمز کے نشے میں مبتلا ہو گئیں اداکارہ نہ صرف خود



[pred] query: דצמבר 2016: אין אוכלוסייה חדשה באפריקה - 15,000 מתושביה בשנת 2016
[true] query: نئی دہلی:مہاراشٹر میں گزشتہ 5 سالوں میں (18-2014)14034 کسانوں نے خود
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720131621/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720131621
{'eval_loss': 7.709118843078613, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.16089581864606467, 'eval_token_set_recall': 0.23816307466183684, 'eval_token_set_f1': 0.18832989028632854, 'eval_token_set_f1_sem': 0.0025942439893751343, 'eval_n_ngrams_match_1': 2.67, 'eval_n_ngrams_match_2': 1.028, 'eval_n_ngrams_match_3': 0.01, 'eval_num_true_words': 17.024, 'eval_num_pred_words': 15.136, 'eval_bleu_score': 4.7756833966390255, 'eval_bleu_score_sem': 0.03658868513852528, 'eval_rouge_score': 0.6795061440713759, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8874975442886353, 'eval_emb_cos_sim_sem': 0.005942444197866389, 'eval_emb_top1_equal': 1.0, 'eval_emb_top1_equal_sem': 0.0, 'eval_runtime': 126.1141, 'eval_samples_per_second': 3.965, 'eval_steps_per_second': 0.5}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/urd_Arab_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: ממקור ראשון ועד מקור ראשי / אלכסנדרה / רפאורה / רפאורה
[true] query: قائداعظم سے نہرو تک ۔۔ رؤف کلاسرا مرکزی صفحہ/ لکھاری/ ر



[pred] query: סנאי סנאי התבטא בעצמה אין יותר ממיליוני משתתפים באינסטגרם עצמו
[true] query: سوناکشی سنہا سوشل میڈیا میمز کے نشے میں مبتلا ہو گئیں اداکارہ نہ صرف خود



[pred] query: מרדכי: אין אוכלוסייה חדשה בתאילנד - לפני 4 שנים (שנת 2016) 
[true] query: نئی دہلی:مہاراشٹر میں گزشتہ 5 سالوں میں (18-2014)14034 کسانوں نے خود
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720136083/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720136083
{'eval_loss': 7.709118843078613, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.16109594471538582, 'eval_token_set_recall': 0.23027106605125236, 'eval_token_set_f1': 0.18591624398414214, 'eval_token_set_f1_sem': 0.002621149346335287, 'eval_n_ngrams_match_1': 2.678, 'eval_n_ngrams_match_2': 1.028, 'eval_n_ngrams_match_3': 0.01, 'eval_num_true_words': 17.024, 'eval_num_pred_words': 15.362, 'eval_bleu_score': 4.770279342547093, 'eval_bleu_score_sem': 0.035776938001945945, 'eval_rouge_score': 0.6583541705942914, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8969841599464417, 'eval_emb_cos_sim_sem': 0.00713686151851833, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 4388.1976, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/urd_Arab_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating guj_Gujr val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: "למה היא נישאה? למה היא נישאה?" - שאל בן-זוגי מקהילה
[true] query: 'અમારા વિરોધીને લગ્નમાં કેમ બોલાવ્યો?' કહી કન્યાના મા-બા



[pred] query: עכשיו, הנה תכנית ביקור גבעת יעקובוביץ' מ- 2016, כל מה שתצטרכו ללמוד
[true] query: આજે સાંજથી કેજરીવાલની ગુજરાત યાત્રા શરૂ, જાણો આખો કાર્યક્રમ | Read here



[pred] query: דף הבית » סרטים » סרטים 3D » דניאל דרייבן, זוהר בן גוריון
[true] query: ફરહાનની ડૉન 3માં જોવા મળશે ગુજરાતી શાહરુખ ખાન | Shahrukh Kahn
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720136251/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720136251
{'eval_loss': 10.357881546020508, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.21576792136312295, 'eval_token_set_recall': 0.2558386006150713, 'eval_token_set_f1': 0.22957994655005073, 'eval_token_set_f1_sem': 0.002937733624831681, 'eval_n_ngrams_match_1': 2.908, 'eval_n_ngrams_match_2': 1.036, 'eval_n_ngrams_match_3': 0.01, 'eval_num_true_words': 13.074, 'eval_num_pred_words': 14.454, 'eval_bleu_score': 5.9756325431404855, 'eval_bleu_score_sem': 0.048355953155787335, 'eval_rouge_score': 0.7121794388472241, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8993576765060425, 'eval_emb_cos_sim_sem': 0.00395670234499295, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 130.5521, 'eval_samples_per_second': 3.83, 'eval_steps_per_second': 0.483}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/guj_Gujr_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: "למה היא נישאה? למה היא נישאה?" - שאל בן-זוגי מקהילה
[true] query: 'અમારા વિરોધીને લગ્નમાં કેમ બોલાવ્યો?' કહી કન્યાના મા-બા



[pred] query: עכשיו, גבעת יעקובוביץ' עולה לשידור: הנה תכנית אירועים מ- 2016, כל
[true] query: આજે સાંજથી કેજરીવાલની ગુજરાત યાત્રા શરૂ, જાણો આખો કાર્યક્રમ | Read here



[pred] query: דף הבית » סרטים » דניאל זועקוב / DR 3D – דרווין הופעה לש
[true] query: ફરહાનની ડૉન 3માં જોવા મળશે ગુજરાતી શાહરુખ ખાન | Shahrukh Kahn
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720140724/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720140724
{'eval_loss': 10.357881546020508, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.21399991334774662, 'eval_token_set_recall': 0.2567062875033465, 'eval_token_set_f1': 0.2284500426626661, 'eval_token_set_f1_sem': 0.002882125038178774, 'eval_n_ngrams_match_1': 2.892, 'eval_n_ngrams_match_2': 1.034, 'eval_n_ngrams_match_3': 0.01, 'eval_num_true_words': 13.074, 'eval_num_pred_words': 14.53, 'eval_bleu_score': 5.965398675114169, 'eval_bleu_score_sem': 0.05054222832104384, 'eval_rouge_score': 0.6953864629750385, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8996258974075317, 'eval_emb_cos_sim_sem': 0.004260882736141153, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4397.9497, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/guj_Gujr_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating sin_Sinh val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: מדריכים - תנו תלמידים להסתכל אם אתם מרגישים אם אתם מרגישים 
[true] query: මිනිස්සු | ඇවිද යන මඟ ← ගරු කිරීම ගුරුවරු → 16 අප් රේල් අපිට කෙනෙක් දැක්කාම ආ



[pred] query: ציפורה עץ | ציפורה עץ | ציפורה עץ | ציפורה עץ |
[true] query: සිවුමැලියා සිකුරු ලියා... | Sunday Apple සිවුමැලියා සිකුරු ලියා... March 24, 2017 | 11:00 am 0



[pred] query: חדשות נערות - חדשות נערות - חדשות נערות - חדשות נערה בת חצי שנה 
[true] query: නිළිකමට මැදි වුණේ පුංචි කෙල්ලක කාලේ... - Sri Lanka News Update නිළිකමට මැ
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720140886/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720140886
{'eval_loss': 9.62022876739502, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.21217391588459733, 'eval_token_set_recall': 0.30563414787826604, 'eval_token_set_f1': 0.245040187865435, 'eval_token_set_f1_sem': 0.0032913216772979207, 'eval_n_ngrams_match_1': 2.994, 'eval_n_ngrams_match_2': 1.07, 'eval_n_ngrams_match_3': 0.018, 'eval_num_true_words': 14.694, 'eval_num_pred_words': 14.518, 'eval_bleu_score': 5.75791367894732, 'eval_bleu_score_sem': 0.05798735265791412, 'eval_rouge_score': 0.644500898631042, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8835883140563965, 'eval_emb_cos_sim_sem': 0.005384593779332259, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 125.3375, 'eval_samples_per_second': 3.989, 'eval_steps_per_second': 0.503}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/sin_Sinh_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: תלמידים - תנו לחיות אם אתם מרגישים אם אתם מרגישים - 26 בדצמ
[true] query: මිනිස්සු | ඇවිද යන මඟ ← ගරු කිරීම ගුරුවරු → 16 අප් රේල් අපිට කෙනෙක් දැක්කාම ආ



[pred] query: ציפורה סיליקונית - עמוד הבית » ציפורה סיליקונית » ציפורה סיליקונית -
[true] query: සිවුමැලියා සිකුරු ලියා... | Sunday Apple සිවුමැලියා සිකුරු ලියා... March 24, 2017 | 11:00 am 0



[pred] query: חדשות - חדשות - נערה בשנות ה-30 לחייה - נערה בשנות ה-30 לחייה -
[true] query: නිළිකමට මැදි වුණේ පුංචි කෙල්ලක කාලේ... - Sri Lanka News Update නිළිකමට මැ
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720145350/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720145350
{'eval_loss': 9.62022876739502, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.21272992572443056, 'eval_token_set_recall': 0.2998934861115897, 'eval_token_set_f1': 0.24361275952977748, 'eval_token_set_f1_sem': 0.003404334967432194, 'eval_n_ngrams_match_1': 3.018, 'eval_n_ngrams_match_2': 1.084, 'eval_n_ngrams_match_3': 0.02, 'eval_num_true_words': 14.694, 'eval_num_pred_words': 14.798, 'eval_bleu_score': 5.717400799866381, 'eval_bleu_score_sem': 0.057791713789246114, 'eval_rouge_score': 0.6371461892962644, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8800691962242126, 'eval_emb_cos_sim_sem': 0.010027142241247624, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 4390.0683, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/sin_Sinh_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating pan_Guru val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: ארכיון אטרקציות אטרקציות אטרקציות אטרקציות אטרקציות אחרי המחסום
[true] query: ਕਰੋਨਾ ਮਹਾਂਮਾਰੀ ਦੇ ਚੱਲਦੇ ਫੋਟੋਗ੍ਰਾਫਰ ਦੀਆਂ ਬੰਦ ਪਈਆਂ ਦੁਕਾਨਾਂ ਖੋ



[pred] query: יכול להיות עתיד להתחיל עם יצירת הרעיונות של מפא"ה - Fedora Digital
[true] query: ਮੋਦੀ ਦੇ ਸੁਪਨਿਆਂ ਦਾ ਯੂ ਪੀ ਬਣਾਉਣ ਲਈ ਕਵਾਇਦ ਆਰੰਭ - Panja



[pred] query: : : : : : : : : : : : : : :
[true] query: ਸ਼ਹੀਦਾਂ ਦੀਆਂ ਤਸਵੀਰਾਂ ਅਜਾਇਬ ਘਰ 'ਚ ਲਗਾਉਣ ਦੀ ਮੰਗ :
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720145514/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720145514
{'eval_loss': 8.674747467041016, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22265501850230995, 'eval_token_set_recall': 0.2806727124183012, 'eval_token_set_f1': 0.2439603702481963, 'eval_token_set_f1_sem': 0.003160265141359723, 'eval_n_ngrams_match_1': 2.92, 'eval_n_ngrams_match_2': 1.03, 'eval_n_ngrams_match_3': 0.004, 'eval_num_true_words': 13.07, 'eval_num_pred_words': 14.144, 'eval_bleu_score': 6.148885374314093, 'eval_bleu_score_sem': 0.03831749322640113, 'eval_rouge_score': 0.8155837606837608, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8589076399803162, 'eval_emb_cos_sim_sem': 0.015159003102028815, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 127.0491, 'eval_samples_per_second': 3.935, 'eval_steps_per_second': 0.496}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/pan_Guru_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: ארכיון אטרקציות אטרקציות אטרקציות אטרקציות אחרי המחסום בתעשיית התע
[true] query: ਕਰੋਨਾ ਮਹਾਂਮਾਰੀ ਦੇ ਚੱਲਦੇ ਫੋਟੋਗ੍ਰਾਫਰ ਦੀਆਂ ਬੰਦ ਪਈਆਂ ਦੁਕਾਨਾਂ ਖੋ



[pred] query: יכול להיות עתיד להחליט אם להיות מקור יצירתו של ע"מ 琮 -
[true] query: ਮੋਦੀ ਦੇ ਸੁਪਨਿਆਂ ਦਾ ਯੂ ਪੀ ਬਣਾਉਣ ਲਈ ਕਵਾਇਦ ਆਰੰਭ - Panja



[pred] query: : : : : : : : : : : : : : :
[true] query: ਸ਼ਹੀਦਾਂ ਦੀਆਂ ਤਸਵੀਰਾਂ ਅਜਾਇਬ ਘਰ 'ਚ ਲਗਾਉਣ ਦੀ ਮੰਗ :
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720149980/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720149980
{'eval_loss': 8.674747467041016, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.22227038450885242, 'eval_token_set_recall': 0.2782025588790299, 'eval_token_set_f1': 0.2419724111986306, 'eval_token_set_f1_sem': 0.003179833178120637, 'eval_n_ngrams_match_1': 2.896, 'eval_n_ngrams_match_2': 1.032, 'eval_n_ngrams_match_3': 0.008, 'eval_num_true_words': 13.07, 'eval_num_pred_words': 14.322, 'eval_bleu_score': 6.088544417894382, 'eval_bleu_score_sem': 0.03805771153750011, 'eval_rouge_score': 0.8107191083426377, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8579018115997314, 'eval_emb_cos_sim_sem': 0.014197169872161701, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4390.9064, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/pan_Guru_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating tur_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: חדשות צ'פ | צ'פ | צ'פ | צ'פ | צ'פ | צ'פ | צ'
[true] query: tek parti devri TEK PARTİ DEVRİ haberleri haber haberi | Sayfa 7 Tek parti chp zamanında yapılan... 15 Şu



[pred] query: דנה: שחקני Dinamo Zagreb: שחקני Dinamo Zagreb: שחרור ל-21:45: צפו: 
[true] query: Anasayfa Spor Fenerbahçe-Dinamo Zagreb maçının hakemleri açıklandı kaynuka Tarih: 2018-11-27 Saat: 15:07:19 



[pred] query: Home » חינוך בריאות ילדים ילדים ילדים ילדים ונוער » Instruction for Exercise and Nutrition in Children 14
[true] query: Alışveriş Annelik Bebek Çocuk Çocuk Kitapları Eğitim Sağlık 7–14 yaş çocuklar için öğretim metodları 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720150145/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720150145
{'eval_loss': 5.871555805206299, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.1924242176602663, 'eval_token_set_recall': 0.2747436337854129, 'eval_token_set_f1': 0.217375360657279, 'eval_token_set_f1_sem': 0.002856732590013218, 'eval_n_ngrams_match_1': 3.138, 'eval_n_ngrams_match_2': 1.072, 'eval_n_ngrams_match_3': 0.018, 'eval_num_true_words': 16.658, 'eval_num_pred_words': 16.168, 'eval_bleu_score': 4.812345520237966, 'eval_bleu_score_sem': 0.06267028759951936, 'eval_rouge_score': 0.11480743625385967, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8635547161102295, 'eval_emb_cos_sim_sem': 0.010786492132700781, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 128.0131, 'eval_samples_per_second': 3.906, 'eval_steps_per_second': 0.492}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/tur_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: חדשות על צ'פ | צ'פ | צ'פ ראשון | צ'פ ראשון | צ'פ ראשון
[true] query: tek parti devri TEK PARTİ DEVRİ haberleri haber haberi | Sayfa 7 Tek parti chp zamanında yapılan... 15 Şu



[pred] query: דנה: שחקני Dinamo Zagreb: שחקני Dinamo Zagreb: שחקני Dinamo Zagreb: שחקני שח
[true] query: Anasayfa Spor Fenerbahçe-Dinamo Zagreb maçının hakemleri açıklandı kaynuka Tarih: 2018-11-27 Saat: 15:07:19 



[pred] query: Home » חינוך בריאות ילדים ילדים ונוער - Acrobat Reader for Adult Education - Acrobat Reader for Adult
[true] query: Alışveriş Annelik Bebek Çocuk Çocuk Kitapları Eğitim Sağlık 7–14 yaş çocuklar için öğretim metodları 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720154605/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720154605
{'eval_loss': 5.871555805206299, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.1942771746421873, 'eval_token_set_recall': 0.27481258615489057, 'eval_token_set_f1': 0.2186583831712155, 'eval_token_set_f1_sem': 0.0029999991860173584, 'eval_n_ngrams_match_1': 3.146, 'eval_n_ngrams_match_2': 1.074, 'eval_n_ngrams_match_3': 0.018, 'eval_num_true_words': 16.658, 'eval_num_pred_words': 16.356, 'eval_bleu_score': 4.7275351729952435, 'eval_bleu_score_sem': 0.06135710640325998, 'eval_rouge_score': 0.11454336818043913, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8634654879570007, 'eval_emb_cos_sim_sem': 0.01129526715595553, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 4386.2152, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/tur_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating kaz_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: B.A.M.E.M.E.M.E.M.E.M.E.M פעילויות
[true] query: ҚМГ АЛТЫН ЕРЕЖЕЛЕРІ - ЕҢбек қауіпсіздігі және еңбекті қОРҒау жиналыстарын



[pred] query: פסיכיאטריה תיבת תיבת תיבת תיבת תיבת תגובה 20180825 מהווים 8
[true] query: психометриялық тестілеу кестесі инструкция по приму апелляциялық ведомость 18.10.2018 ж No 578 бұйры



[pred] query: י. I. I. I. I. I. I. I. I - תולדות היוצרים הגדולים,
[true] query: Ыбырай Алтынсарин - Ы - Ұлы заман Тұлғалары - Скачать Рефераты,
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720154768/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720154768
{'eval_loss': 5.886670112609863, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.21257182902237107, 'eval_token_set_recall': 0.27694369033099714, 'eval_token_set_f1': 0.2347223938103883, 'eval_token_set_f1_sem': 0.0031402350460771095, 'eval_n_ngrams_match_1': 3.288, 'eval_n_ngrams_match_2': 1.08, 'eval_n_ngrams_match_3': 0.034, 'eval_num_true_words': 15.352, 'eval_num_pred_words': 15.34, 'eval_bleu_score': 5.501573857707151, 'eval_bleu_score_sem': 0.06328559243901434, 'eval_rouge_score': 0.5576070213952489, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8752228021621704, 'eval_emb_cos_sim_sem': 0.008543464513820336, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 125.6568, 'eval_samples_per_second': 3.979, 'eval_steps_per_second': 0.501}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/kaz_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: BMG AKTIVITĖS - פעילותם ותמיכה ארגונית - סיכויי ההצלחה 
[true] query: ҚМГ АЛТЫН ЕРЕЖЕЛЕРІ - ЕҢбек қауіпсіздігі және еңбекті қОРҒау жиналыстарын



[pred] query: תיבת סיכום מבחינה פסיכואנליטית מבחינה פסיכואנליטית 18.08.2017 פסק הדין
[true] query: психометриялық тестілеу кестесі инструкция по приму апелляциялық ведомость 18.10.2018 ж No 578 бұйры



[pred] query: I. I. I. I. I. I. I. I. I. היא תולדות המחזירים הגדולים
[true] query: Ыбырай Алтынсарин - Ы - Ұлы заман Тұлғалары - Скачать Рефераты,
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720159229/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720159229
{'eval_loss': 5.886670112609863, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2113966917843081, 'eval_token_set_recall': 0.27960087089515706, 'eval_token_set_f1': 0.2335619525771313, 'eval_token_set_f1_sem': 0.0031604759508802233, 'eval_n_ngrams_match_1': 3.252, 'eval_n_ngrams_match_2': 1.084, 'eval_n_ngrams_match_3': 0.034, 'eval_num_true_words': 15.352, 'eval_num_pred_words': 15.598, 'eval_bleu_score': 5.443168500777562, 'eval_bleu_score_sem': 0.0666579836531065, 'eval_rouge_score': 0.5176053282214814, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.879009485244751, 'eval_emb_cos_sim_sem': 0.00878906276117192, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 4387.1665, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/kaz_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating cmn_Hani val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query:,,,,, סיכון גבוה יחסית במשכנתאות מתחילים חזק מכירה
[true] query: 下行以及现货成交偏弱拖累,市场主导地区价格继续快速调低,邯郸中板价格重



[pred] query: סוג המוצר: מוצרי ביטוח בזול יותר מבזק בזול יותר מבזק בזול<?xml:
[true] query: 每日彩票 珍珠棉的包装定位是一种具有高强的缓冲吸震抗震作用的有明显环保效力的包装



[pred] query: באתר Coins.co.il יצא מיזם שיכול לבצע חינם כסף בחינם, שבשנים האחרונות,
[true] query: 火币云于上个月刚刚推出,允许用户开发他们自己的类似火币网的数字货币交易平台,其中包括钱
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720159393/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720159393
{'eval_loss': 7.455591678619385, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 31.9140625, 'eval_token_set_precision': 0.3987826682089843, 'eval_token_set_recall': 0.2252927804393753, 'eval_token_set_f1': 0.2737283205359154, 'eval_token_set_f1_sem': 0.0031505818831080888, 'eval_n_ngrams_match_1': 2.876, 'eval_n_ngrams_match_2': 1.01, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 7.802, 'eval_num_pred_words': 16.486, 'eval_bleu_score': 4.979099514497457, 'eval_bleu_score_sem': 0.07450593391023505, 'eval_rouge_score': 0.4646528194474947, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8209083080291748, 'eval_emb_cos_sim_sem': 0.015215710369611207, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 126.5393, 'eval_samples_per_second': 3.951, 'eval_steps_per_second': 0.498}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/cmn_Hani_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query:,,, סיכון גבוה במכשירי רכישה מתחילים במחיר נמוך במכשירי רכי
[true] query: 下行以及现货成交偏弱拖累,市场主导地区价格继续快速调低,邯郸中板价格重



[pred] query: קטגוריה: ביטוח מזון בזול<?xml:namespace prefix = o ns = "
[true] query: 每日彩票 珍珠棉的包装定位是一种具有高强的缓冲吸震抗震作用的有明显环保效力的包装



[pred] query: Coins.co.il מציע שימוש בחינם במטבעות שהם משתמשים בו כבר בשנים האחרונות,”
[true] query: 火币云于上个月刚刚推出,允许用户开发他们自己的类似火币网的数字货币交易平台,其中包括钱
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720163853/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720163853
{'eval_loss': 7.455591678619385, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 31.9140625, 'eval_token_set_precision': 0.39556239608344906, 'eval_token_set_recall': 0.219869613418917, 'eval_token_set_f1': 0.26950225190535115, 'eval_token_set_f1_sem': 0.003230272502994786, 'eval_n_ngrams_match_1': 2.852, 'eval_n_ngrams_match_2': 1.014, 'eval_n_ngrams_match_3': 0.004, 'eval_num_true_words': 7.802, 'eval_num_pred_words': 16.702, 'eval_bleu_score': 5.038720078966022, 'eval_bleu_score_sem': 0.08498172501842227, 'eval_rouge_score': 0.4516216075728915, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8230910301208496, 'eval_emb_cos_sim_sem': 0.012610032826896805, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4386.0604, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/cmn_Hani_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating jpn_Jpan val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: תחילה:<br><br><br><br><br><br><br><br><br><br><br><br
[true] query: 花立山荘では夜半から雨が降り続いていて、朝にちょっとだけ雨脚が弱くなったときに出発。 しばらく



[pred] query: צילום\צילום\צילום\צילום\צילום\צילום\צילום\צילום\צילום\סקי תחתונה<?xml:namespace
[true] query: 今シーズン初の凍結した桧原湖の湖上撮影です。 かなり結氷は進んでいましたが、まだワカサギ



[pred] query: קודם כל, מטרה:<?xml:namespace prefix = st1 ns = "urn:schemas
[true] query: まず少なくともこの人たちチームにネット周りが強い人間がいることは間違いなくて、YouTubeのメタタグ(メタタグって?って人はこの記事
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720164017/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720164017
{'eval_loss': 7.647457122802734, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.5273332924385539, 'eval_token_set_recall': 0.20199275289554014, 'eval_token_set_f1': 0.2666069957622489, 'eval_token_set_f1_sem': 0.003559169880264491, 'eval_n_ngrams_match_1': 2.16, 'eval_n_ngrams_match_2': 1.002, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 4.93, 'eval_num_pred_words': 17.36, 'eval_bleu_score': 4.367659914836564, 'eval_bleu_score_sem': 0.060973293561117005, 'eval_rouge_score': 0.4003547285337073, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8700873851776123, 'eval_emb_cos_sim_sem': 0.006955351505594609, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 126.5944, 'eval_samples_per_second': 3.95, 'eval_steps_per_second': 0.498}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/jpn_Jpan_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: <br><br><br><br><br><br><br><br><br><br><br><br><br><br
[true] query: 花立山荘では夜半から雨が降り続いていて、朝にちょっとだけ雨脚が弱くなったときに出発。 しばらく



[pred] query: צילום<?xml:namespace prefix = st1 ns = "urn:schemas-microsoft-com/
[true] query: 今シーズン初の凍結した桧原湖の湖上撮影です。 かなり結氷は進んでいましたが、まだワカサギ



[pred] query: קודם כל,<?xml:namespace prefix = st1 ns = "urn:schemas-microsoft-
[true] query: まず少なくともこの人たちチームにネット周りが強い人間がいることは間違いなくて、YouTubeのメタタグ(メタタグって?って人はこの記事
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720168484/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720168484
{'eval_loss': 7.647457122802734, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.5268888479941094, 'eval_token_set_recall': 0.20158880697046394, 'eval_token_set_f1': 0.2639523935603844, 'eval_token_set_f1_sem': 0.0036530723757474395, 'eval_n_ngrams_match_1': 2.162, 'eval_n_ngrams_match_2': 1.002, 'eval_n_ngrams_match_3': 0.0, 'eval_num_true_words': 4.93, 'eval_num_pred_words': 17.92, 'eval_bleu_score': 4.368279850246753, 'eval_bleu_score_sem': 0.07052119859073315, 'eval_rouge_score': 0.3587582720762588, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8707369565963745, 'eval_emb_cos_sim_sem': 0.010977732140729947, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 4393.0855, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/jpn_Jpan_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating kor_Hang val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: ראשי » אירועים » עמוד ראשי » עמוד ראשי » Updated on October 24, 2020 -
[true] query: 세일 중 – Natural Health {% if first_available_variant.compare_at_price > first_available_variant.price



[pred] query: דף הבית / חדשות / חוקי ביטוח / חוקי ביטוח / נבחרות כניסה
[true] query: 신용위험 결정요인 과 관련 - 토토사이트 검증사이트 메이저토토사이트 - 토토탐정 - 신



[pred] query: SAMSUNG - SAMSUNG - SAMSUNG - SAMSUNG - SAMSUNG - דגם חדש באנגלית
[true] query: 안드로이드 : 삼성바다폰 영국 온라인 등록 - 타이젠 - 안드로이드 스마트폰과 태블릿 새
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720168649/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720168649
{'eval_loss': 7.829573154449463, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.2068669477495333, 'eval_token_set_recall': 0.2681226637741346, 'eval_token_set_f1': 0.22682191345648572, 'eval_token_set_f1_sem': 0.0033115492130151446, 'eval_n_ngrams_match_1': 2.884, 'eval_n_ngrams_match_2': 1.044, 'eval_n_ngrams_match_3': 0.016, 'eval_num_true_words': 14.33, 'eval_num_pred_words': 15.006, 'eval_bleu_score': 5.304037861270752, 'eval_bleu_score_sem': 0.06888514776517898, 'eval_rouge_score': 0.5273364148000337, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.9109585881233215, 'eval_emb_cos_sim_sem': 0.012527712129220934, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 128.2532, 'eval_samples_per_second': 3.899, 'eval_steps_per_second': 0.491}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/kor_Hang_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: עמוד הבית › עמוד ראשי › Pre-conditioned › עמוד ראשי › עמוד ראשי ›
[true] query: 세일 중 – Natural Health {% if first_available_variant.compare_at_price > first_available_variant.price



[pred] query: ראשי / חדשות בנקאי / חדשות בנקאי / חוקי כניסה בנקאי / חוקי כניסה
[true] query: 신용위험 결정요인 과 관련 - 토토사이트 검증사이트 메이저토토사이트 - 토토탐정 - 신



[pred] query: SAMSUNG - SAMSUNG - SAMSUNG - SAMSUNG - SAMSUNG - SAMSUNG - טאבלט
[true] query: 안드로이드 : 삼성바다폰 영국 온라인 등록 - 타이젠 - 안드로이드 스마트폰과 태블릿 새
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720173117/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720173117
{'eval_loss': 7.829573154449463, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.20709789531074407, 'eval_token_set_recall': 0.2675034297745601, 'eval_token_set_f1': 0.22595058844909663, 'eval_token_set_f1_sem': 0.0032506424531854234, 'eval_n_ngrams_match_1': 2.902, 'eval_n_ngrams_match_2': 1.05, 'eval_n_ngrams_match_3': 0.02, 'eval_num_true_words': 14.33, 'eval_num_pred_words': 14.894, 'eval_bleu_score': 5.317586649909614, 'eval_bleu_score_sem': 0.07068966064775078, 'eval_rouge_score': 0.5108892627001822, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.914713978767395, 'eval_emb_cos_sim_sem': 0.007071761759726207, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 4394.1375, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/kor_Hang_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mon_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: התועלת הבינלאומית תהיה גבוה. הכנסות Angola עלו ל-66 מיליארד דו
[true] query: Алтны олборлолт амжилтад хүрнэ ОХУын гадаад өр 56 тэрбум ам.доллар болж өсчээ



[pred] query: חשוב לשים לב - שינויים בתפריט Google Chrome - תיבת התגובות של Google Chrome -
[true] query: Google Текстийн зарын өөрчлөлтийг анхаарч үзэх 3 зүйл | Martech Zone Google Текстийн зарын



[pred] query: 0x0x0x0x0x0x0x0x0x0x0x0x0x0x
[true] query: » СЕКС (1085550) » ШАР МЭДЭЭ (805751) » ӨГҮҮЛЛЭГ (906359) хөлийг 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720173280/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720173280
{'eval_loss': 7.205585479736328, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.20538178367899784, 'eval_token_set_recall': 0.28119928666105165, 'eval_token_set_f1': 0.22892217706865275, 'eval_token_set_f1_sem': 0.002947724942900803, 'eval_n_ngrams_match_1': 2.91, 'eval_n_ngrams_match_2': 1.08, 'eval_n_ngrams_match_3': 0.052, 'eval_num_true_words': 14.004, 'eval_num_pred_words': 14.382, 'eval_bleu_score': 5.582197444253809, 'eval_bleu_score_sem': 0.0716097191438358, 'eval_rouge_score': 0.549050577729797, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8924072980880737, 'eval_emb_cos_sim_sem': 0.015325271102315276, 'eval_emb_top1_equal': 0.375, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 125.9566, 'eval_samples_per_second': 3.97, 'eval_steps_per_second': 0.5}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/mon_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: התועלת הבינלאומית תהיה גבוה. היקף הדולרים התגלגל ל-66 מיליון
[true] query: Алтны олборлолт амжилтад хүрнэ ОХУын гадаад өр 56 тэрбум ам.доллар болж өсчээ



[pred] query: חשוב לדעת - 3 שינויים בתיבת הקוד של Google Translate - שינויים בתיבת הקוד של Google
[true] query: Google Текстийн зарын өөрчлөлтийг анхаарч үзэх 3 зүйл | Martech Zone Google Текстийн зарын



[pred] query: X0X0X0X0X0X0X0X0X0X0X0X0X0X0
[true] query: » СЕКС (1085550) » ШАР МЭДЭЭ (805751) » ӨГҮҮЛЛЭГ (906359) хөлийг 
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720177739/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720177739
{'eval_loss': 7.205585479736328, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.20523693077648242, 'eval_token_set_recall': 0.28923293487558266, 'eval_token_set_f1': 0.22946992123954996, 'eval_token_set_f1_sem': 0.0029707367021748177, 'eval_n_ngrams_match_1': 2.91, 'eval_n_ngrams_match_2': 1.074, 'eval_n_ngrams_match_3': 0.048, 'eval_num_true_words': 14.004, 'eval_num_pred_words': 14.554, 'eval_bleu_score': 5.530931902026831, 'eval_bleu_score_sem': 0.0756548138384105, 'eval_rouge_score': 0.5319667781815718, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8908851146697998, 'eval_emb_cos_sim_sem': 0.01277645624368715, 'eval_emb_top1_equal': 0.5, 'eval_emb_top1_equal_sem': 0.18898223296457348, 'eval_runtime': 4384.5381, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/mon_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating hun_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: שיעורים: טיפול בלחץ מוחלט של כף היד (Electromagnetic stimulation). חסימת כף 
[true] query: Ultrahang kezelés: Fájdalomcsillapítás tűszűrás nélkül [teljes útmutató] Ízületi fájdalom fono



[pred] query: T.M.T., שחקני UFC, שחקני UFC, שחררו את התוכנית שלו היום.
[true] query: Az új Mike Tyson | SamanSport.hu 2019. 12. 13., Péntek, 18:40 Gervonta "Tank" Davis hatalmas



[pred] query: מכשיר כלי רכב: The Variety of Vehicles: The Variety of Vehicles: The Variety of Vehicles:
[true] query: Járművek | Hobbi Zóna - Part 2 TankChair – az Off Road tolószék A rendkívüli gépezet
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720177902/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720177902
{'eval_loss': 6.183481216430664, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.19710992700969515, 'eval_token_set_recall': 0.3007754834518771, 'eval_token_set_f1': 0.2295502261592201, 'eval_token_set_f1_sem': 0.003094500944515669, 'eval_n_ngrams_match_1': 3.192, 'eval_n_ngrams_match_2': 1.046, 'eval_n_ngrams_match_3': 0.012, 'eval_num_true_words': 16.376, 'eval_num_pred_words': 14.584, 'eval_bleu_score': 4.782293322733725, 'eval_bleu_score_sem': 0.06409482830134766, 'eval_rouge_score': 0.10280202359843793, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8941938877105713, 'eval_emb_cos_sim_sem': 0.0075276062681518445, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 125.903, 'eval_samples_per_second': 3.971, 'eval_steps_per_second': 0.5}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/hun_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: שיעורים: טיפול בלחץ כף הרגליים [electromagnetic stimulation]. טיפול בלחץ כף 
[true] query: Ultrahang kezelés: Fájdalomcsillapítás tűszűrás nélkül [teljes útmutató] Ízületi fájdalom fono



[pred] query: T.M.T, שחקני UFC, שחקני UFC, שחקני UFC: "הכל כבר כאן
[true] query: Az új Mike Tyson | SamanSport.hu 2019. 12. 13., Péntek, 18:40 Gervonta "Tank" Davis hatalmas



[pred] query: מכשיר כלי רכב – The Variety of Trains: The Variety of Trains: The Variety of Trains:
[true] query: Járművek | Hobbi Zóna - Part 2 TankChair – az Off Road tolószék A rendkívüli gépezet
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720182355/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720182355
{'eval_loss': 6.183481216430664, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.19412466642404771, 'eval_token_set_recall': 0.31919520956339875, 'eval_token_set_f1': 0.23007065676440866, 'eval_token_set_f1_sem': 0.003171742508164574, 'eval_n_ngrams_match_1': 3.146, 'eval_n_ngrams_match_2': 1.08, 'eval_n_ngrams_match_3': 0.016, 'eval_num_true_words': 16.376, 'eval_num_pred_words': 14.812, 'eval_bleu_score': 4.690970627670598, 'eval_bleu_score_sem': 0.07197333178279142, 'eval_rouge_score': 0.1033673028882158, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8725451231002808, 'eval_emb_cos_sim_sem': 0.010281915330684422, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 4379.0014, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/hun_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating mhr_Cyrl val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: הוא היה לילה אחד, אבל הוא היה לילה ארבעה צלצל'ה זלזול
[true] query: Тургым жапыште, шошо ага годым, кеч ик гана Кугу Качак ялыште лияш



[pred] query: Мая́л́а́л́а я́л́а я́л́а тебя צעירים צעירים
[true] query: Мо тыгай Агавайрем? Кузе тудо эрта? Тиде да моло йодышлан вашмутым Марий в



[pred] query: אך הוא עשה תנועה. "Петропо́р Победи́т Победи́т Рог
[true] query: Коряк рвезе Пётр Нестеров дене мый Наро-Фоминск олаште эртыше «По
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720182518/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720182518
{'eval_loss': 6.0964741706848145, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.20510193515634728, 'eval_token_set_recall': 0.33976891270064696, 'eval_token_set_f1': 0.24638636793866328, 'eval_token_set_f1_sem': 0.002869157133163214, 'eval_n_ngrams_match_1': 2.882, 'eval_n_ngrams_match_2': 1.084, 'eval_n_ngrams_match_3': 0.052, 'eval_num_true_words': 13.858, 'eval_num_pred_words': 13.546, 'eval_bleu_score': 5.852185630291821, 'eval_bleu_score_sem': 0.08285879234987259, 'eval_rouge_score': 0.6654798528346858, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8601011037826538, 'eval_emb_cos_sim_sem': 0.010320817530404484, 'eval_emb_top1_equal': 0.75, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 126.1146, 'eval_samples_per_second': 3.965, 'eval_steps_per_second': 0.5}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/mhr_Cyrl_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: היה לו לילה אחד, אבל היה לו לילה אחד, 'Luchushka Rukhushka' שני
[true] query: Тургым жапыште, шошо ага годым, кеч ик гана Кугу Качак ялыште лияш



[pred] query: Мая́л́и́л́и́л́и́л́и́л́и́л́и́т
[true] query: Мо тыгай Агавайрем? Кузе тудо эрта? Тиде да моло йодышлан вашмутым Марий в



[pred] query: אך הוא עשיר. “Петрович Победа – Наши хвосты – Наши хвосты –
[true] query: Коряк рвезе Пётр Нестеров дене мый Наро-Фоминск олаште эртыше «По
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720186965/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720186965
{'eval_loss': 6.0964741706848145, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.20203613510672366, 'eval_token_set_recall': 0.35972024103347705, 'eval_token_set_f1': 0.24824621147166412, 'eval_token_set_f1_sem': 0.002977600899274963, 'eval_n_ngrams_match_1': 2.842, 'eval_n_ngrams_match_2': 1.082, 'eval_n_ngrams_match_3': 0.048, 'eval_num_true_words': 13.858, 'eval_num_pred_words': 13.914, 'eval_bleu_score': 5.737960886252555, 'eval_bleu_score_sem': 0.08699096414077274, 'eval_rouge_score': 0.6670955688068556, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.841071367263794, 'eval_emb_cos_sim_sem': 0.008313037156299151, 'eval_emb_top1_equal': 0.875, 'eval_emb_top1_equal_sem': 0.12499999786071611, 'eval_runtime': 4373.4904, 'eval_samples_per_second': 0.114, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/mhr_Cyrl_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating fin_Latn val_dataset
evaluating corrector 
evaluating corrector with steps 1
[pred] query: אנו משתדלים לשנות את חוויית הלקוח ואת תהליך הטיפול. “haina” משתנה
[true] query: Tulemme jatkamaan asiakkaan kuuntelua, osallistamista ja haastamista. Työympäristö muutos hankkeissa tuotetaan myö



[pred] query: תפקידי: כיצד היא יכולה להיות מוצלחת בחיי החיים?Donetsk-Donetsk-Donetsk
[true] query: Pohjois-suomalaisuus ja kaikille arvokas elämä - siinä tavoitteet toiminnalle. Työskentelen Diakonia-



[pred] query: גם במקרים של קוקטיילים, נחשפנו לעובדה: “substance of medicinal drugs”, אך
[true] query: Kun kirjoitimme koivunjalojen lääketieteellisistä ominaisuuksista, mainitsimme, että paitsi munuaiset, myös ko
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720187128/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720187128
{'eval_loss': 6.225722312927246, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.18291712093376192, 'eval_token_set_recall': 0.22297744701978486, 'eval_token_set_f1': 0.19486527347432042, 'eval_token_set_f1_sem': 0.0024449759808564536, 'eval_n_ngrams_match_1': 2.928, 'eval_n_ngrams_match_2': 1.034, 'eval_n_ngrams_match_3': 0.016, 'eval_num_true_words': 15.462, 'eval_num_pred_words': 16.396, 'eval_bleu_score': 5.1421822378361135, 'eval_bleu_score_sem': 0.04862437215852095, 'eval_rouge_score': 0.11643270914236598, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8782777190208435, 'eval_emb_cos_sim_sem': 0.012006057951023566, 'eval_emb_top1_equal': 0.25, 'eval_emb_top1_equal_sem': 0.16366341987889688, 'eval_runtime': 125.3096, 'eval_samples_per_second': 3.99, 'eval_steps_per_second': 0.503}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/fin_Latn_steps-1.json
evaluating corrector with steps 20
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 7.97 GiB is free. Including non-PyTorch memory, this process has 36.58 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 6.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
evaluating corrector with steps 50 and beam width 4
[pred] query: אנו הופכים את פעילותם של העובדים ל“wohnen aufnehmen”. יישום תהליך משתנה
[true] query: Tulemme jatkamaan asiakkaan kuuntelua, osallistamista ja haastamista. Työympäristö muutos hankkeissa tuotetaan myö



[pred] query: תפקידי: כיצד היא יכולה להיות מוצלחת בחיינו?Donetsk-Donetsk-Donetsk-Po
[true] query: Pohjois-suomalaisuus ja kaikille arvokas elämä - siinä tavoitteet toiminnalle. Työskentelen Diakonia-



[pred] query: גם במקרים של קוקטיילים, נחשפנו לעובדה: “substance of medicinal organisms”, אך
[true] query: Kun kirjoitimme koivunjalojen lääketieteellisistä ominaisuuksista, mainitsimme, että paitsi munuaiset, myös ko
outptufile for decoded sequences:  saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720191607/decoded_sequences.csv
saving embeddings for preds and labels ....
saving embeddings for preds and labels to saves/yiyic__mt5_me5_heb_Hebr_32_2layers_corrector/decoded_eval_1720191607
{'eval_loss': 6.225722312927246, 'eval_pred_num_tokens': 31.0, 'eval_true_num_tokens': 32.0, 'eval_token_set_precision': 0.1851439434576433, 'eval_token_set_recall': 0.2288296621507148, 'eval_token_set_f1': 0.197419861781553, 'eval_token_set_f1_sem': 0.002386388720371186, 'eval_n_ngrams_match_1': 2.956, 'eval_n_ngrams_match_2': 1.034, 'eval_n_ngrams_match_3': 0.018, 'eval_num_true_words': 15.462, 'eval_num_pred_words': 16.686, 'eval_bleu_score': 5.0988560236001526, 'eval_bleu_score_sem': 0.052106478657193325, 'eval_rouge_score': 0.11447132302525642, 'eval_exact_match': 0.0, 'eval_exact_match_sem': 0.0, 'eval_emb_cos_sim': 0.8770647048950195, 'eval_emb_cos_sim_sem': 0.011897521913573919, 'eval_emb_top1_equal': 0.625, 'eval_emb_top1_equal_sem': 0.1829812593064711, 'eval_runtime': 4405.7159, 'eval_samples_per_second': 0.113, 'eval_steps_per_second': 0.014}
saving results to ./saves/correctors/mt5_multilingual_e5_base_mt-ms_heb_Hebr_32_2layers_prefix/evaluations/fin_Latn_steps-50_sbeam-4.json
evaluating corrector with steps 50 and beam width 8
CUDA out of memory. Tried to allocate 14.79 GiB. GPU 0 has a total capacty of 44.56 GiB of which 8.45 GiB is free. Including non-PyTorch memory, this process has 36.10 GiB memory in use. Of the allocated memory 29.90 GiB is allocated by PyTorch, and 5.89 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF, eval did not finish
